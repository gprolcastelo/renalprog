{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#renalprog","title":"renalprog","text":"<p>A Python package for simulating kidney cancer progression with synthetic data generation and machine learning</p> <p> </p> <p> <sub>Logo: Kidneys icons created by Smashicons - Flaticon</sub> </p> <p>Warning</p> <p>This site is under construction. Some pages may be missing.</p>"},{"location":"#overview","title":"Overview","text":"<p><code>renalprog</code> is a comprehensive bioinformatics pipeline for analyzing kidney cancer (KIRC) progression using deep learning and pathway enrichment analysis. The package integrates Variational Autoencoders (VAEs) with differential expression analysis and gene set enrichment to model and predict cancer progression trajectories.</p>"},{"location":"#scientific-context","title":"Scientific Context","text":"<p>Cancer progression is a complex, dynamic process involving multiple molecular alterations across time. Traditional static analyses fail to capture the temporal dynamics of tumor evolution. <code>renalprog</code> addresses this challenge by:</p> <ol> <li>Learning latent representations of gene expression data using deep generative models (VAEs)</li> <li>Generating synthetic trajectories between cancer stages in latent space</li> <li>Identifying enriched biological pathways along progression trajectories</li> <li>Classifying cancer stages using interpretable machine learning</li> </ol> <p>This approach enables researchers to:</p> <ul> <li>Identify key biological pathways driving cancer progression</li> <li>Predict patient outcomes based on molecular profiles</li> <li>Generate testable hypotheses about therapeutic targets</li> <li>Understand the temporal dynamics of tumor evolution</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#data-processing","title":"\ud83d\udd2c Data Processing","text":"<ul> <li>Automated filtering of low-expression genes</li> <li>Robust outlier detection using Mahalanobis distance</li> <li>Normalization and batch effect correction</li> <li>Integration with TCGA and other genomics datasets</li> </ul>"},{"location":"#deep-learning-models","title":"\ud83e\udde0 Deep Learning Models","text":"<ul> <li>Variational Autoencoder (VAE) for unsupervised representation learning</li> <li>Conditional VAE (CVAE) for stage-specific modeling</li> <li>Support for custom architectures and hyperparameters</li> <li>GPU acceleration for large-scale datasets</li> </ul>"},{"location":"#trajectory-generation","title":"\ud83d\udd04 Trajectory Generation","text":"<ul> <li>Generate synthetic patient trajectories between cancer stages</li> <li>Interpolation in latent space with biological constraints</li> <li>Multiple trajectory types (early-to-late, stage-specific, custom)</li> <li>Quality control and validation metrics</li> </ul>"},{"location":"#stage-classification","title":"\ud83d\udcca Stage Classification","text":"<ul> <li>XGBoost-based classification of early vs. late stage cancer</li> <li>SHAP values for feature importance and interpretability</li> <li>Cross-validation and performance evaluation</li> <li>Gene signature discovery</li> </ul>"},{"location":"#enrichment-analysis","title":"\ud83e\uddec Enrichment Analysis","text":"<ul> <li>Integration with DESeq2 for differential expression</li> <li>GSEA (Gene Set Enrichment Analysis) for pathway analysis</li> <li>Support for Reactome, KEGG, and custom pathway databases</li> <li>Parallel processing for large-scale analyses</li> </ul>"},{"location":"#visualization","title":"\ud83d\udcc8 Visualization","text":"<ul> <li>Comprehensive plotting functions for all analysis steps</li> <li>UMAP/t-SNE visualizations of latent space</li> <li>Pathway enrichment heatmaps</li> <li>Classification performance metrics</li> <li>Interactive plots with Plotly</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone repository\ngit clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\n\n# Create environment (includes Python and R)\nmamba env create -f environment.yml\nmamba activate renalprog\n\n# Install package\npip install -e .\n</code></pre> <p>See Installation Guide for detailed instructions.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from renalprog import config, dataset, modeling\n\n# Load and process data\ndata = dataset.load_data('data/processed/rnaseq_maha.csv')\nprocessed = dataset.preprocess(data)\n\n# Train VAE\nvae = modeling.VAE(input_dim=processed.shape[1])\nvae.train(processed, epochs=100)\n\n# Generate trajectories\ntrajectories = modeling.generate_trajectories(\n    vae=vae,\n    start_stage='early',\n    end_stage='late',\n    n_samples=100\n)\n\n# Run enrichment analysis\nresults = modeling.enrichment_analysis(trajectories)\n</code></pre> <p>See Quick Start Tutorial for a complete example.</p>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<p>The <code>renalprog</code> pipeline consists of six main steps:</p> <pre><code>graph LR\n    A[Raw Data] --&gt; B[1. Data Processing]\n    B --&gt; C[2. VAE Training]\n    C --&gt; D[3. Reconstruction Check]\n    D --&gt; E[4. Trajectory Generation]\n    E --&gt; F[5. Classification]\n    F --&gt; G[6. Enrichment Analysis]\n    G --&gt; H[Results &amp; Visualization]</code></pre>"},{"location":"#step-1-data-processing","title":"Step 1: Data Processing","text":"<ul> <li>Filter low-expression genes</li> <li>Remove outliers using Mahalanobis distance</li> <li>Normalize expression values</li> <li>Prepare clinical metadata</li> </ul>"},{"location":"#step-2-vae-training","title":"Step 2: VAE Training","text":"<ul> <li>Train deep generative models on gene expression data</li> <li>Learn low-dimensional latent representations</li> <li>Validate reconstruction quality</li> </ul>"},{"location":"#step-3-reconstruction-validation","title":"Step 3: Reconstruction Validation","text":"<ul> <li>Assess VAE reconstruction accuracy</li> <li>Visualize latent space structure</li> <li>Identify potential issues</li> </ul>"},{"location":"#step-4-trajectory-generation","title":"Step 4: Trajectory Generation","text":"<ul> <li>Generate synthetic patient trajectories</li> <li>Interpolate between cancer stages</li> <li>Export trajectory gene expression</li> </ul>"},{"location":"#step-5-classification","title":"Step 5: Classification","text":"<ul> <li>Train XGBoost classifier for stage prediction</li> <li>Calculate SHAP values for interpretability</li> <li>Identify important gene signatures</li> </ul>"},{"location":"#step-6-enrichment-analysis","title":"Step 6: Enrichment Analysis","text":"<ul> <li>Differential expression analysis with DESeq2</li> <li>Pathway enrichment with GSEA</li> <li>Identify biological processes along trajectories</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#for-new-users","title":"\ud83d\udcda For New Users","text":"<p>Start with:</p> <ol> <li>Installation Guide - Set up your environment</li> <li>Quick Start Tutorial - Run your first analysis</li> <li>Complete Pipeline Tutorial - End-to-end workflow</li> </ol>"},{"location":"#for-reproducibility","title":"\ud83d\udd2c For Reproducibility","text":"<p>Reproduce published results:</p> <ol> <li>System Requirements - Hardware and software needs</li> <li>Data Preparation - Download and prepare data</li> <li>Running the Pipeline - Step-by-step execution</li> <li>Expected Results - Validate your outputs</li> </ol>"},{"location":"#for-developers","title":"\ud83d\udee0\ufe0f For Developers","text":"<p>Extend and customize:</p> <ol> <li>API Reference - Complete function documentation</li> <li>Architecture Guide - Design principles</li> <li>Custom Models - Implement new architectures</li> <li>Contributing Guidelines - Join development</li> </ol>"},{"location":"#system-requirements","title":"System Requirements","text":""},{"location":"#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>OS: Linux, macOS, or Windows (with WSL for enrichment analysis)</li> <li>Python: 3.9 or higher</li> <li>R: 4.0 or higher (for enrichment analysis)</li> <li>RAM: 8 GB</li> <li>Storage: 20 GB free space</li> </ul>"},{"location":"#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>RAM: 16+ GB</li> <li>CPU: 8+ cores</li> <li>GPU: CUDA-capable GPU with 6+ GB VRAM (for VAE training)</li> <li>Storage: 50+ GB on SSD</li> </ul>"},{"location":"#software-dependencies","title":"Software Dependencies","text":"<ul> <li>PyTorch 2.0+</li> <li>scikit-learn 1.0+</li> <li>XGBoost 1.5+</li> <li>pandas, numpy, scipy</li> <li>R packages: DESeq2, gprofiler2</li> </ul> <p>See System Requirements for complete details.</p>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#cancer-research","title":"Cancer Research","text":"<ul> <li>Model tumor evolution over time</li> <li>Identify driver pathways in progression</li> <li>Predict patient outcomes</li> <li>Discover therapeutic targets</li> </ul>"},{"location":"#computational-biology","title":"Computational Biology","text":"<ul> <li>Learn representations of high-dimensional genomics data</li> <li>Generate synthetic data for validation</li> <li>Integrate multi-omics datasets</li> <li>Perform pathway-level analysis</li> </ul>"},{"location":"#machine-learning-research","title":"Machine Learning Research","text":"<ul> <li>Apply VAEs to biological data</li> <li>Develop interpretable deep learning models</li> <li>Benchmark generative models</li> <li>Study latent space interpolation</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>renalprog</code> in your research, please cite:</p> <pre><code>@software{renalprog2025,\n  author = {Prol-Castelo, Guillermo and EVENFLOW Project},\n  title = {renalprog: Simulating Kidney Cancer Progression with Generative AI},\n  year = {2025},\n  publisher = {GitHub},\n  url = {https://github.com/gprolcastelo/renalprog}\n}\n</code></pre> <p>See How to Cite for additional references.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0 - see the LICENSE file for details.</p>"},{"location":"#support","title":"Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: Contact the EVENFLOW Project team</li> </ul>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This work is supported by the EVENFLOW Project and builds upon numerous open-source tools and databases. See Acknowledgments for complete credits.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> <li>Reproducibility Guide</li> <li>Contributing Guidelines</li> </ul>"},{"location":"CLASSIFICATION/","title":"Stage Classification Pipeline","text":"<p>This document describes the cancer stage classification pipeline in renalprog.</p>"},{"location":"CLASSIFICATION/#overview","title":"Overview","text":"<p>The classification pipeline trains and evaluates XGBoost models to classify cancer samples as early-stage or late-stage based on gene expression data. This is a critical component for validating synthetic trajectories generated by the VAE model.</p>"},{"location":"CLASSIFICATION/#purpose","title":"Purpose","text":"<p>Stage classification serves two main purposes in the renalprog pipeline:</p> <ol> <li>Validation: Verify that the VAE has learned meaningful representations that separate cancer stages</li> <li>Trajectory Assessment: Evaluate synthetic trajectories by classifying intermediate timepoints to track progression</li> </ol>"},{"location":"CLASSIFICATION/#pipeline-steps","title":"Pipeline Steps","text":""},{"location":"CLASSIFICATION/#1-data-preparation","title":"1. Data Preparation","text":"<pre><code>from renalprog.modeling.classification import prepare_classification_data\n\n# Prepare training and test sets\nX_train, X_test, y_train, y_test = prepare_classification_data(\n    rnaseq_path=\"data/interim/preprocessed_KIRC/rnaseq_preprocessed.csv\",\n    clinical_path=\"data/interim/preprocessed_KIRC/clinical.csv\",\n    test_size=0.2,\n    seed=2023\n)\n</code></pre>"},{"location":"CLASSIFICATION/#2-model-training","title":"2. Model Training","text":"<pre><code>from renalprog.modeling.classification import train_xgboost_classifier\n\n# Train XGBoost classifier\nmodel, metrics = train_xgboost_classifier(\n    X_train, y_train,\n    X_test, y_test,\n    output_dir=\"models/classification\"\n)\n\nprint(f\"Test Accuracy: {metrics['test_accuracy']:.3f}\")\nprint(f\"Test F1-Score: {metrics['test_f1']:.3f}\")\n</code></pre>"},{"location":"CLASSIFICATION/#3-feature-importance-analysis","title":"3. Feature Importance Analysis","text":"<pre><code>from renalprog.modeling.classification import get_feature_importance\n\n# Get top important genes\nimportance_df = get_feature_importance(model, feature_names=gene_names)\ntop_genes = importance_df.head(50)\n\n# Save for downstream analysis\ntop_genes.to_csv(\"data/external/important_genes_shap.csv\", index=False)\n</code></pre>"},{"location":"CLASSIFICATION/#4-model-evaluation","title":"4. Model Evaluation","text":"<p>The classifier is evaluated on: - Accuracy: Overall correct predictions - Precision: Correct positive predictions - Recall: Ability to find all positive cases - F1-Score: Harmonic mean of precision and recall - ROC-AUC: Area under the ROC curve - Confusion Matrix: True/false positives and negatives</p>"},{"location":"CLASSIFICATION/#5-trajectory-classification","title":"5. Trajectory Classification","text":"<p>Apply the trained classifier to synthetic trajectories:</p> <pre><code>from renalprog.modeling.classification import classify_trajectories\n\n# Classify each timepoint in trajectories\npredictions = classify_trajectories(\n    model=model,\n    trajectories=synthetic_trajectories,\n    threshold=0.5\n)\n\n# Analyze progression patterns\nprogression_rate = calculate_progression_rate(predictions)\n</code></pre>"},{"location":"CLASSIFICATION/#model-architecture","title":"Model Architecture","text":""},{"location":"CLASSIFICATION/#xgboost-configuration","title":"XGBoost Configuration","text":"<p>Default hyperparameters: <pre><code>{\n    'n_estimators': 100,\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'random_state': 2023\n}\n</code></pre></p>"},{"location":"CLASSIFICATION/#feature-selection","title":"Feature Selection","text":"<p>Two modes of operation:</p> <ol> <li>All Genes: Use complete gene expression profile (~20,000 genes)</li> <li>Important Genes: Use SHAP-selected important features (~500-1000 genes)</li> </ol> <p>Using important genes typically: - Improves interpretability - Reduces overfitting - Maintains comparable performance - Speeds up inference</p>"},{"location":"CLASSIFICATION/#output-files","title":"Output Files","text":"<p>The classification pipeline generates:</p> <pre><code>models/classification/\n\u251c\u2500\u2500 xgboost_model.json           # Trained XGBoost model\n\u251c\u2500\u2500 feature_importance.csv       # SHAP feature importance scores\n\u251c\u2500\u2500 classification_report.txt    # Detailed metrics\n\u251c\u2500\u2500 confusion_matrix.png        # Visualization\n\u251c\u2500\u2500 roc_curve.png               # ROC curve plot\n\u2514\u2500\u2500 training_history.csv        # Training metrics per epoch\n</code></pre>"},{"location":"CLASSIFICATION/#usage-example","title":"Usage Example","text":""},{"location":"CLASSIFICATION/#complete-classification-workflow","title":"Complete Classification Workflow","text":"<pre><code>from renalprog.modeling.classification import (\n    prepare_classification_data,\n    train_xgboost_classifier,\n    get_feature_importance,\n    classify_trajectories\n)\nfrom renalprog.config import get_dated_dir, MODELS_DIR\n\n# 1. Prepare data\nX_train, X_test, y_train, y_test = prepare_classification_data(\n    rnaseq_path=\"data/interim/preprocessed_KIRC/rnaseq_preprocessed.csv\",\n    clinical_path=\"data/interim/preprocessed_KIRC/clinical.csv\",\n    test_size=0.2,\n    seed=2023\n)\n\n# 2. Train model\noutput_dir = get_dated_dir(MODELS_DIR, \"classification_kirc\")\nmodel, metrics = train_xgboost_classifier(\n    X_train, y_train,\n    X_test, y_test,\n    output_dir=output_dir\n)\n\n# 3. Get important features\nimportance_df = get_feature_importance(model, X_train.columns)\nimportance_df.to_csv(f\"{output_dir}/feature_importance.csv\", index=False)\n\n# 4. Classify trajectories\ntrajectory_predictions = classify_trajectories(\n    model=model,\n    trajectories=synthetic_data,\n    threshold=0.5\n)\n\nprint(f\"Model saved to: {output_dir}\")\nprint(f\"Test F1-Score: {metrics['test_f1']:.3f}\")\n</code></pre>"},{"location":"CLASSIFICATION/#script-usage","title":"Script Usage","text":"<pre><code># Run classification pipeline\npython scripts/pipeline_steps/5_classification.py \\\n  --input_dir data/interim/20251216_train_test_split \\\n  --output_dir models/20251216_classification_kirc \\\n  --use_important_genes \\\n  --n_important 1000\n</code></pre>"},{"location":"CLASSIFICATION/#advanced-topics","title":"Advanced Topics","text":""},{"location":"CLASSIFICATION/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>For optimal performance, tune hyperparameters using cross-validation:</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [4, 6, 8],\n    'learning_rate': [0.01, 0.1, 0.3],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9]\n}\n\ngrid_search = GridSearchCV(\n    XGBClassifier(),\n    param_grid,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\nbest_model = grid_search.best_estimator_\n</code></pre>"},{"location":"CLASSIFICATION/#feature-selection-strategies","title":"Feature Selection Strategies","text":"<ol> <li>SHAP-based: Use SHAP values to identify important features</li> <li>Mutual Information: Information gain between features and target</li> <li>Recursive Feature Elimination: Iteratively remove least important features</li> <li>Variance Threshold: Remove low-variance features</li> </ol>"},{"location":"CLASSIFICATION/#handling-imbalanced-data","title":"Handling Imbalanced Data","text":"<p>If stage distribution is imbalanced:</p> <pre><code>from xgboost import XGBClassifier\n\n# Calculate scale_pos_weight\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n\nmodel = XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    # ... other parameters\n)\n</code></pre>"},{"location":"CLASSIFICATION/#see-also","title":"See Also","text":"<ul> <li>Trajectory Generation - Generate synthetic cancer trajectories</li> <li>Enrichment Analysis - Pathway enrichment on trajectories</li> <li>API Reference - Detailed API documentation</li> <li>Tutorial - Step-by-step classification tutorial</li> </ul>"},{"location":"CLASSIFICATION/#references","title":"References","text":"<ol> <li> <p>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22<sup>nd</sup> ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785-794).</p> </li> <li> <p>Lundberg, S. M., &amp; Lee, S. I. (2017). A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems (pp. 4765-4774).</p> </li> <li> <p>TCGA Research Network. (2013). Comprehensive molecular characterization of clear cell renal cell carcinoma. Nature, 499(7456), 43-49.</p> </li> </ol>"},{"location":"ENRICHMENT_ANALYSIS/","title":"Dynamic Enrichment Analysis","text":"<p>This document describes the dynamic enrichment analysis pipeline for renalprog.</p>"},{"location":"ENRICHMENT_ANALYSIS/#overview","title":"Overview","text":"<p>The enrichment analysis pipeline performs Gene Set Enrichment Analysis (GSEA) on synthetic cancer progression trajectories. This allows us to identify biological pathways that are enriched at different stages of progression.</p>"},{"location":"ENRICHMENT_ANALYSIS/#pipeline-steps","title":"Pipeline Steps","text":""},{"location":"ENRICHMENT_ANALYSIS/#1-deseq2-differential-expression-analysis","title":"1. DESeq2 Differential Expression Analysis","text":"<p>For each synthetic trajectory timepoint: 1. Load trajectory gene expression data (reverse log-transform from RSEM) 2. Load healthy control samples (reverse log-transform from RSEM) 3. Run PyDESeq2 differential expression analysis comparing trajectory vs controls 4. Extract log2 fold-change and adjusted p-values for each gene 5. Rank genes by log2 fold-change 6. Save ranked gene list (<code>.rnk</code> file) for GSEA</p> <p>Note: The pipeline uses PyDESeq2 for proper differential expression analysis, not simple fold-change calculations. This ensures statistical rigor and proper handling of count data variance.</p>"},{"location":"ENRICHMENT_ANALYSIS/#2-gsea-analysis","title":"2. GSEA Analysis","text":"<p>For each ranked gene list: 1. Run GSEA using preranked mode 2. Test against pathway database (ReactomePathways.gmt) 3. Calculate enrichment scores and FDR q-values 4. Generate positive and negative enrichment reports</p>"},{"location":"ENRICHMENT_ANALYSIS/#3-results-combination","title":"3. Results Combination","text":"<p>Combine all GSEA results into a single dataset: - One row per (patient, timepoint, pathway) - Includes enrichment score (ES), normalized ES (NES), and FDR q-value - Missing pathways filled with NaN values</p>"},{"location":"ENRICHMENT_ANALYSIS/#installation-requirements","title":"Installation Requirements","text":""},{"location":"ENRICHMENT_ANALYSIS/#python-dependencies","title":"Python Dependencies","text":"<p>The enrichment pipeline requires PyDESeq2 for differential expression analysis:</p> <pre><code>pip install pydeseq2\n</code></pre> <p>PyDESeq2 is a Python implementation of the DESeq2 method for differential expression analysis of count data.</p> <p>Citation: <pre><code>Muzellec, B., Telenczuk, M., &amp; Cabeli, V. (2022).\nPyDESeq2: a python package for bulk RNA-seq differential expression analysis.\nbioRxiv, 2022-12.\n</code></pre></p>"},{"location":"ENRICHMENT_ANALYSIS/#gsea-cli-tool","title":"GSEA CLI Tool","text":"<ol> <li>Download GSEA from: https://www.gsea-msigdb.org/gsea/downloads.jsp</li> <li>Extract to project root (creates <code>GSEA_4.3.2/</code> directory)</li> <li>Ensure <code>gsea-cli.sh</code> (Unix) or <code>gsea-cli.bat</code> (Windows) is executable</li> </ol> <p>Citation: <pre><code>Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., ... &amp; Mesirov, J. P. (2005).\nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles.\nProceedings of the National Academy of Sciences, 102(43), 15545-15550.\n</code></pre></p>"},{"location":"ENRICHMENT_ANALYSIS/#pathway-database","title":"Pathway Database","text":"<p>The ReactomePathways.gmt file is included in <code>data/external/ReactomePathways.gmt</code>.</p> <p>Citation: <pre><code>Jassal, B., Matthews, L., Viteri, G., Gong, C., Lorente, P., Fabregat, A., ... &amp; D'Eustachio, P. (2020).\nThe reactome pathway knowledgebase.\nNucleic acids research, 48(D1), D498-D503.\n</code></pre></p>"},{"location":"ENRICHMENT_ANALYSIS/#usage","title":"Usage","text":""},{"location":"ENRICHMENT_ANALYSIS/#basic-usage","title":"Basic Usage","text":"<pre><code>from renalprog.enrichment import EnrichmentPipeline\n\n# Initialize pipeline\npipeline = EnrichmentPipeline(\n    trajectory_dir='data/interim/20251216_synthetic_data/kirc/early_to_late',\n    output_dir='data/processed/20251217_enrichment',\n    cancer_type='kirc',\n    n_threads=8,\n    n_threads_per_deseq=8,  # Threads per DESeq2 analysis to prevent CPU over-subscription\n    gsea_path='./GSEA_4.3.2/gsea-cli.sh',\n    pathways_file='data/external/ReactomePathways.gmt'\n)\n\n# Run complete pipeline\nresults = pipeline.run()\n\n# Results are saved to: data/processed/20251217_enrichment/trajectory_enrichment.csv\n</code></pre>"},{"location":"ENRICHMENT_ANALYSIS/#parameter-tuning-for-parallel-processing","title":"Parameter Tuning for Parallel Processing","text":"<p>The pipeline supports parallel processing with careful control over CPU usage:</p> <ul> <li><code>n_threads</code>: Number of trajectory files to process in parallel (default: 4)</li> <li><code>n_threads_per_deseq</code>: Number of threads for each PyDESeq2 analysis (default: 8)</li> </ul> <p>Important: Total CPU usage = <code>n_threads \u00d7 n_threads_per_deseq</code>. Choose values based on your system:</p> <ul> <li>Local PC (8 cores): <code>n_threads=2, n_threads_per_deseq=4</code> \u2192 8 threads total</li> <li>Workstation (32 cores): <code>n_threads=4, n_threads_per_deseq=8</code> \u2192 32 threads total  </li> <li>HPC cluster (112 cores): <code>n_threads=14, n_threads_per_deseq=8</code> \u2192 112 threads total</li> </ul> <p>This prevents CPU over-subscription which can cause NODE_FAIL errors on SLURM clusters.</p>"},{"location":"ENRICHMENT_ANALYSIS/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Run complete pipeline\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/interim/20251216_synthetic_data/kirc/early_to_late \\\n    --output_dir data/processed/20251217_enrichment \\\n    --cancer_type kirc \\\n    --n_threads 8 \\\n    --gsea_path ./GSEA_4.3.2/gsea-cli.sh \\\n    --pathways_file data/external/ReactomePathways.gmt\n</code></pre>"},{"location":"ENRICHMENT_ANALYSIS/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Run steps separately\nfrom renalprog.enrichment import (\n    process_trajectories_for_deseq,\n    run_gsea_parallel,\n    combine_gsea_results\n)\n\n# Step 1: DESeq2 processing (reverse log-transform + differential expression)\nprocess_trajectories_for_deseq(\n    trajectory_dir='data/interim/20251216_synthetic_data/kirc/early_to_late',\n    output_dir='data/processed/20251217_enrichment',\n    cancer_type='kirc',\n    n_threads=8\n)\n\n# Step 2: Run GSEA\nrun_gsea_parallel(\n    deseq_dir='data/processed/20251217_enrichment/deseq',\n    n_threads=8\n)\n\n# Step 3: Combine results\nresults = combine_gsea_results(\n    deseq_dir='data/processed/20251217_enrichment/deseq',\n    pathways_file='data/external/ReactomePathways.gmt'\n)\n</code></pre> <p>Technical Details:</p> <p>The DESeq2 processing involves: 1. Reverse log-transformation: Input data is log-transformed RSEM values, which are converted back to counts 2. PyDESeq2 analysis: Proper variance modeling and statistical testing 3. Rank file generation: Genes ranked by log2 fold-change for GSEA preranked mode</p>"},{"location":"ENRICHMENT_ANALYSIS/#resume-from-checkpoint","title":"Resume from Checkpoint","text":"<p>If the pipeline fails or is interrupted, you can resume:</p> <pre><code># Skip DESeq if already completed\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/interim/20251216_synthetic_data/kirc/early_to_late \\\n    --output_dir data/processed/20251217_enrichment \\\n    --skip_deseq\n\n# Skip GSEA if already completed\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/interim/20251216_synthetic_data/kirc/early_to_late \\\n    --output_dir data/processed/20251217_enrichment \\\n    --skip_deseq \\\n    --skip_gsea\n</code></pre>"},{"location":"ENRICHMENT_ANALYSIS/#configuration","title":"Configuration","text":""},{"location":"ENRICHMENT_ANALYSIS/#thread-count","title":"Thread Count","text":"<p>The <code>n_threads</code> parameter controls parallelization: - Recommended: Number of CPU cores - 1 - Minimum: 1 (sequential processing, very slow) - Maximum: Number of CPU cores - Default: 4</p> <p>Example for different systems: <pre><code># Modern PC (8+ cores)\npipeline = EnrichmentPipeline(..., n_threads=6)\n\n# Older PC (4 cores)\npipeline = EnrichmentPipeline(..., n_threads=2)\n\n# High-performance cluster\npipeline = EnrichmentPipeline(..., n_threads=32)\n</code></pre></p>"},{"location":"ENRICHMENT_ANALYSIS/#gsea-parameters","title":"GSEA Parameters","text":"<p>Default GSEA parameters in <code>generate_gsea_command()</code>: - <code>collapse</code>: false (use all genes) - <code>nperm</code>: 1000 permutations - <code>set_max</code>: 500 (maximum pathway size) - <code>set_min</code>: 15 (minimum pathway size)</p> <p>To modify, edit <code>renalprog/modeling/enrichment.py</code>: <pre><code>def generate_gsea_command(...):\n    cmd = (\n        f'\"{gsea_path}\" GSEAPreranked '\n        f'-gmx \"{gmt_file}\" '\n        f'-rnk \"{rnk_file}\" '\n        f'-out \"{output_dir}\" '\n        f'-collapse false '\n        f'-nperm 2000 '  # Increase permutations\n        f'-set_max 1000 '  # Allow larger pathways\n        f'-set_min 10'  # Allow smaller pathways\n    )\n</code></pre></p>"},{"location":"ENRICHMENT_ANALYSIS/#output-format","title":"Output Format","text":""},{"location":"ENRICHMENT_ANALYSIS/#directory-structure","title":"Directory Structure","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 deseq/                          # DESeq results\n\u2502   \u251c\u2500\u2500 early_to_late/             # Transition type\n\u2502   \u2502   \u251c\u2500\u2500 patient1/              # Patient trajectory\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_tp0_foldchange.rnk\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_tp1_foldchange.rnk\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 gsea_tp0/          # GSEA output for timepoint 0\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 gsea_report_for_na_pos_*.tsv\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 gsea_report_for_na_neg_*.tsv\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 gsea_tp1/\n\u2502   \u2502   \u2514\u2500\u2500 patient2/\n\u2502   \u2514\u2500\u2500 gsea_commands_*.cmd        # GSEA command files\n\u251c\u2500\u2500 gsea/                           # GSEA working directory\n\u2514\u2500\u2500 trajectory_enrichment.csv       # Final combined results\n</code></pre>"},{"location":"ENRICHMENT_ANALYSIS/#final-results-format","title":"Final Results Format","text":"<p><code>trajectory_enrichment.csv</code> columns: - <code>Patient</code>: Patient identifier (e.g., \"TCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01\") - <code>Idx</code>: Timepoint index (0 to n_samples-1) - <code>Transition</code>: Transition type (e.g., \"early_to_late\") - <code>NAME</code>: Pathway name (from ReactomePathways.gmt) - <code>ES</code>: Enrichment score - <code>NES</code>: Normalized enrichment score - <code>FDR q-val</code>: False discovery rate q-value</p> <p>Example: <pre><code>Patient,Idx,Transition,NAME,ES,NES,FDR q-val\nTCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01,0,early_to_late,Cell Cycle,0.65,2.13,0.001\nTCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01,0,early_to_late,DNA Repair,0.52,1.87,0.012\nTCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01,1,early_to_late,Cell Cycle,0.71,2.31,0.000\n</code></pre></p>"},{"location":"ENRICHMENT_ANALYSIS/#performance","title":"Performance","text":""},{"location":"ENRICHMENT_ANALYSIS/#computational-requirements","title":"Computational Requirements","text":"<ul> <li>Memory: ~4 GB per thread</li> <li>Disk Space: ~10 GB for KIRC dataset (500+ trajectories)</li> <li>Time: ~2-6 hours for KIRC dataset (depends on threads)</li> </ul>"},{"location":"ENRICHMENT_ANALYSIS/#benchmarks","title":"Benchmarks","text":"<p>Testing on different hardware configurations:</p> System Cores RAM Trajectories Time Threads Used Modern Desktop (2023) 16 32 GB 500 2 hrs 12 Laptop (2020) 8 16 GB 500 4 hrs 6 Workstation (2018) 4 8 GB 500 6 hrs 3 HPC Cluster 64 256 GB 500 0.5 hrs 48"},{"location":"ENRICHMENT_ANALYSIS/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use SSD: GSEA creates many temporary files</li> <li>Adjust threads: Set to CPU cores - 1 for best performance</li> <li>Clean up: Use <code>--cleanup</code> flag to save disk space</li> <li>Resume: Use <code>--skip_deseq</code> or <code>--skip_gsea</code> to resume interrupted runs</li> </ol>"},{"location":"ENRICHMENT_ANALYSIS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ENRICHMENT_ANALYSIS/#cpu-over-subscription-node_fail-on-hpc","title":"CPU Over-Subscription / NODE_FAIL on HPC","text":"<p>Error: <code>NODE_FAIL</code> on SLURM, or system reports &gt;500% CPU usage</p> <p>Cause: Too many parallel processes using too many threads each</p> <p>Solution: The pipeline uses two levels of parallelization: 1. <code>n_threads</code>: Number of trajectory files processed in parallel 2. <code>n_threads_per_deseq</code>: Number of threads for each PyDESeq2 analysis</p> <p>Total CPU usage = <code>n_threads \u00d7 n_threads_per_deseq</code></p> <p>Fix: Adjust these parameters based on available cores:</p> <pre><code># For 112-core HPC node\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --n_threads 14 \\\n    --n_threads_per_deseq 8  # Total: 112 threads\n\n# For 32-core workstation\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --n_threads 4 \\\n    --n_threads_per_deseq 8  # Total: 32 threads\n\n# For 8-core PC\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --n_threads 2 \\\n    --n_threads_per_deseq 4  # Total: 8 threads\n</code></pre> <p>The pipeline sets environment variables (OMP_NUM_THREADS, MKL_NUM_THREADS, etc.) to limit threading per process.</p>"},{"location":"ENRICHMENT_ANALYSIS/#gsea-not-found","title":"GSEA Not Found","text":"<p>Error: <code>GSEA CLI not found at ./GSEA_4.3.2/gsea-cli.sh</code></p> <p>Solution: 1. Download GSEA from https://www.gsea-msigdb.org/gsea/downloads.jsp 2. Extract to project root 3. Or specify custom path with <code>--gsea_path</code></p>"},{"location":"ENRICHMENT_ANALYSIS/#memory-errors","title":"Memory Errors","text":"<p>Error: <code>OutOfMemoryError</code> or system freezes</p> <p>Solution: 1. Reduce <code>n_threads</code> 2. Process trajectories in batches 3. Increase system swap space</p>"},{"location":"ENRICHMENT_ANALYSIS/#gsea-command-failures","title":"GSEA Command Failures","text":"<p>Error: GSEA commands fail with non-zero exit code</p> <p>Solution: 1. Check GSEA installation 2. Verify pathway file format (GMT) 3. Check file permissions 4. Review GSEA log files in output directories</p>"},{"location":"ENRICHMENT_ANALYSIS/#missing-pathways","title":"Missing Pathways","text":"<p>Warning: Some pathways have all NaN values</p> <p>Explanation: Normal - not all pathways are significant in every sample</p>"},{"location":"ENRICHMENT_ANALYSIS/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>Error: Cannot run <code>gsea-cli.sh</code> on Windows</p> <p>Solution: 1. Install Git Bash or WSL (Windows Subsystem for Linux) 2. Use <code>gsea-cli.bat</code> instead if available 3. Or run in WSL environment</p>"},{"location":"ENRICHMENT_ANALYSIS/#references","title":"References","text":"<ol> <li>GSEA Method:</li> <li>Subramanian et al. (2005). \"Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles.\" PNAS 102(43):15545-15550.</li> <li> <p>https://www.gsea-msigdb.org/</p> </li> <li> <p>PyDESeq2:</p> </li> <li>Muzellec, B., Telenczuk, M., &amp; Cabeli, V. (2022). \"PyDESeq2: a python package for bulk RNA-seq differential expression analysis.\" bioRxiv, 2022-12.</li> <li> <p>https://github.com/owkin/PyDESeq2</p> </li> <li> <p>DESeq2 Original Method:</p> </li> <li> <p>Love, M.I., Huber, W., Anders, S. (2014). \"Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2.\" Genome Biology 15(12):550.</p> </li> <li> <p>Reactome Pathways:</p> </li> <li>Jassal et al. (2020). \"The reactome pathway knowledgebase.\" Nucleic Acids Research 48(D1):D498-D503.</li> <li> <p>https://reactome.org/</p> </li> <li> <p>Original Implementation:</p> </li> <li>Prol-Castelo, G. (2024). My_BRCA repository</li> <li>Files: <code>src_deseq_and_gsea_NCSR/py_deseq.py</code>, <code>trajectory_analysis.py</code></li> </ol>"},{"location":"ENRICHMENT_ANALYSIS/#see-also","title":"See Also","text":"<ul> <li>Classification Pipeline</li> <li>Trajectory Generation</li> <li>R Analysis Scripts</li> </ul>"},{"location":"ENRICHMENT_QUICK_REF/","title":"Enrichment Analysis Quick Reference","text":"<p>Quick reference guide for trajectory-based enrichment analysis.</p>"},{"location":"ENRICHMENT_QUICK_REF/#prerequisites","title":"Prerequisites","text":"<pre><code># Install PyDESeq2\npip install pydeseq2\n\n# Download GSEA CLI tool\n# From: https://www.gsea-msigdb.org/gsea/downloads.jsp\n# Extract to project root\n</code></pre>"},{"location":"ENRICHMENT_QUICK_REF/#quick-start","title":"Quick Start","text":""},{"location":"ENRICHMENT_QUICK_REF/#1-run-complete-pipeline","title":"1. Run Complete Pipeline","text":"<pre><code>python scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/processed/20251216_synthetic_data/kirc/early_to_late \\\n    --output_dir data/processed/20251219_enrichment \\\n    --cancer_type kirc \\\n    --n_threads 8 \\\n    --gsea_path ./GSEA_4.3.2/gsea-cli.sh \\\n    --pathways_file data/external/ReactomePathways.gmt\n</code></pre>"},{"location":"ENRICHMENT_QUICK_REF/#2-generate-pathway-heatmaps","title":"2. Generate Pathway Heatmaps","text":"<pre><code>python scripts/pipeline_steps/6b_generate_pathway_heatmap.py \\\n    --enrichment_file data/processed/20251219_enrichment/trajectory_enrichment.csv \\\n    --output_dir data/processed/20251219_enrichment \\\n    --fdr_threshold 0.05\n</code></pre>"},{"location":"ENRICHMENT_QUICK_REF/#python-api","title":"Python API","text":""},{"location":"ENRICHMENT_QUICK_REF/#basic-usage","title":"Basic Usage","text":"<pre><code>from renalprog.enrichment import EnrichmentPipeline\n\npipeline = EnrichmentPipeline(\n    trajectory_dir='data/processed/trajectories',\n    output_dir='data/processed/enrichment',\n    cancer_type='kirc',\n    n_threads=8\n)\n\nresults = pipeline.run()\n</code></pre>"},{"location":"ENRICHMENT_QUICK_REF/#generate-heatmaps","title":"Generate Heatmaps","text":"<pre><code>from renalprog.enrichment import generate_pathway_heatmap\n\nheatmap_data, figures = generate_pathway_heatmap(\n    enrichment_file='data/processed/enrichment/trajectory_enrichment.csv',\n    output_dir='data/processed/enrichment',\n    fdr_threshold=0.05\n)\n</code></pre>"},{"location":"ENRICHMENT_QUICK_REF/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>DESeq2 Analysis: Differential expression using PyDESeq2</li> <li>Reverse log-transform RSEM data</li> <li>Compare trajectory vs healthy controls</li> <li> <p>Generate ranked gene lists</p> </li> <li> <p>GSEA Execution: Pathway enrichment analysis</p> </li> <li>Run GSEAPreranked in parallel</li> <li>Test against Reactome pathways</li> <li> <p>Calculate enrichment scores and FDR</p> </li> <li> <p>Results Combination: Aggregate all results</p> </li> <li>Combine all timepoints and trajectories</li> <li>Filter by FDR threshold</li> <li> <p>Save to CSV</p> </li> <li> <p>Visualization: Generate heatmaps</p> </li> <li>Top varying pathways</li> <li>Top upregulated/downregulated</li> <li>Selected high-level and literature pathways</li> </ol>"},{"location":"ENRICHMENT_QUICK_REF/#output-files","title":"Output Files","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 deseq/                              # DESeq2 results\n\u2502   \u2514\u2500\u2500 early_to_late/\n\u2502       \u2514\u2500\u2500 patient_id/\n\u2502           \u251c\u2500\u2500 *.rnk                   # Ranked gene lists\n\u2502           \u2514\u2500\u2500 gsea_*/                 # GSEA results\n\u251c\u2500\u2500 trajectory_enrichment.csv           # Combined results\n\u2514\u2500\u2500 pathway_heatmaps/                   # Heatmap visualizations\n    \u251c\u2500\u2500 top_varying_pathways.pdf\n    \u251c\u2500\u2500 top_upregulated_pathways.pdf\n    \u251c\u2500\u2500 top_downregulated_pathways.pdf\n    \u251c\u2500\u2500 high_level_pathways.pdf\n    \u2514\u2500\u2500 literature_pathways.pdf\n</code></pre>"},{"location":"ENRICHMENT_QUICK_REF/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>n_threads</code> Parallel processing threads 4 <code>fdr_threshold</code> FDR q-value cutoff 0.05 <code>gsea_nperm</code> GSEA permutations 1000 <code>set_min</code> Min pathway size 15 <code>set_max</code> Max pathway size 500"},{"location":"ENRICHMENT_QUICK_REF/#common-issues","title":"Common Issues","text":""},{"location":"ENRICHMENT_QUICK_REF/#memory-errors","title":"Memory errors","text":"<ul> <li>Reduce <code>n_threads</code></li> <li>Process in smaller batches</li> </ul>"},{"location":"ENRICHMENT_QUICK_REF/#gsea-not-found","title":"GSEA not found","text":"<ul> <li>Check <code>gsea_path</code> points to correct location</li> <li>Ensure execute permissions on gsea-cli.sh</li> </ul>"},{"location":"ENRICHMENT_QUICK_REF/#no-significant-pathways","title":"No significant pathways","text":"<ul> <li>Lower <code>fdr_threshold</code></li> <li>Check input data quality</li> <li>Verify pathway database matches gene IDs</li> </ul>"},{"location":"ENRICHMENT_QUICK_REF/#see-also","title":"See Also","text":"<ul> <li>Full Documentation</li> <li>API Reference</li> <li>Trajectory Generation</li> </ul>"},{"location":"GSEA_INSTALLATION/","title":"GSEA Installation Guide","text":"<p>This guide provides instructions for installing and configuring GSEA (Gene Set Enrichment Analysis) for use with renalprog.</p>"},{"location":"GSEA_INSTALLATION/#what-is-gsea","title":"What is GSEA?","text":"<p>GSEA is a computational method that determines whether a priori defined sets of genes show statistically significant, concordant differences between two biological states.</p> <p>Citation: <pre><code>Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., ... &amp; Mesirov, J. P. (2005).\nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles.\nProceedings of the National Academy of Sciences, 102(43), 15545-15550.\nDOI: 10.1073/pnas.0506580102\n</code></pre></p>"},{"location":"GSEA_INSTALLATION/#installation","title":"Installation","text":""},{"location":"GSEA_INSTALLATION/#step-1-download-gsea","title":"Step 1: Download GSEA","text":"<ol> <li>Visit the GSEA downloads page:</li> <li> <p>https://www.gsea-msigdb.org/gsea/downloads.jsp</p> </li> <li> <p>Register for an account (free for academic use)</p> </li> <li> <p>Download the latest version of GSEA:</p> </li> <li>For Windows/Mac/Linux: Download \"GSEA_X.X.X.zip\" (X.X.X = version number)</li> <li>Recommended version: GSEA 4.3.2 or later</li> </ol>"},{"location":"GSEA_INSTALLATION/#step-2-extract-gsea","title":"Step 2: Extract GSEA","text":"<ol> <li> <p>Extract the downloaded ZIP file to your renalprog project root:    <pre><code>renalprog/\n\u251c\u2500\u2500 GSEA_4.3.2/          # \u2190 Extract here\n\u2502   \u251c\u2500\u2500 gsea-cli.sh      # Unix/Linux/Mac\n\u2502   \u251c\u2500\u2500 gsea-cli.bat     # Windows\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 data/\n\u251c\u2500\u2500 renalprog/\n\u2514\u2500\u2500 ...\n</code></pre></p> </li> <li> <p>The extraction should create a folder named <code>GSEA_X.X.X/</code> (e.g., <code>GSEA_4.3.2/</code>)</p> </li> </ol>"},{"location":"GSEA_INSTALLATION/#step-3-make-executable-unixlinuxmac","title":"Step 3: Make Executable (Unix/Linux/Mac)","text":"<pre><code>cd GSEA_4.3.2\nchmod +x gsea-cli.sh\n</code></pre>"},{"location":"GSEA_INSTALLATION/#step-4-test-installation","title":"Step 4: Test Installation","text":""},{"location":"GSEA_INSTALLATION/#on-unixlinuxmac","title":"On Unix/Linux/Mac:","text":"<pre><code>./GSEA_4.3.2/gsea-cli.sh --help\n</code></pre>"},{"location":"GSEA_INSTALLATION/#on-windows-git-bash-or-wsl","title":"On Windows (Git Bash or WSL):","text":"<pre><code>bash GSEA_4.3.2/gsea-cli.sh --help\n</code></pre>"},{"location":"GSEA_INSTALLATION/#on-windows-command-prompt","title":"On Windows (Command Prompt):","text":"<pre><code>GSEA_4.3.2\\gsea-cli.bat --help\n</code></pre> <p>You should see the GSEA help message if installation was successful.</p>"},{"location":"GSEA_INSTALLATION/#system-requirements","title":"System Requirements","text":""},{"location":"GSEA_INSTALLATION/#java","title":"Java","text":"<p>GSEA requires Java 11 or later.</p> <p>Check Java version: <pre><code>java -version\n</code></pre></p> <p>If Java is not installed:</p> <ul> <li>Windows: Download from https://adoptium.net/</li> <li>Mac: <code>brew install openjdk@11</code></li> <li>Ubuntu/Debian: <code>sudo apt-get install openjdk-11-jdk</code></li> <li>CentOS/RHEL: <code>sudo yum install java-11-openjdk</code></li> </ul>"},{"location":"GSEA_INSTALLATION/#memory","title":"Memory","text":"<p>GSEA can be memory-intensive. Recommended: - Minimum: 4 GB RAM - Recommended: 8 GB RAM per parallel thread - Large datasets: 16+ GB RAM</p> <p>To increase GSEA memory limit, edit the GSEA configuration file or set Java heap size.</p>"},{"location":"GSEA_INSTALLATION/#pathway-databases","title":"Pathway Databases","text":""},{"location":"GSEA_INSTALLATION/#reactomepathwaysgmt","title":"ReactomePathways.gmt","text":"<p>The renalprog package includes ReactomePathways.gmt in <code>data/external/</code>.</p> <p>Citation: <pre><code>Jassal, B., Matthews, L., Viteri, G., Gong, C., Lorente, P., Fabregat, A., ... &amp; D'Eustachio, P. (2020).\nThe reactome pathway knowledgebase.\nNucleic acids research, 48(D1), D498-D503.\nDOI: 10.1093/nar/gkz1031\n</code></pre></p>"},{"location":"GSEA_INSTALLATION/#additional-gmt-files","title":"Additional GMT Files","text":"<p>You can download additional gene set databases from MSigDB: - https://www.gsea-msigdb.org/gsea/msigdb/collections.jsp</p> <p>Available collections: - H: Hallmark gene sets (50 gene sets) - C1: Positional gene sets (326 gene sets) - C2: Curated gene sets (&gt;6000 gene sets)   - CGP: Chemical and genetic perturbations   - CP: Canonical pathways (includes KEGG, Reactome, BioCarta) - C3: Regulatory target gene sets - C4: Computational gene sets - C5: Ontology gene sets (GO) - C6: Oncogenic signature gene sets - C7: Immunologic signature gene sets - C8: Cell type signature gene sets</p> <p>Place downloaded GMT files in <code>data/external/</code> and reference them with the <code>--pathways_file</code> argument.</p>"},{"location":"GSEA_INSTALLATION/#configuration","title":"Configuration","text":""},{"location":"GSEA_INSTALLATION/#default-configuration","title":"Default Configuration","text":"<p>Renalprog uses these GSEA parameters by default:</p> <pre><code># In renalprog/modeling/enrichment.py\ngsea_params = {\n    'collapse': 'false',      # Don't collapse probe sets\n    'nperm': 1000,            # Number of permutations\n    'set_max': 500,           # Maximum gene set size\n    'set_min': 15,            # Minimum gene set size\n    'scoring_scheme': 'weighted',  # Scoring scheme\n    'mode': 'Max_probe',      # Normalization mode\n}\n</code></pre>"},{"location":"GSEA_INSTALLATION/#custom-configuration","title":"Custom Configuration","text":"<p>To modify GSEA parameters, edit <code>renalprog/modeling/enrichment.py</code>:</p> <pre><code>def generate_gsea_command(\n    gsea_path: Path,\n    rnk_file: Path,\n    gmt_file: Path,\n    output_dir: Path\n) -&gt; str:\n    cmd = (\n        f'\"{gsea_path}\" GSEAPreranked '\n        f'-gmx \"{gmt_file}\" '\n        f'-rnk \"{rnk_file}\" '\n        f'-out \"{output_dir}\" '\n        f'-collapse false '\n        f'-nperm 2000 '        # \u2190 Increase permutations\n        f'-set_max 1000 '      # \u2190 Allow larger gene sets\n        f'-set_min 10 '        # \u2190 Allow smaller gene sets\n        f'-scoring_scheme weighted '\n    )\n    return cmd\n</code></pre>"},{"location":"GSEA_INSTALLATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"GSEA_INSTALLATION/#gsea-cli-not-found","title":"\"GSEA CLI not found\"","text":"<p>Problem: Script cannot find GSEA installation</p> <p>Solution: 1. Verify GSEA is extracted to project root 2. Check the folder name matches <code>GSEA_X.X.X/</code> 3. Specify custom path: <code>--gsea_path /path/to/gsea-cli.sh</code></p>"},{"location":"GSEA_INSTALLATION/#java-not-found-or-unsupportedclassversionerror","title":"\"Java not found\" or \"UnsupportedClassVersionError\"","text":"<p>Problem: Java is not installed or version is too old</p> <p>Solution: 1. Install Java 11 or later 2. Update PATH environment variable 3. Set JAVA_HOME environment variable</p>"},{"location":"GSEA_INSTALLATION/#outofmemoryerror","title":"\"OutOfMemoryError\"","text":"<p>Problem: GSEA runs out of memory</p> <p>Solution 1: Increase Java heap size <pre><code># Edit gsea-cli.sh and modify:\njava -Xmx8g -jar gsea.jar  # 8 GB heap\n</code></pre></p> <p>Solution 2: Reduce parallel threads <pre><code># Use fewer threads\npython scripts/pipeline_steps/6_enrichment_analysis.py ... --n_threads 2\n</code></pre></p> <p>Solution 3: Process in batches <pre><code># Process subset of trajectories\nfrom pathlib import Path\n\ntrajectory_files = list(Path('data/interim/trajectories').glob('*.csv'))\n\n# Process in chunks\nchunk_size = 50\nfor i in range(0, len(trajectory_files), chunk_size):\n    chunk = trajectory_files[i:i+chunk_size]\n    # Process chunk...\n</code></pre></p>"},{"location":"GSEA_INSTALLATION/#permission-denied-unixlinuxmac","title":"\"Permission denied\" (Unix/Linux/Mac)","text":"<p>Problem: GSEA script is not executable</p> <p>Solution: <pre><code>chmod +x GSEA_4.3.2/gsea-cli.sh\n</code></pre></p>"},{"location":"GSEA_INSTALLATION/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>Problem: Cannot run <code>gsea-cli.sh</code> on Windows</p> <p>Solution 1: Use Git Bash <pre><code># Install Git for Windows (includes Git Bash)\n# Then run:\nbash GSEA_4.3.2/gsea-cli.sh --help\n</code></pre></p> <p>Solution 2: Use WSL (Windows Subsystem for Linux) <pre><code># Install WSL\nwsl --install\n\n# Then in WSL terminal:\n./GSEA_4.3.2/gsea-cli.sh --help\n</code></pre></p> <p>Solution 3: Use batch file (if available) <pre><code>GSEA_4.3.2\\gsea-cli.bat --help\n</code></pre></p>"},{"location":"GSEA_INSTALLATION/#no-such-file-or-directory-when-running-gsea-commands","title":"\"No such file or directory\" when running GSEA commands","text":"<p>Problem: Path contains spaces or special characters</p> <p>Solution: Use quotes in paths <pre><code># In enrichment.py, paths are already quoted:\ncmd = f'\"{gsea_path}\" GSEAPreranked -gmx \"{gmt_file}\" ...'\n</code></pre></p>"},{"location":"GSEA_INSTALLATION/#alternative-using-gseapy-python-implementation","title":"Alternative: Using GSEApy (Python Implementation)","text":"<p>If you cannot install GSEA CLI, you can modify renalprog to use GSEApy instead:</p> <pre><code>pip install gseapy\n</code></pre> <p>Note: Results may differ slightly from official GSEA CLI. The CLI version is considered the gold standard.</p> <p>To switch to GSEApy, modify <code>renalprog/modeling/enrichment.py</code>:</p> <pre><code>import gseapy as gp\n\ndef run_gsea_python(rnk_file, gmt_file, output_dir):\n    \"\"\"Alternative GSEA using Python.\"\"\"\n    pre_res = gp.prerank(\n        rnk=rnk_file,\n        gene_sets=gmt_file,\n        outdir=output_dir,\n        permutation_num=1000,\n        min_size=15,\n        max_size=500,\n    )\n    return pre_res\n</code></pre>"},{"location":"GSEA_INSTALLATION/#license-and-citation","title":"License and Citation","text":""},{"location":"GSEA_INSTALLATION/#gsea-license","title":"GSEA License","text":"<p>GSEA software is distributed under a custom license: - Free for academic and non-profit use - Commercial use requires a license - See: https://www.gsea-msigdb.org/gsea/login.jsp</p>"},{"location":"GSEA_INSTALLATION/#required-citations","title":"Required Citations","text":"<p>If you use GSEA in your research, please cite:</p> <p>Primary Citation: <pre><code>Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., ... &amp; Mesirov, J. P. (2005).\nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles.\nProceedings of the National Academy of Sciences, 102(43), 15545-15550.\n</code></pre></p> <p>If using MSigDB gene sets: <pre><code>Liberzon, A., Birger, C., Thorvaldsd\u00f3ttir, H., Ghandi, M., Mesirov, J. P., &amp; Tamayo, P. (2015).\nThe molecular signatures database hallmark gene set collection.\nCell systems, 1(6), 417-425.\n</code></pre></p> <p>If using Reactome pathways: <pre><code>Jassal, B., Matthews, L., Viteri, G., Gong, C., Lorente, P., Fabregat, A., ... &amp; D'Eustachio, P. (2020).\nThe reactome pathway knowledgebase.\nNucleic acids research, 48(D1), D498-D503.\n</code></pre></p>"},{"location":"GSEA_INSTALLATION/#additional-resources","title":"Additional Resources","text":"<ul> <li>GSEA User Guide: https://www.gsea-msigdb.org/gsea/doc/GSEAUserGuideFrame.html</li> <li>MSigDB User Guide: https://www.gsea-msigdb.org/gsea/msigdb/</li> <li>GSEA Forum: https://groups.google.com/g/gsea-help</li> <li>Reactome: https://reactome.org/</li> <li>GSEApy Documentation: https://gseapy.readthedocs.io/</li> </ul>"},{"location":"GSEA_INSTALLATION/#contact","title":"Contact","text":"<p>For renalprog-specific issues, please open an issue on GitHub.</p> <p>For GSEA software issues, contact the GSEA team through their forum or email.</p>"},{"location":"PATHWAY_HEATMAP/","title":"Pathway Enrichment Heatmap","text":""},{"location":"PATHWAY_HEATMAP/#overview","title":"Overview","text":"<p>The pathway enrichment heatmap visualization summarizes GSEA results across all trajectories and timepoints. The pipeline generates five different heatmaps to provide comprehensive views of pathway dynamics during cancer progression.</p>"},{"location":"PATHWAY_HEATMAP/#heatmap-types","title":"Heatmap Types","text":""},{"location":"PATHWAY_HEATMAP/#1-top-varying-pathways","title":"1. Top Varying Pathways","text":"<p>Shows the 50 pathways with the highest variance between first and last timepoints.</p> <ul> <li>Purpose: Identify pathways that change most dramatically during progression</li> <li>Colormap: RdBu_r (red-white-blue diverging)</li> <li>Interpretation: Pathways with largest temporal dynamics</li> </ul>"},{"location":"PATHWAY_HEATMAP/#2-top-upregulated-pathways","title":"2. Top Upregulated Pathways","text":"<p>Shows the 50 pathways with highest average NES (most consistently upregulated).</p> <ul> <li>Purpose: Identify pathways activated during progression</li> <li>Colormap: YlGn (yellow-green sequential)</li> <li>Interpretation: Consistently activated pathways across progression</li> </ul>"},{"location":"PATHWAY_HEATMAP/#3-top-downregulated-pathways","title":"3. Top Downregulated Pathways","text":"<p>Shows the 50 pathways with lowest average NES (most consistently downregulated).</p> <ul> <li>Purpose: Identify pathways suppressed during progression</li> <li>Colormap: YlOrBr (yellow-orange-brown sequential)</li> <li>Interpretation: Consistently suppressed pathways across progression</li> </ul>"},{"location":"PATHWAY_HEATMAP/#4-high-level-pathways","title":"4. High-Level Pathways","text":"<p>Shows 29 Reactome high-level pathways (top-level categories).</p> <ul> <li>Purpose: Get broad overview of biological processes</li> <li>Pathways: Cell Cycle, DNA Repair, Immune System, Metabolism, Signal Transduction, etc.</li> <li>Colormap: RdBu_r</li> <li>Interpretation: System-level view of cancer progression</li> </ul>"},{"location":"PATHWAY_HEATMAP/#5-literature-pathways","title":"5. Literature Pathways","text":"<p>Shows 33 pathways from kidney cancer literature.</p> <ul> <li>Purpose: Focus on pathways known to be important in kidney cancer</li> <li>Pathways: VHL/HIF pathway, PI3K/AKT/MTOR, Warburg effect, TCA cycle, etc.</li> <li>Colormap: RdBu_r</li> <li>Interpretation: Validation of known biology and discovery of new patterns</li> </ul>"},{"location":"PATHWAY_HEATMAP/#what-the-heatmaps-show","title":"What the Heatmaps Show","text":"<ul> <li>Rows: Pathway names</li> <li>Columns: Pseudotime progression (50 timepoints from early to late)</li> <li>Values: Sum of NES (Normalized Enrichment Score) across all patients at each timepoint</li> <li>Colors: </li> <li>Red/Green/Brown (high values): Pathway enriched/upregulated</li> <li>Blue/Yellow (low values): Pathway depleted/downregulated</li> <li>White (middle): NES near zero or not significant</li> </ul>"},{"location":"PATHWAY_HEATMAP/#filtering-criteria","title":"Filtering criteria","text":"<p>Only pathways meeting these criteria are included:</p> <ol> <li>FDR q-value &lt; 0.05 (default, configurable)</li> <li>At least one significant enrichment across all trajectories</li> </ol>"},{"location":"PATHWAY_HEATMAP/#interpretation","title":"Interpretation","text":""},{"location":"PATHWAY_HEATMAP/#temporal-dynamics","title":"Temporal Dynamics","text":"<p>For each pathway, the heatmap shows how enrichment changes across pseudotime:</p> <ul> <li>Consistently high across time: Pathway remains activated throughout progression</li> <li>Increasing over time: Pathway becomes more activated as disease progresses</li> <li>Decreasing over time: Pathway becomes less activated/more suppressed</li> <li>Transient changes: Pathway activated/suppressed at specific stages</li> </ul>"},{"location":"PATHWAY_HEATMAP/#example-interpretations","title":"Example Interpretations","text":"<ol> <li>Cell Cycle pathway shows increasing upregulation:</li> <li>Increased proliferation during disease progression</li> <li> <p>Consistent with cancer biology</p> </li> <li> <p>DNA Repair pathway shows decreasing activity:</p> </li> <li>Loss of DNA repair capacity</li> <li> <p>May enable accumulation of mutations</p> </li> <li> <p>Immune System pathway varies across time:</p> </li> <li>Complex immune response dynamics</li> <li> <p>May indicate immune escape mechanisms</p> </li> <li> <p>Metabolic pathways (Warburg effect):</p> </li> <li>Early activation suggests metabolic reprogramming</li> <li>Known hallmark of cancer</li> </ol>"},{"location":"PATHWAY_HEATMAP/#files-generated","title":"Files Generated","text":"<p>When you run the pathway heatmap generation (Step 6b), the following files are created:</p>"},{"location":"PATHWAY_HEATMAP/#data-file","title":"Data File","text":"<ul> <li>pathway_heatmap_data.csv: Matrix of NES values (pathways \u00d7 timepoints)</li> </ul>"},{"location":"PATHWAY_HEATMAP/#visualization-files-for-each-heatmap-type","title":"Visualization Files (for each heatmap type)","text":"<ol> <li>top_varying_pathways.[pdf/png/svg]: Top 50 most varying pathways</li> <li>top_upregulated_pathways.[pdf/png/svg]: Top 50 upregulated pathways</li> <li>top_downregulated_pathways.[pdf/png/svg]: Top 50 downregulated pathways</li> <li>high_level_pathways.[pdf/png/svg]: 29 Reactome high-level pathways</li> <li>literature_pathways.[pdf/png/svg]: 33 kidney cancer literature pathways</li> </ol> <p>All heatmaps are saved in PDF (publication quality), PNG (high-res), and SVG (vector) formats.</p>"},{"location":"PATHWAY_HEATMAP/#usage","title":"Usage","text":""},{"location":"PATHWAY_HEATMAP/#generate-heatmaps-from-enrichment-results","title":"Generate Heatmaps from Enrichment Results","text":"<pre><code>python scripts/pipeline_steps/6b_generate_pathway_heatmap.py \\\n    --enrichment_file data/processed/20251219_enrichment/trajectory_enrichment.csv \\\n    --output_dir data/processed/20251219_enrichment \\\n    --fdr_threshold 0.05\n</code></pre>"},{"location":"PATHWAY_HEATMAP/#parameters","title":"Parameters","text":"<ul> <li><code>--enrichment_file</code>: Path to combined enrichment results CSV</li> <li><code>--output_dir</code>: Directory to save heatmaps</li> <li><code>--fdr_threshold</code>: FDR q-value cutoff (default: 0.05)</li> </ul>"},{"location":"PATHWAY_HEATMAP/#python-api","title":"Python API","text":"<pre><code>from renalprog.enrichment import generate_pathway_heatmap\n\nheatmap_data, figures_dict = generate_pathway_heatmap(\n    enrichment_file='data/processed/enrichment/trajectory_enrichment.csv',\n    output_dir='data/processed/enrichment',\n    fdr_threshold=0.05\n)\n\n# figures_dict contains:\n# - 'top_varying': Figure object\n# - 'top_upregulated': Figure object\n# - 'top_downregulated': Figure object\n# - 'high_level': Figure object\n# - 'literature': Figure object\n</code></pre>"},{"location":"PATHWAY_HEATMAP/#implementation-details","title":"Implementation Details","text":"<p>The heatmaps are generated by:</p> <ol> <li>Filtering: Keep only results with FDR q-val &lt; threshold</li> <li>Grouping: Group by Pathway name and Timepoint index</li> <li>Aggregation: Sum NES values across all patients at each timepoint</li> <li>Pathway Selection:</li> <li>Top varying: Calculate variance between first and last timepoint, select top 50</li> <li>Top upregulated: Calculate mean NES, select top 50 positive</li> <li>Top downregulated: Calculate mean NES, select top 50 negative</li> <li>High-level: Filter for 29 predefined Reactome pathways</li> <li>Literature: Filter for 33 kidney cancer-related pathways</li> <li>Pivoting: Create matrix with pathways as rows, timepoints as columns</li> <li>Visualization: Create heatmap with appropriate colormap and styling</li> </ol>"},{"location":"PATHWAY_HEATMAP/#technical-notes","title":"Technical Notes","text":"<ul> <li>Uses PyDESeq2 for differential expression (not simple fold-change)</li> <li>RSEM data is reverse log-transformed before DESeq2 analysis</li> <li>Each trajectory timepoint compared against healthy controls</li> <li>NES values summed across all patients for each (pathway, timepoint) pair</li> </ul>"},{"location":"PATHWAY_HEATMAP/#tips-for-analysis","title":"Tips for Analysis","text":"<ol> <li>Compare heatmap types: Look for pathways appearing in multiple heatmaps</li> <li>Temporal patterns: Identify early vs late activation/suppression</li> <li>Literature validation: Check if known pathways show expected patterns</li> <li>Novel discoveries: Look for unexpected pathway dynamics in top varying</li> <li>Biological context: Use high-level pathways for systems-level understanding</li> <li>Publication: Use vector formats (SVG/PDF) for manuscript figures</li> </ol>"},{"location":"PATHWAY_HEATMAP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PATHWAY_HEATMAP/#no-significant-pathways-found","title":"No significant pathways found","text":"<p>If heatmaps are empty or have few pathways:</p> <ul> <li>Check FDR threshold (try 0.1 or 0.25)</li> <li>Verify GSEA ran successfully for all trajectories</li> <li>Check that PyDESeq2 analysis completed without errors</li> <li>Ensure trajectory data has sufficient dynamic range</li> </ul>"},{"location":"PATHWAY_HEATMAP/#missing-literaturehigh-level-pathways","title":"Missing literature/high-level pathways","text":"<p>If specific pathway categories are missing:</p> <ul> <li>Pathway names must match exactly (case-sensitive)</li> <li>Check pathway database (ReactomePathways.gmt) contains these pathways</li> <li>Pathways may not be significant at current FDR threshold</li> </ul>"},{"location":"PATHWAY_HEATMAP/#memory-errors-during-generation","title":"Memory errors during generation","text":"<p>If heatmap generation fails:</p> <ul> <li>Large datasets may require more memory</li> <li>Process heatmaps individually if needed</li> <li>Use lower resolution for initial exploration</li> </ul>"},{"location":"PATHWAY_HEATMAP/#related-functions","title":"Related Functions","text":"<ul> <li><code>generate_pathway_heatmap()</code>: Main function in <code>renalprog.enrichment</code></li> <li><code>plot_heatmap_regulation()</code>: Low-level plotting function</li> <li><code>EnrichmentPipeline.run()</code>: Full enrichment pipeline</li> <li>Script: <code>scripts/pipeline_steps/6_enrichment_analysis.py</code></li> <li>Script: <code>scripts/pipeline_steps/6b_generate_pathway_heatmap.py</code></li> </ul>"},{"location":"PATHWAY_HEATMAP/#references","title":"References","text":"<ul> <li>PyDESeq2: Muzellec et al. (2022). PyDESeq2: a python package for bulk RNA-seq differential expression analysis.</li> <li>GSEA: Subramanian et al. (2005). Gene set enrichment analysis. PNAS 102(43):15545-15550.</li> <li>Reactome: Jassal et al. (2020). The reactome pathway knowledgebase. Nucleic Acids Research 48(D1):D498-D503.</li> <li>Original implementation: Prol-Castelo, G. (2024). My_BRCA repository.</li> </ul>"},{"location":"TRAJECTORIES/","title":"Trajectory Generation Pipeline","text":"<p>This document describes the synthetic cancer progression trajectory generation pipeline in renalprog.</p>"},{"location":"TRAJECTORIES/#overview","title":"Overview","text":"<p>The trajectory generation pipeline creates synthetic cancer progression paths by interpolating between early-stage and late-stage cancer samples in the VAE's latent space. This allows us to model intermediate states of cancer progression that may not be directly observable in the data.</p>"},{"location":"TRAJECTORIES/#biological-motivation","title":"Biological Motivation","text":"<p>Cancer progression is a continuous process, but clinical data typically captures discrete snapshots (stages I-IV). Synthetic trajectories help us:</p> <ol> <li>Model Intermediate States: Generate hypothetical intermediate stages between observed samples</li> <li>Identify Progression Markers: Find genes and pathways that change during progression</li> <li>Understand Dynamics: Study the temporal ordering of molecular events</li> <li>Generate Hypotheses: Predict which pathways are activated during progression</li> </ol>"},{"location":"TRAJECTORIES/#pipeline-steps","title":"Pipeline Steps","text":""},{"location":"TRAJECTORIES/#1-patient-connection-creation","title":"1. Patient Connection Creation","text":"<p>Identify pairs of patients to connect:</p> <pre><code>from renalprog.modeling.trajectories import create_patient_connections\n\nconnections = create_patient_connections(\n    clinical_df=clinical_data,\n    source_stage=\"early\",  # Stage I-II\n    target_stage=\"late\",   # Stage III-IV\n    method=\"all_to_all\"    # Connect every early to every late\n)\n\n# Each connection: (source_patient_id, target_patient_id)\n</code></pre> <p>Connection strategies: - all_to_all: Every early patient \u2192 every late patient - nearest_neighbor: Early patients \u2192 nearest late patients in latent space - matched: Match on clinical covariates (age, sex, etc.)</p>"},{"location":"TRAJECTORIES/#2-latent-space-interpolation","title":"2. Latent Space Interpolation","text":"<p>Generate intermediate points in VAE latent space:</p> <pre><code>from renalprog.modeling.predict import generate_trajectories\n\ntrajectories = generate_trajectories(\n    model=vae_model,\n    source_samples=early_stage_data,\n    target_samples=late_stage_data,\n    n_steps=50,           # Number of intermediate timepoints\n    method=\"linear\"       # Interpolation method\n)\n\n# Output shape: (n_trajectories, n_steps, n_genes)\n</code></pre> <p>Interpolation methods: - linear: Straight line in latent space (default) - spherical: Interpolate along geodesic on hypersphere - bezier: Smooth curve using Bezier interpolation</p>"},{"location":"TRAJECTORIES/#3-gene-expression-reconstruction","title":"3. Gene Expression Reconstruction","text":"<p>Decode latent trajectories to gene expression space:</p> <pre><code>from renalprog.modeling.predict import reconstruct_from_latent\n\n# For each timepoint, decode latent vector to gene expression\ngene_expression = []\nfor t in range(n_steps):\n    z_t = trajectories[:, t, :]  # Latent vectors at time t\n    x_t = vae_model.decode(z_t)  # Reconstruct gene expression\n    gene_expression.append(x_t)\n\n# Shape: (n_trajectories, n_steps, n_genes)\ngene_expression = np.stack(gene_expression, axis=1)\n</code></pre>"},{"location":"TRAJECTORIES/#4-quality-control","title":"4. Quality Control","text":"<p>Validate trajectory quality:</p> <pre><code>from renalprog.modeling.trajectories import validate_trajectories\n\nquality_metrics = validate_trajectories(\n    trajectories=gene_expression,\n    early_samples=X_early,\n    late_samples=X_late,\n    vae_model=vae_model\n)\n\n# Metrics:\n# - reconstruction_error: How well VAE reconstructs samples\n# - monotonicity: Do progression markers increase/decrease consistently?\n# - smoothness: Are trajectories smooth or erratic?\n# - stage_separation: Do endpoints match early/late distributions?\n</code></pre>"},{"location":"TRAJECTORIES/#5-control-trajectory-generation","title":"5. Control Trajectory Generation","text":"<p>Generate negative controls for comparison:</p> <pre><code>from renalprog.modeling.trajectories import generate_control_trajectories\n\n# Random noise trajectories\nnoise_controls = generate_control_trajectories(\n    n_trajectories=len(trajectories),\n    n_steps=n_steps,\n    n_genes=n_genes,\n    method=\"gaussian_noise\",\n    noise_scale=1.0\n)\n\n# Shuffled gene trajectories\nshuffled_controls = generate_control_trajectories(\n    trajectories=trajectories,\n    method=\"shuffle_genes\"\n)\n</code></pre> <p>Control types: - gaussian_noise: Random Gaussian noise - shuffle_genes: Shuffle gene order independently - shuffle_time: Randomize temporal ordering - reverse: Reverse early\u2192late to late\u2192early</p>"},{"location":"TRAJECTORIES/#output-files","title":"Output Files","text":"<p>The trajectory pipeline generates:</p> <pre><code>data/interim/trajectories/\n\u251c\u2500\u2500 early_to_late/\n\u2502   \u251c\u2500\u2500 trajectory_metadata.csv      # Patient pairs, stages\n\u2502   \u251c\u2500\u2500 latent_trajectories.npy      # Latent space paths\n\u2502   \u251c\u2500\u2500 gene_expression.npy          # Reconstructed expression\n\u2502   \u2514\u2500\u2500 quality_metrics.json         # Validation metrics\n\u251c\u2500\u2500 controls/\n\u2502   \u251c\u2500\u2500 noise_controls.npy           # Random noise\n\u2502   \u2514\u2500\u2500 shuffled_controls.npy        # Shuffled genes\n\u2514\u2500\u2500 visualizations/\n    \u251c\u2500\u2500 latent_space_trajectories.png\n    \u251c\u2500\u2500 sample_trajectories.png\n    \u2514\u2500\u2500 progression_heatmap.png\n</code></pre>"},{"location":"TRAJECTORIES/#usage-example","title":"Usage Example","text":""},{"location":"TRAJECTORIES/#complete-trajectory-workflow","title":"Complete Trajectory Workflow","text":"<pre><code>from renalprog.modeling.predict import generate_trajectories\nfrom renalprog.modeling.trajectories import (\n    create_patient_connections,\n    validate_trajectories,\n    generate_control_trajectories\n)\nfrom renalprog.config import get_dated_dir, INTERIM_DATA_DIR\nimport numpy as np\n\n# 1. Load trained VAE\nfrom renalprog.modeling.train import load_vae\nvae = load_vae(\"models/20251216_VAE_KIRC/best_model.pth\")\n\n# 2. Load data\nX_early = np.load(\"data/interim/samples/early_stage.npy\")\nX_late = np.load(\"data/interim/samples/late_stage.npy\")\n\n# 3. Create connections\nconnections = create_patient_connections(\n    clinical_df=clinical,\n    source_stage=\"early\",\n    target_stage=\"late\",\n    method=\"all_to_all\"\n)\n\n# 4. Generate trajectories\ntrajectories = generate_trajectories(\n    model=vae,\n    source_samples=X_early,\n    target_samples=X_late,\n    n_steps=50,\n    method=\"linear\"\n)\n\n# 5. Validate quality\nmetrics = validate_trajectories(\n    trajectories=trajectories,\n    early_samples=X_early,\n    late_samples=X_late,\n    vae_model=vae\n)\n\n# 6. Generate controls\ncontrols = generate_control_trajectories(\n    n_trajectories=len(trajectories),\n    n_steps=50,\n    n_genes=trajectories.shape[-1],\n    method=\"gaussian_noise\"\n)\n\n# 7. Save results\noutput_dir = get_dated_dir(INTERIM_DATA_DIR, \"synthetic_trajectories\")\nnp.save(f\"{output_dir}/trajectories.npy\", trajectories)\nnp.save(f\"{output_dir}/controls.npy\", controls)\n\nprint(f\"Generated {len(trajectories)} trajectories\")\nprint(f\"Quality metrics: {metrics}\")\n</code></pre>"},{"location":"TRAJECTORIES/#script-usage","title":"Script Usage","text":"<pre><code># Run trajectory generation pipeline\npython scripts/pipeline_steps/4_trajectories.py \\\n  --vae_model models/20251216_VAE_KIRC/best_model.pth \\\n  --data_dir data/interim/20251216_train_test_split \\\n  --output_dir data/interim/20251216_synthetic_trajectories \\\n  --n_steps 50 \\\n  --method linear \\\n  --generate_controls\n</code></pre>"},{"location":"TRAJECTORIES/#visualization","title":"Visualization","text":""},{"location":"TRAJECTORIES/#latent-space-trajectories","title":"Latent Space Trajectories","text":"<p>Visualize trajectories in 2D latent space (using UMAP/t-SNE):</p> <pre><code>from renalprog.plots import plot_latent_trajectories\n\nfig = plot_latent_trajectories(\n    latent_trajectories=z_trajectories,\n    early_latent=z_early,\n    late_latent=z_late,\n    reduction_method=\"umap\"\n)\nfig.savefig(\"reports/figures/latent_trajectories.png\")\n</code></pre>"},{"location":"TRAJECTORIES/#gene-expression-heatmaps","title":"Gene Expression Heatmaps","text":"<p>Show how genes change along trajectories:</p> <pre><code>from renalprog.plots import plot_trajectory_heatmap\n\nfig = plot_trajectory_heatmap(\n    trajectories=trajectories[:10],  # First 10 trajectories\n    gene_names=important_genes,\n    n_genes_show=50\n)\nfig.savefig(\"reports/figures/trajectory_heatmap.png\")\n</code></pre>"},{"location":"TRAJECTORIES/#individual-trajectory-plots","title":"Individual Trajectory Plots","text":"<p>Plot selected genes over time:</p> <pre><code>from renalprog.plots import plot_individual_trajectory\n\ngenes_of_interest = [\"VHL\", \"PBRM1\", \"SETD2\", \"BAP1\"]\n\nfig = plot_individual_trajectory(\n    trajectory=trajectories[0],\n    gene_names=all_genes,\n    highlight_genes=genes_of_interest\n)\nfig.savefig(\"reports/figures/individual_trajectory.png\")\n</code></pre>"},{"location":"TRAJECTORIES/#advanced-topics","title":"Advanced Topics","text":""},{"location":"TRAJECTORIES/#custom-interpolation-methods","title":"Custom Interpolation Methods","text":"<p>Implement custom interpolation:</p> <pre><code>from renalprog.modeling.trajectories import BaseInterpolator\n\nclass CustomInterpolator(BaseInterpolator):\n    def interpolate(self, z_start, z_end, n_steps):\n        \"\"\"Custom interpolation between latent vectors.\"\"\"\n        # Your custom interpolation logic\n        # For example, cubic spline:\n        from scipy.interpolate import CubicSpline\n\n        t = np.linspace(0, 1, n_steps)\n        cs = CubicSpline([0, 1], np.vstack([z_start, z_end]))\n        return cs(t)\n\n# Use custom interpolator\ntrajectories = generate_trajectories(\n    model=vae,\n    source_samples=X_early,\n    target_samples=X_late,\n    n_steps=50,\n    interpolator=CustomInterpolator()\n)\n</code></pre>"},{"location":"TRAJECTORIES/#trajectory-clustering","title":"Trajectory Clustering","text":"<p>Identify different progression patterns:</p> <pre><code>from sklearn.cluster import KMeans\nfrom renalprog.modeling.trajectories import extract_trajectory_features\n\n# Extract features from trajectories\nfeatures = extract_trajectory_features(\n    trajectories,\n    method=\"slope\"  # Gene expression slopes\n)\n\n# Cluster trajectories\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(features)\n\n# Analyze each cluster\nfor i in range(3):\n    cluster_trajs = trajectories[clusters == i]\n    print(f\"Cluster {i}: {len(cluster_trajs)} trajectories\")\n</code></pre>"},{"location":"TRAJECTORIES/#multi-stage-trajectories","title":"Multi-Stage Trajectories","text":"<p>Generate trajectories across multiple stages:</p> <pre><code># Generate trajectories: Stage I \u2192 Stage II \u2192 Stage III \u2192 Stage IV\nstages = [\"stage_i\", \"stage_ii\", \"stage_iii\", \"stage_iv\"]\nmulti_stage_trajs = []\n\nfor i in range(len(stages) - 1):\n    trajs = generate_trajectories(\n        model=vae,\n        source_samples=samples[stages[i]],\n        target_samples=samples[stages[i+1]],\n        n_steps=20,\n        method=\"linear\"\n    )\n    multi_stage_trajs.append(trajs)\n\n# Concatenate all segments\nfull_trajectories = np.concatenate(multi_stage_trajs, axis=1)\n</code></pre>"},{"location":"TRAJECTORIES/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"TRAJECTORIES/#what-do-trajectories-represent","title":"What Do Trajectories Represent?","text":"<ul> <li>Not Real Patients: Trajectories are hypothetical, model-generated paths</li> <li>Biological Plausibility: Validated by enrichment analysis and clinical markers</li> <li>Statistical Averages: Represent general trends, not individual variation</li> <li>Hypothesis Generation: Use to identify candidates for experimental validation</li> </ul>"},{"location":"TRAJECTORIES/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Over-interpretation: Don't assume every trajectory detail is biologically meaningful</li> <li>Model Artifacts: VAE may introduce interpolation artifacts</li> <li>Missing Biology: Model can't capture unknown mechanisms</li> <li>Data Bias: Trajectories reflect training data distributions</li> </ol>"},{"location":"TRAJECTORIES/#best-practices","title":"Best Practices","text":"<ol> <li>Generate Controls: Always compare to random/shuffled controls</li> <li>Validate Endpoints: Ensure trajectories start/end at correct stages</li> <li>Check Monotonicity: Progression markers should change consistently</li> <li>Cross-validate: Compare to independent validation datasets</li> <li>Biological Validation: Prioritize findings that match known biology</li> </ol>"},{"location":"TRAJECTORIES/#performance-considerations","title":"Performance Considerations","text":""},{"location":"TRAJECTORIES/#memory-usage","title":"Memory Usage","text":"<p>For large datasets: <pre><code># Generate trajectories in batches\nbatch_size = 100\nall_trajectories = []\n\nfor i in range(0, len(connections), batch_size):\n    batch_connections = connections[i:i+batch_size]\n    batch_trajs = generate_trajectories(\n        model=vae,\n        source_samples=X_early[batch_connections[:, 0]],\n        target_samples=X_late[batch_connections[:, 1]],\n        n_steps=50\n    )\n    all_trajectories.append(batch_trajs)\n\ntrajectories = np.concatenate(all_trajectories, axis=0)\n</code></pre></p>"},{"location":"TRAJECTORIES/#computational-speed","title":"Computational Speed","text":"<ul> <li>Linear interpolation: Fast (~1-10 ms per trajectory)</li> <li>Spherical interpolation: Medium (~10-50 ms per trajectory)</li> <li>Bezier interpolation: Slow (~50-200 ms per trajectory)</li> </ul> <p>For 1000 trajectories with 50 steps: - Linear: ~10 seconds - Spherical: ~1 minute - Bezier: ~5 minutes</p>"},{"location":"TRAJECTORIES/#see-also","title":"See Also","text":"<ul> <li>VAE Training - Train the VAE model</li> <li>Classification - Classify trajectory timepoints</li> <li>Enrichment Analysis - Find enriched pathways</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"TRAJECTORIES/#references","title":"References","text":"<ol> <li> <p>White, T. (2016). Sampling Generative Networks. arXiv preprint arXiv:1609.04468.</p> </li> <li> <p>Shoemake, K. (1985). Animating rotation with quaternion curves. ACM SIGGRAPH Computer Graphics, 19(3), 245-254.</p> </li> <li> <p>Way, G. P., &amp; Greene, C. S. (2018). Extracting a biologically relevant latent space from cancer transcriptomes with variational autoencoders. Pacific Symposium on Biocomputing, 23, 80-91.</p> </li> <li> <p>Ding, J., et al. (2018). Interpretable dimensionality reduction of single cell transcriptome data with deep generative models. Nature Communications, 9(1), 1-13.</p> </li> </ol>"},{"location":"acknowledgments/","title":"Acknowledgments","text":""},{"location":"acknowledgments/#funding","title":"Funding","text":"<p>This work was supported by the EU project EVENFLOW under Horizon Europe agreement No. 101070430.</p>"},{"location":"acknowledgments/#research-institutions","title":"Research Institutions","text":"<p>We gratefully acknowledge the Barcelona Supercomputing Center - Centro Nacional de Supercomputaci\u00f3n (BSC-CNS) for providing access to high-performance computing resources that were essential for the development and validation of this framework, for their continued support in computational biology research and for fostering an environment of scientific excellence.</p> <p>We thank NCSR Demokritos for their support and collaboration in this project.</p>"},{"location":"acknowledgments/#data-sources","title":"Data Sources","text":"<p>We acknowledge the following data sources:</p> <ul> <li>The Cancer Genome Atlas (TCGA) for providing the cancer genomics data</li> <li>Reactome for pathway annotations</li> <li>MSigDB for gene set collections (KEGG pathways)</li> </ul>"},{"location":"acknowledgments/#open-source-community","title":"Open Source Community","text":"<p>This project builds upon numerous open-source Python libraries and tools. We thank the developers and maintainers of:</p> <ul> <li>PyTorch and the deep learning community</li> <li>scikit-learn and the machine learning community</li> <li>pandas, numpy, and the scientific Python ecosystem</li> <li>GSEA (Gene Set Enrichment Analysis) tools</li> <li>All other dependencies listed in our requirements</li> </ul>"},{"location":"acknowledgments/#design-attribution","title":"Design Attribution","text":"<p>The repository logo is: Kidneys icons created by Smashicons - Flaticon</p> <p>The views and opinions expressed in this documentation are those of the authors and do not necessarily reflect the official policy or position of the EVENFLOW project or the Barcelona Supercomputing Center.</p>"},{"location":"citation/","title":"Citation","text":"<p>Temporary Citation</p> <p>This citation is temporary and will be updated once the preprint/publication is available.</p> <p>Please check back regularly for updates, or watch the GitHub repository for announcements.</p>"},{"location":"citation/#current-citation-temporary","title":"Current Citation (Temporary)","text":"<p>Warning</p> <p>This citation is temporary and will be updated once the preprint publication is available.</p> <p>If you use RenalProg in your research, please cite it as follows until the official publication is available:</p> <pre><code>@software{renalprog2024,\n  title = {RenalProg: A Deep Learning Framework for Kidney Cancer Progression Modeling},\n  author = {[Guillermo Prol-Castelo, Elina Syrri, Nikolaos Manginas, Vasileos Manginas, Nikos Katzouris, Davide Cirillo, George Paliouras, Alfonso Valencia]},\n  year = {2025},\n  url = {https://github.com/gprolcas/renalprog},\n  note = {Preprint in preparation}\n}\n</code></pre>"},{"location":"citation/#updates","title":"Updates","text":"<p>Last updated: December 2025</p> <p>Status: Preprint in preparation</p> <p>Expected publication: 2026</p> <p>Stay Updated</p> <p>To receive notifications when the official publication is available:</p> <ul> <li>\u2b50 Star the GitHub repository</li> <li>\ud83d\udc40 Watch for releases</li> <li>\ud83d\udce7 Follow project updates</li> </ul> <p>This page will be updated immediately upon preprint/publication release.</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the <code>sync_data_from_s3</code> command, for example), and then how to make the cleaned, final data sets.</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#apache-license-20","title":"Apache License 2.0","text":"<p>RenalProg is licensed under the Apache License, Version 2.0.</p> <p>The full license text is available in the LICENSE file in the repository.</p>"},{"location":"license/#citation-requirement","title":"Citation Requirement","text":"<p>While not legally required by the Apache 2.0 License, we kindly request that you cite this work in any publications or presentations that use RenalProg. See the Citation page for details.</p>"},{"location":"license/#third-party-components","title":"Third-Party Components","text":"<p>RenalProg includes or depends on several third-party libraries and tools, each with their own licenses.</p>"},{"location":"license/#contributing","title":"Contributing","text":"<p>By contributing to RenalProg, you agree that your contributions will be licensed under the Apache License 2.0. See the Contributing Guidelines for more information.</p> <p>Last updated: December 2025</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API reference for the <code>renalprog</code> package. All functions, classes, and modules are documented here.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>renalprog/\n\u251c\u2500\u2500 __init__.py              # Package initialization\n\u251c\u2500\u2500 config.py                # Configuration and paths\n\u251c\u2500\u2500 dataset.py               # Data loading and splitting\n\u251c\u2500\u2500 features.py              # Preprocessing and feature engineering\n\u251c\u2500\u2500 plots.py                 # Visualization functions\n\u251c\u2500\u2500 modeling/                # Machine learning models\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 models.py           # VAE architectures\n\u2502   \u251c\u2500\u2500 train.py            # Training functions\n\u2502   \u251c\u2500\u2500 predict.py          # Prediction and inference\n\u2502   \u251c\u2500\u2500 trajectories.py     # Trajectory generation\n\u2502   \u251c\u2500\u2500 classification.py   # XGBoost classification\n\u2502   \u2514\u2500\u2500 enrichment.py       # GSEA integration\n\u2514\u2500\u2500 utils/                   # Utility functions\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 logging.py          # Logging configuration\n    \u2514\u2500\u2500 helpers.py          # Helper functions\n</code></pre>"},{"location":"api/#quick-links","title":"Quick Links","text":""},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li>Configuration - Paths and settings</li> <li>Dataset - Data loading and processing</li> <li>Features - Preprocessing and filtering</li> </ul>"},{"location":"api/#modeling","title":"Modeling","text":"<ul> <li>VAE Models - Variational autoencoders</li> <li>Training - Model training</li> <li>Prediction - Inference and generation</li> <li>Trajectories - Synthetic progression</li> <li>Classification - Stage prediction</li> <li>Enrichment - Pathway analysis</li> </ul>"},{"location":"api/#visualization","title":"Visualization","text":"<ul> <li>Plots - All plotting functions</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<ul> <li>Utils - Helper functions and logging</li> </ul>"},{"location":"api/#installation","title":"Installation","text":"<pre><code>pip install renalprog\n</code></pre> <p>Or for development:</p> <pre><code>git clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\npip install -e .\n</code></pre>"},{"location":"api/#basic-usage","title":"Basic Usage","text":""},{"location":"api/#import-the-package","title":"Import the Package","text":"<pre><code>import renalprog\nfrom renalprog import config, dataset, features, modeling, plots\n</code></pre>"},{"location":"api/#load-data","title":"Load Data","text":"<pre><code># Load preprocessed data\nrnaseq = dataset.load_data('data/processed/rnaseq_maha.csv')\nclinical = dataset.load_data('data/processed/clinical.csv')\n\n# Create train/test split\nX_train, X_test, y_train, y_test = dataset.create_train_test_split(\n    rnaseq_path='data/processed/rnaseq_maha.csv',\n    clinical_path='data/processed/clinical.csv',\n    test_size=0.2,\n    seed=2023\n)\n</code></pre>"},{"location":"api/#train-a-model","title":"Train a Model","text":"<pre><code>from renalprog.modeling import VAE, train_vae\n\n# Initialize VAE\nvae = VAE(\n    input_dim=X_train.shape[1],\n    latent_dim=256,\n    hidden_dims=[512, 256]\n)\n\n# Train\nvae.fit(X_train, epochs=100, batch_size=32)\n\n# Save\nvae.save('models/my_vae.pt')\n</code></pre>"},{"location":"api/#generate-trajectories","title":"Generate Trajectories","text":"<pre><code>from renalprog.modeling import generate_trajectories\n\ntrajectories = generate_trajectories(\n    vae=vae,\n    start_samples=early_stage_samples,\n    end_samples=late_stage_samples,\n    n_trajectories=100,\n    n_timepoints=20\n)\n</code></pre>"},{"location":"api/#run-classification","title":"Run Classification","text":"<pre><code>from renalprog.modeling.classification import train_stage_classifier\n\nclf, metrics, shap_values = train_stage_classifier(\n    X=X_train,\n    y=y_train,\n    feature_names=gene_names,\n    n_folds=5\n)\n</code></pre>"},{"location":"api/#visualize","title":"Visualize","text":"<pre><code>from renalprog import plots\n\n# Plot latent space\nplots.plot_latent_space(\n    latent_data,\n    color_by='stage',\n    method='umap',\n    save_path='figures/latent_umap.png'\n)\n\n# Plot training history\nplots.plot_training_history(\n    history,\n    save_path='figures/training.png'\n)\n</code></pre>"},{"location":"api/#module-details","title":"Module Details","text":""},{"location":"api/#renalprogconfig","title":"renalprog.config","text":"<p>Configuration module with paths and constants.</p> <p>Key Components:</p> <ul> <li><code>PATHS</code>: Dictionary of all project paths</li> <li><code>VAEConfig</code>: VAE hyperparameter configuration</li> <li><code>get_dated_dir()</code>: Create dated output directories</li> <li><code>KIRCPaths</code>: KIRC-specific data paths</li> </ul> <p>Example: <pre><code>from renalprog.config import PATHS, VAEConfig\n\n# Access paths\ndata_dir = PATHS['data']\nmodels_dir = PATHS['models']\n\n# Configure VAE\nconfig = VAEConfig()\nconfig.LATENT_DIM = 256\nconfig.EPOCHS = 600\n</code></pre></p> <p>See: Configuration Reference</p>"},{"location":"api/#renalprogdataset","title":"renalprog.dataset","text":"<p>Data loading, processing, and splitting utilities.</p> <p>Key Functions:</p> <ul> <li><code>download_data()</code>: Download TCGA data from Xena</li> <li><code>process_downloaded_data()</code>: Process raw TCGA files</li> <li><code>load_data()</code>: Load CSV files</li> <li><code>create_train_test_split()</code>: Stratified train/test split</li> </ul> <p>Example: <pre><code>from renalprog import dataset\n\n# Download data\nrnaseq, clinical, pheno = dataset.download_data(\n    destination='data/raw',\n    cancer_type='KIRC'\n)\n\n# Create split\nX_train, X_test, y_train, y_test = dataset.create_train_test_split(\n    rnaseq_path='data/processed/rnaseq.csv',\n    clinical_path='data/processed/clinical.csv',\n    test_size=0.2,\n    seed=2023\n)\n</code></pre></p> <p>See: Dataset Reference</p>"},{"location":"api/#renalprogfeatures","title":"renalprog.features","text":"<p>Preprocessing and feature engineering.</p> <p>Key Functions:</p> <ul> <li><code>preprocess_rnaseq()</code>: Filter and normalize gene expression</li> <li><code>filter_low_expression()</code>: Remove lowly expressed genes</li> <li><code>detect_outliers()</code>: Mahalanobis distance outlier detection</li> <li><code>normalize()</code>: Various normalization methods</li> </ul> <p>Example: <pre><code>from renalprog import features\n\n# Preprocess RNA-seq data\nprocessed, info = features.preprocess_rnaseq(\n    data=rnaseq,\n    filter_expression=True,\n    detect_outliers=True,\n    mean_threshold=0.5,\n    var_threshold=0.5,\n    alpha=0.05\n)\n</code></pre></p> <p>See: Features Reference</p>"},{"location":"api/#renalprogmodeling","title":"renalprog.modeling","text":"<p>Machine learning models and training.</p> <p>Submodules:</p> <ul> <li><code>models</code>: VAE architectures</li> <li><code>train</code>: Training functions</li> <li><code>predict</code>: Inference and generation</li> <li><code>trajectories</code>: Synthetic trajectory generation</li> <li><code>classification</code>: XGBoost stage classification</li> <li><code>enrichment</code>: GSEA integration</li> </ul> <p>Example: <pre><code>from renalprog.modeling import VAE, train_vae, generate_trajectories\n\n# Train VAE\nvae, history = train_vae(\n    X_train=X_train,\n    X_test=X_test,\n    config=vae_config\n)\n\n# Generate trajectories\ntrajectories = generate_trajectories(\n    vae=vae,\n    start_samples=early_samples,\n    end_samples=late_samples,\n    n_trajectories=100\n)\n</code></pre></p> <p>See: - Models Reference - Training Reference - Trajectories Reference - Classification Reference - Enrichment Reference</p>"},{"location":"api/#renalprogplots","title":"renalprog.plots","text":"<p>Visualization functions for all analysis steps.</p> <p>Key Functions:</p> <ul> <li><code>plot_latent_space()</code>: UMAP/t-SNE visualization</li> <li><code>plot_training_history()</code>: Loss curves</li> <li><code>plot_reconstruction()</code>: Original vs reconstructed</li> <li><code>plot_trajectories()</code>: Trajectory visualization</li> <li><code>plot_classification_metrics()</code>: ROC, confusion matrix</li> <li><code>plot_enrichment_heatmap()</code>: Pathway enrichment</li> </ul> <p>Example: <pre><code>from renalprog import plots\n\n# Visualize latent space\nplots.plot_latent_space(\n    latent_data,\n    color_by='stage',\n    method='umap',\n    save_path='figures/latent.png',\n    figsize=(10, 8),\n    title='VAE Latent Space'\n)\n</code></pre></p> <p>See: Plots Reference</p>"},{"location":"api/#renalprogutils","title":"renalprog.utils","text":"<p>Utility functions and helpers.</p> <p>Key Functions:</p> <ul> <li><code>configure_logging()</code>: Set up logging</li> <li><code>set_seed()</code>: Set random seeds for reproducibility</li> <li><code>Timer</code>: Context manager for timing code</li> <li><code>check_file_exists()</code>: File validation</li> </ul> <p>Example: <pre><code>from renalprog.utils import configure_logging, set_seed, Timer\n\n# Configure logging\nconfigure_logging(level='INFO')\n\n# Set random seed\nset_seed(2023)\n\n# Time code execution\nwith Timer(\"Training\"):\n    vae.fit(X_train, epochs=100)\n</code></pre></p> <p>See: Utils Reference</p>"},{"location":"api/#advanced-topics","title":"Advanced Topics","text":""},{"location":"api/#custom-vae-architectures","title":"Custom VAE Architectures","text":"<pre><code>from renalprog.modeling.models import BaseVAE\nimport torch.nn as nn\n\nclass CustomVAE(BaseVAE):\n    def __init__(self, input_dim, latent_dim):\n        super().__init__(input_dim, latent_dim)\n\n        # Custom encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n        )\n\n        self.fc_mu = nn.Linear(512, latent_dim)\n        self.fc_logvar = nn.Linear(512, latent_dim)\n\n        # Custom decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, input_dim)\n        )\n</code></pre>"},{"location":"api/#custom-preprocessing","title":"Custom Preprocessing","text":"<pre><code>from renalprog.features import preprocess_rnaseq\n\ndef custom_preprocess(data, **kwargs):\n    \"\"\"Custom preprocessing pipeline.\"\"\"\n\n    # Step 1: Log transform\n    data_log = np.log2(data + 1)\n\n    # Step 2: Quantile normalization\n    data_norm = quantile_normalize(data_log)\n\n    # Step 3: Standard preprocessing\n    data_processed, info = preprocess_rnaseq(\n        data_norm,\n        **kwargs\n    )\n\n    return data_processed, info\n</code></pre>"},{"location":"api/#batch-processing","title":"Batch Processing","text":"<pre><code>from renalprog.modeling import generate_trajectories\nimport glob\n\n# Process multiple experiments\nfor experiment_dir in glob.glob('data/interim/experiment_*'):\n    vae = VAE.load(f'{experiment_dir}/vae_model.pt')\n\n    trajectories = generate_trajectories(\n        vae=vae,\n        start_samples=early,\n        end_samples=late,\n        n_trajectories=100\n    )\n\n    trajectories.to_csv(f'{experiment_dir}/trajectories.csv')\n</code></pre>"},{"location":"api/#type-hints","title":"Type Hints","text":"<p>The package uses type hints throughout:</p> <pre><code>from typing import Tuple, Optional, Dict, List\nimport pandas as pd\nimport numpy as np\nimport torch\n\ndef preprocess_rnaseq(\n    data: pd.DataFrame,\n    filter_expression: bool = True,\n    mean_threshold: float = 0.5,\n    var_threshold: float = 0.5,\n    detect_outliers: bool = True,\n    alpha: float = 0.05,\n    seed: Optional[int] = None\n) -&gt; Tuple[pd.DataFrame, Dict[str, any]]:\n    \"\"\"Preprocess RNA-seq data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>Common exceptions:</p> <pre><code>from renalprog.dataset import load_data\n\ntry:\n    data = load_data('nonexistent_file.csv')\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\nexcept pd.errors.EmptyDataError:\n    print(\"File is empty\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/#performance","title":"Performance","text":""},{"location":"api/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Use GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nvae = VAE(\n    input_dim=5000,\n    latent_dim=256,\n    device=device\n)\n</code></pre>"},{"location":"api/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from renalprog.enrichment import run_gsea_parallel\n\n# Use multiple cores\nrun_gsea_parallel(\n    deseq_dir='data/processed/deseq',\n    n_threads=8\n)\n</code></pre>"},{"location":"api/#testing","title":"Testing","text":"<p>Run the test suite:</p> <pre><code># All tests\npytest\n\n# Specific module\npytest tests/test_dataset.py\n\n# With coverage\npytest --cov=renalprog --cov-report=html\n</code></pre>"},{"location":"api/#contributing","title":"Contributing","text":"<p>See Contributing Guidelines for development information.</p>"},{"location":"api/#version-history","title":"Version History","text":"<ul> <li>v0.1.0 (2024-12): Initial release</li> <li>VAE training pipeline</li> <li>Trajectory generation</li> <li>XGBoost classification</li> <li>GSEA integration</li> </ul>"},{"location":"api/#license","title":"License","text":"<p>Apache License 2.0. See LICENSE.</p>"},{"location":"api/#citation","title":"Citation","text":"<pre><code>@software{renalprog2024,\n  author = {Prol-Castelo, Guillermo},\n  title = {renalprog: Cancer Progression Forecasting with Generative AI},\n  year = {2024},\n  url = {https://github.com/gprolcastelo/renalprog}\n}\n</code></pre>"},{"location":"api/classification/","title":"Classification API","text":"<p>Functions for trajectory classification and survival analysis.</p>"},{"location":"api/classification/#overview","title":"Overview","text":"<p>The classification module provides:</p> <ul> <li>Trajectory-based classification</li> <li>Survival prediction</li> <li>Classifier training and evaluation</li> <li>Feature importance analysis</li> </ul>"},{"location":"api/classification/#main-classification-function","title":"Main Classification Function","text":""},{"location":"api/classification/#classify_trajectories","title":"classify_trajectories","text":"<p>Train classifier to predict disease progression from trajectories.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import classify_trajectories\nimport pandas as pd\nfrom pathlib import Path\n\n# Load trajectory data\ntrajectories = pd.read_csv(\"data/processed/trajectories.csv\")\nlabels = pd.read_csv(\"data/processed/progression_labels.csv\")\n\n# Train classifier\nclassifier, metrics = classify_trajectories(\n    trajectories=trajectories.values,\n    labels=labels['progressed'].values,\n    output_dir=Path(\"models/trajectory_classifier\"),\n    model_type='random_forest',  # or 'logistic', 'svm', 'gradient_boosting'\n    test_size=0.2,\n    random_state=42\n)\n\n# Print performance\nprint(f\"Accuracy: {metrics['accuracy']:.3f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.3f}\")\nprint(f\"Precision: {metrics['precision']:.3f}\")\nprint(f\"Recall: {metrics['recall']:.3f}\")\nprint(f\"F1 Score: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"api/classification/#renalprog.modeling.predict.classify_trajectories","title":"classify_trajectories","text":"<pre><code>classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, DataFrame],\n    gene_subset: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Apply stage classifier to synthetic trajectories.</p> <p>Args:     classifier: Trained classifier model     trajectory_data: Dictionary of patient pair to trajectory DataFrames     gene_subset: Optional subset of genes to use for classification</p> <p>Returns:     DataFrame with classification results for each trajectory point</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, pd.DataFrame],\n    gene_subset: Optional[List[str]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply stage classifier to synthetic trajectories.\n\n    Args:\n        classifier: Trained classifier model\n        trajectory_data: Dictionary of patient pair to trajectory DataFrames\n        gene_subset: Optional subset of genes to use for classification\n\n    Returns:\n        DataFrame with classification results for each trajectory point\n    \"\"\"\n    logger.info(\"Classifying trajectory points\")\n\n    # TODO: Implement trajectory classification\n    # Migrate from notebooks/kirc_classification_trajectory.ipynb\n\n    raise NotImplementedError(\n        \"classify_trajectories() needs implementation from \"\n        \"notebooks/kirc_classification_trajectory.ipynb\"\n    )\n</code></pre>"},{"location":"api/classification/#classification-models","title":"Classification Models","text":"<p>The <code>classify_trajectories</code> function supports multiple model types:</p>"},{"location":"api/classification/#random-forest","title":"Random Forest","text":"<p>Default choice for interpretability and feature importance:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='random_forest',\n    n_estimators=100,\n    max_depth=None,\n    min_samples_split=2\n)\n\n# Feature importance\nimportances = classifier.feature_importances_\n</code></pre>"},{"location":"api/classification/#logistic-regression","title":"Logistic Regression","text":"<p>For linear decision boundaries:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='logistic',\n    C=1.0,\n    penalty='l2',\n    max_iter=1000\n)\n</code></pre>"},{"location":"api/classification/#support-vector-machine","title":"Support Vector Machine","text":"<p>For complex decision boundaries:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='svm',\n    kernel='rbf',\n    C=1.0,\n    gamma='scale'\n)\n</code></pre>"},{"location":"api/classification/#gradient-boosting","title":"Gradient Boosting","text":"<p>For maximum performance:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='gradient_boosting',\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3\n)\n</code></pre>"},{"location":"api/classification/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The classification function returns comprehensive metrics:</p> Metric Description <code>accuracy</code> Overall classification accuracy <code>auc_roc</code> Area under ROC curve <code>precision</code> Positive predictive value <code>recall</code> Sensitivity/True positive rate <code>f1</code> Harmonic mean of precision and recall <code>confusion_matrix</code> 2\u00d72 confusion matrix <code>classification_report</code> Detailed per-class metrics"},{"location":"api/classification/#visualization","title":"Visualization","text":""},{"location":"api/classification/#plot_confusion_matrix","title":"plot_confusion_matrix","text":"<p>Visualize classifier performance:</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_confusion_matrix\nfrom pathlib import Path\n\n# Plot confusion matrix\nplot_confusion_matrix(\n    confusion_matrix=metrics['confusion_matrix'],\n    class_names=['Non-progressing', 'Progressing'],\n    output_path=Path(\"reports/figures/confusion_matrix.png\"),\n    title=\"Trajectory Classification\"\n)\n</code></pre>"},{"location":"api/classification/#renalprog.plots.plot_confusion_matrix","title":"plot_confusion_matrix","text":"<pre><code>plot_confusion_matrix(\n    confusion_matrix: ndarray,\n    class_names: List[str],\n    save_path: Optional[Path] = None,\n    title: str = \"Confusion Matrix\",\n    normalize: bool = False,\n) -&gt; go.Figure\n</code></pre> <p>Plot confusion matrix as heatmap.</p> <p>Args:     confusion_matrix: Square confusion matrix     class_names: List of class names     save_path: Optional path to save figure     title: Plot title     normalize: Whether to normalize by row</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_confusion_matrix(\n    confusion_matrix: np.ndarray,\n    class_names: List[str],\n    save_path: Optional[Path] = None,\n    title: str = \"Confusion Matrix\",\n    normalize: bool = False,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot confusion matrix as heatmap.\n\n    Args:\n        confusion_matrix: Square confusion matrix\n        class_names: List of class names\n        save_path: Optional path to save figure\n        title: Plot title\n        normalize: Whether to normalize by row\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    if normalize:\n        cm = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n        text = [[f'{confusion_matrix[i, j]}&lt;br&gt;({cm[i, j]:.2%})'\n                for j in range(len(class_names))]\n               for i in range(len(class_names))]\n    else:\n        cm = confusion_matrix\n        text = [[str(confusion_matrix[i, j])\n                for j in range(len(class_names))]\n               for i in range(len(class_names))]\n\n    fig = go.Figure(data=go.Heatmap(\n        z=cm,\n        x=class_names,\n        y=class_names,\n        text=text,\n        texttemplate='%{text}',\n        colorscale='Blues',\n        showscale=True\n    ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='Predicted',\n        yaxis_title='Actual',\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        yaxis={'autorange': 'reversed'}\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/classification/#complete-classification-workflow","title":"Complete Classification Workflow","text":"<pre><code>import torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nfrom renalprog.modeling.train import VAE\nfrom renalprog.modeling.predict import (\n    apply_vae,\n    generate_trajectories,\n    classify_trajectories\n)\nfrom renalprog.plots import plot_confusion_matrix\n\n# 1. Load model and data\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel.load_state_dict(torch.load(\"models/my_vae/best_model.pt\"))\n\nexpr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# 2. Generate trajectories for each patient\nearly_mask = clinical['stage'] == 'early'\nlate_mask = clinical['stage'] == 'late'\n\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=expr.values[early_mask],\n    end_data=expr.values[late_mask],\n    n_steps=50,\n    interpolation='spherical',\n    device='cuda'\n)\n\n# 3. Extract trajectory features\n# Option A: Use trajectory statistics (mean, std, slope)\ntrajectory_features = []\nfor traj in trajectories:\n    features = np.concatenate([\n        traj.mean(axis=0),  # Mean expression\n        traj.std(axis=0),   # Variance\n        (traj[-1] - traj[0])  # Net change\n    ])\n    trajectory_features.append(features)\ntrajectory_features = np.array(trajectory_features)\n\n# Option B: Use latent trajectory\nresults = apply_vae(model, expr.values, device='cuda')\nlatent_trajectories = results['latent']\n\n# 4. Create labels (e.g., based on survival)\nlabels = clinical['progressed'].values[early_mask]\n\n# 5. Train classifier\nclassifier, metrics = classify_trajectories(\n    trajectories=trajectory_features,\n    labels=labels,\n    output_dir=Path(\"models/trajectory_classifier\"),\n    model_type='random_forest',\n    test_size=0.2,\n    random_state=42\n)\n\n# 6. Visualize results\nplot_confusion_matrix(\n    confusion_matrix=metrics['confusion_matrix'],\n    class_names=['Stable', 'Progressed'],\n    output_path=Path(\"reports/figures/classification_cm.png\")\n)\n\n# 7. Feature importance (for tree-based models)\nif hasattr(classifier, 'feature_importances_'):\n    importances = pd.DataFrame({\n        'feature': [f'feature_{i}' for i in range(len(classifier.feature_importances_))],\n        'importance': classifier.feature_importances_\n    }).sort_values('importance', ascending=False)\n\n    print(\"Top 10 important features:\")\n    print(importances.head(10))\n\n# 8. Save results\nresults_df = pd.DataFrame({\n    'patient_id': clinical.index[early_mask],\n    'true_label': labels,\n    'predicted_label': classifier.predict(trajectory_features),\n    'prediction_proba': classifier.predict_proba(trajectory_features)[:, 1]\n})\nresults_df.to_csv(\"reports/classification_results.csv\", index=False)\n\nprint(\"\\nClassification Performance:\")\nprint(f\"Accuracy: {metrics['accuracy']:.3f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.3f}\")\nprint(f\"F1 Score: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"api/classification/#cross-validation","title":"Cross-Validation","text":"<p>For robust performance estimation:</p> <pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Cross-validation\ncv_scores = cross_val_score(\n    rf, trajectory_features, labels,\n    cv=5,\n    scoring='roc_auc'\n)\n\nprint(f\"CV AUC-ROC: {cv_scores.mean():.3f} \u00b1 {cv_scores.std():.3f}\")\n</code></pre>"},{"location":"api/classification/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid search\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    rf, param_grid,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(trajectory_features, labels)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV AUC-ROC: {grid_search.best_score_:.3f}\")\n</code></pre>"},{"location":"api/classification/#see-also","title":"See Also","text":"<ul> <li>Trajectories API - Generate trajectories</li> <li>Prediction API - Apply VAE models</li> <li>Plots API - Visualization tools</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/config/","title":"Configuration API","text":"<p>The <code>config</code> module provides centralized configuration management for the RenalProg pipeline, including paths, hyperparameters, and project-wide constants.</p>"},{"location":"api/config/#overview","title":"Overview","text":"<p>The configuration module is organized into several components:</p> <ul> <li>Path Management: Centralized directory structure for data, models, and outputs</li> <li>Preprocessing Configuration: Parameters for data preprocessing</li> <li>VAE Configuration: Hyperparameters for VAE model training</li> <li>Trajectory Configuration: Settings for trajectory generation</li> <li>Classification Configuration: Parameters for survival classification</li> <li>Enrichment Configuration: Settings for pathway enrichment analysis</li> </ul>"},{"location":"api/config/#path-structure","title":"Path Structure","text":"<p>The module defines a comprehensive directory structure:</p> <pre><code>from renalprog.config import PATHS\n\n# Access common paths\ndata_dir = PATHS['data']\nmodels_dir = PATHS['models']\nfigures_dir = PATHS['figures']\n</code></pre>"},{"location":"api/config/#available-paths","title":"Available Paths","text":"Key Description <code>root</code> Project root directory <code>data</code> Main data directory <code>raw</code> Raw, immutable data <code>interim</code> Intermediate processed data <code>processed</code> Final, canonical data sets <code>external</code> External data sources (pathways, gene lists) <code>models</code> Trained models and checkpoints <code>reports</code> Analysis reports <code>figures</code> Generated figures and plots <code>notebooks</code> Jupyter notebooks <code>references</code> Reference materials <code>scripts</code> Pipeline scripts"},{"location":"api/config/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/config/#preprocessingconfig","title":"PreprocessingConfig","text":"<p>Configuration for data preprocessing steps.</p> <p>Example Usage:</p> <pre><code>from renalprog.config import PreprocessingConfig\n\n# Access default preprocessing parameters\nconfig = PreprocessingConfig()\nprint(f\"Mean threshold: {config.mean_threshold}\")\nprint(f\"Outlier alpha: {config.outlier_alpha}\")\nprint(f\"Test split ratio: {config.test_size}\")\n</code></pre>"},{"location":"api/config/#renalprog.config.PreprocessingConfig","title":"PreprocessingConfig","text":"<p>Parameters for data preprocessing.</p>"},{"location":"api/config/#vaeconfig","title":"VAEConfig","text":"<p>Configuration for VAE model architecture and training.</p> <p>Example Usage:</p> <pre><code>from renalprog.config import VAEConfig\n\n# Use default VAE configuration\nconfig = VAEConfig()\n\n# Or customize for your experiment\ncustom_config = VAEConfig(\n    mid_dim=256,\n    latent_dim=10,\n    learning_rate=0.0001,\n    num_epochs=500,\n    batch_size=64\n)\n</code></pre>"},{"location":"api/config/#renalprog.config.VAEConfig","title":"VAEConfig","text":"<p>Default hyperparameters for VAE training.</p>"},{"location":"api/config/#trajectoryconfig","title":"TrajectoryConfig","text":"<p>Configuration for trajectory generation from trained VAE models.</p> <p>Example Usage:</p> <pre><code>from renalprog.config import TrajectoryConfig\n\nconfig = TrajectoryConfig(\n    num_trajectories=5000,\n    trajectory_length=100,\n    noise_scale=0.1\n)\n</code></pre>"},{"location":"api/config/#renalprog.config.TrajectoryConfig","title":"TrajectoryConfig","text":"<p>Parameters for synthetic trajectory generation.</p>"},{"location":"api/config/#classificationconfig","title":"ClassificationConfig","text":"<p>Configuration for survival classification models.</p>"},{"location":"api/config/#renalprog.config.ClassificationConfig","title":"ClassificationConfig","text":"<p>Parameters for stage classification.</p>"},{"location":"api/config/#enrichmentconfig","title":"EnrichmentConfig","text":"<p>Configuration for pathway enrichment analysis.</p>"},{"location":"api/config/#renalprog.config.EnrichmentConfig","title":"EnrichmentConfig","text":"<p>Parameters for pathway enrichment analysis.</p>"},{"location":"api/config/#utility-functions","title":"Utility Functions","text":""},{"location":"api/config/#get_dated_dir","title":"get_dated_dir","text":"<p>Create a dated directory for organizing time-stamped outputs.</p> <p>Example:</p> <pre><code>from renalprog.config import get_dated_dir, PATHS\n\n# Create a dated directory for today's model outputs\nmodel_dir = get_dated_dir(PATHS['models'], prefix='VAE_KIRC')\n# Returns: models/20251218_VAE_KIRC/\n</code></pre>"},{"location":"api/config/#renalprog.config.get_dated_dir","title":"get_dated_dir","text":"<pre><code>get_dated_dir(base_dir: Path, prefix: str = '') -&gt; Path\n</code></pre> <p>Create a dated directory for organizing outputs.</p> <p>Args:     base_dir: Base directory path     prefix: Optional prefix for the directory name</p> <p>Returns:     Path to the dated directory</p> Source code in <code>renalprog/config.py</code> <pre><code>def get_dated_dir(base_dir: Path, prefix: str = \"\") -&gt; Path:\n    \"\"\"\n    Create a dated directory for organizing outputs.\n\n    Args:\n        base_dir: Base directory path\n        prefix: Optional prefix for the directory name\n\n    Returns:\n        Path to the dated directory\n    \"\"\"\n    today = datetime.now().strftime(\"%Y%m%d\")\n    if prefix:\n        dir_name = f\"{today}_{prefix}\"\n    else:\n        dir_name = today\n    dated_dir = base_dir / dir_name\n    dated_dir.mkdir(parents=True, exist_ok=True)\n    return dated_dir\n</code></pre>"},{"location":"api/config/#kirc-specific-paths","title":"KIRC-Specific Paths","text":"<p>The module provides a <code>KIRCPaths</code> class for managing KIRC dataset-specific file locations:</p> <pre><code>from renalprog.config import KIRCPaths\n\n# Access KIRC-specific data paths\nrnaseq = KIRCPaths.RNASEQ_RAW\nclinical = KIRCPaths.CLINICAL_RAW\npathways = KIRCPaths.REACTOME_PATHWAYS\n</code></pre>"},{"location":"api/config/#best-practices","title":"Best Practices","text":""},{"location":"api/config/#using-configuration-in-scripts","title":"Using Configuration in Scripts","text":"<p>Always import configuration at the module level:</p> <pre><code>from renalprog.config import PATHS, VAEConfig, PreprocessingConfig\n\ndef main():\n    # Access paths\n    data_dir = PATHS['interim']\n\n    # Use configuration objects\n    vae_config = VAEConfig()\n    preproc_config = PreprocessingConfig()\n</code></pre>"},{"location":"api/config/#creating-custom-configurations","title":"Creating Custom Configurations","text":"<p>For experiments, create configuration variants:</p> <pre><code>from renalprog.config import VAEConfig\n\n# Base configuration\nbase_config = VAEConfig()\n\n# Experiment variations\nconfigs = {\n    'small': VAEConfig(mid_dim=128, latent_dim=5),\n    'medium': VAEConfig(mid_dim=256, latent_dim=10),\n    'large': VAEConfig(mid_dim=512, latent_dim=20)\n}\n</code></pre>"},{"location":"api/config/#saving-configuration","title":"Saving Configuration","text":"<p>Always save configuration with trained models:</p> <pre><code>from renalprog.modeling.checkpointing import save_model_config\n\nconfig = VAEConfig(experiment_name='my_experiment')\nsave_model_config(config, output_dir)\n</code></pre>"},{"location":"api/config/#see-also","title":"See Also","text":"<ul> <li>Dataset API - Data loading and preprocessing</li> <li>Training API - Model training with configuration</li> <li>Reproducibility Guide - Reproducing published results</li> </ul>"},{"location":"api/dataset/","title":"Dataset API","text":"<p>The <code>dataset</code> module provides functions for downloading, loading, and preparing RNA-seq data for analysis.</p>"},{"location":"api/dataset/#overview","title":"Overview","text":"<p>This module handles:</p> <ul> <li>Downloading TCGA Pan-Cancer Atlas data</li> <li>Processing and filtering by cancer type</li> <li>Creating train/test splits</li> <li>Loading preprocessed data for modeling</li> </ul>"},{"location":"api/dataset/#core-functions","title":"Core Functions","text":""},{"location":"api/dataset/#download_data","title":"download_data","text":"<p>Download KIRC datasets from TCGA Pan-Cancer Atlas.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import download_data\nfrom pathlib import Path\n\n# Download to default location\nrnaseq_path, clinical_path, phenotype_path = download_data(\n    destination=Path(\"data/raw\"),\n    remove_gz=True,\n    timeout=300\n)\n\nprint(f\"RNA-seq data: {rnaseq_path}\")\nprint(f\"Clinical data: {clinical_path}\")\nprint(f\"Phenotype data: {phenotype_path}\")\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.download_data","title":"download_data","text":"<pre><code>download_data(\n    destination: Path = \"data/raw\", remove_gz: bool = True, timeout: int = 300\n) -&gt; Tuple[Path, Path, Path]\n</code></pre> <p>Download the KIRC datasets to the specified destination.</p> <p>Args:     destination: Path to directory where dataset should be saved     remove_gz: Whether to remove .gz files after extraction     timeout: Request timeout in seconds</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def download_data(\n        destination: Path = \"data/raw\",\n        remove_gz: bool = True,\n        timeout: int = 300) -&gt; Tuple[Path,Path,Path]:\n    \"\"\"\n    Download the KIRC datasets to the specified destination.\n\n    Args:\n        destination: Path to directory where dataset should be saved\n        remove_gz: Whether to remove .gz files after extraction\n        timeout: Request timeout in seconds\n    \"\"\"\n    # Ensure save directory exists\n    destination = Path(destination)\n    destination.mkdir(parents=True, exist_ok=True)\n\n    datasets = [\n        (\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/\"\n         \"EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena.gz\",\n         \"Gene expression RNAseq - Batch effects normalized mRNA data\"),\n        (\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/\"\n         \"TCGA_phenotype_denseDataOnlyDownload.tsv.gz\",\n         \"TCGA phenotype data\"),\n        (\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/\"\n         \"Survival_SupplementalTable_S1_20171025_xena_sp\",\n         \"Clinical survival data\")\n    ]\n\n    for url, description in datasets:\n        try:\n            filename = url.split('/')[-1]\n            file_path = destination / filename\n            is_gzipped = filename.endswith('.gz')\n\n            logger.info(f\"Downloading dataset: {description}...\")\n            response = requests.get(url, timeout=timeout, stream=True)\n            response.raise_for_status()\n\n            total_size = int(response.headers.get('content-length', 0))\n            with open(file_path, 'wb') as file:\n                downloaded = 0\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        file.write(chunk)\n                        downloaded += len(chunk)\n                        if total_size &gt; 0:\n                            percent = (downloaded / total_size) * 100\n                            print(f\"\\rDownloading {filename}: {percent:.1f}%\", end='', flush=True)\n\n            print(f\"\\nFile downloaded successfully: {file_path}\")\n\n            if is_gzipped:\n                extracted_path = destination / filename.replace('.gz', '')\n                logger.info(\"Extracting compressed file...\")\n                with gzip.open(file_path, 'rb') as f_in, open(extracted_path, 'wb') as f_out:\n                    f_out.write(f_in.read())\n                logger.info(f\"Successfully extracted file to: {extracted_path}\")\n\n                if remove_gz:\n                    file_path.unlink()\n                    logger.info(\"Removed compressed .gz file\")\n\n        except requests.RequestException as e:\n            logger.error(f\"Failed to download {description}: {e}\")\n            raise\n        except IOError as e:\n            logger.error(f\"File I/O error for {description}: {e}\")\n            raise\n\n    # Return paths to downloaded files\n    rnaseq_path = destination / \"EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\"\n    clinical_path = destination / \"Survival_SupplementalTable_S1_20171025_xena_sp\"\n    phenotype_path = destination / \"TCGA_phenotype_denseDataOnlyDownload.tsv\"\n\n    return rnaseq_path, clinical_path, phenotype_path\n</code></pre>"},{"location":"api/dataset/#process_downloaded_data","title":"process_downloaded_data","text":"<p>Process downloaded TCGA data for a specific cancer type.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import process_downloaded_data\nfrom pathlib import Path\n\n# Process data for KIRC (Kidney Renal Clear Cell Carcinoma)\nrnaseq, clinical, phenotype = process_downloaded_data(\n    rnaseq_path=Path(\"data/raw/EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\"),\n    clinical_path=Path(\"data/raw/Survival_SupplementalTable_S1_20171025_xena_sp\"),\n    phenotype_path=Path(\"data/raw/TCGA_phenotype_denseDataOnlyDownload.tsv\"),\n    cancer_type=\"KIRC\",\n    output_dir=Path(\"data/raw\"),\n    early_late=False\n)\n\nprint(f\"RNA-seq shape: {rnaseq.shape}\")\nprint(f\"Clinical shape: {clinical.shape}\")\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.process_downloaded_data","title":"process_downloaded_data","text":"<pre><code>process_downloaded_data(\n    rnaseq_path: Path = \"data/raw/EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\",\n    clinical_path: Path = \"data/raw/Survival_SupplementalTable_S1_20171025_xena_sp\",\n    phenotype_path: Path = \"data/raw/TCGA_phenotype_denseDataOnlyDownload.tsv\",\n    cancer_type: str = \"KIRC\",\n    output_dir: Path = \"data/raw\",\n    early_late: bool = False,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Process TCGA Pan-Cancer Atlas data for a specific cancer type.</p> <p>This function performs comprehensive data processing and quality control on TCGA datasets, including cancer type filtering, sample type selection, stage harmonization, and optional binary classification mapping. The processing pipeline follows TCGA best practices for multi-omics data integration.</p> <p>Processing Steps:     1. Load raw RNA-seq, clinical, and phenotype data from TCGA Xena Hub     2. Filter samples by cancer type (KIRC or BRCA)     3. Select primary tumor samples only (exclude metastatic/recurrent)     4. Remove ambiguous stage annotations (Stage X, discrepancies, missing)     5. Harmonize substages (e.g., Stage IA/IB/IC \u2192 Stage I)     6. Optionally map stages to binary early/late classification     7. Ensure consistency across all three datasets     8. Save processed data in CSV format</p> <p>Args:     rnaseq_path: Path to RNA-seq expression matrix (genes \u00d7 samples).         Expected format: tab-delimited file with gene IDs as rows and         sample IDs (TCGA barcodes) as columns.     clinical_path: Path to clinical survival data.         Expected format: tab-delimited file with survival information,         stage annotations, and patient metadata.     phenotype_path: Path to phenotype annotations.         Expected format: tab-delimited file with sample type information         (Primary Tumor, Metastatic, etc.).     cancer_type: Cancer type abbreviation. Supported values:         - \"KIRC\": Kidney Renal Clear Cell Carcinoma         - \"BRCA\": Breast Invasive Carcinoma (filters to female patients only)     output_dir: Directory where processed CSV files will be saved.     early_late: If True, map AJCC stages to binary classification:         - \"early\": Stage I and Stage II         - \"late\": Stage III and Stage IV         If False, retain original stage granularity (I, II, III, IV).</p> <p>Returns:     Tuple of Paths to processed files:         - rnaseq_path: Processed RNA-seq expression matrix         - clinical_path: Processed clinical annotations         - phenotype_path: Processed phenotype data</p> <p>Raises:     FileNotFoundError: If input files do not exist     KeyError: If required columns are missing from input data     ValueError: If cancer_type is not supported</p> <p>Notes:     - For BRCA, only female patients with ductal or lobular carcinomas are retained     - AJCC pathologic tumor stage is used as the primary staging system     - Substages (A, B, C) are collapsed to main stages for statistical power     - All three output datasets maintain consistent sample identifiers</p> <p>Examples:     &gt;&gt;&gt; # Process KIRC data with 4-stage classification     &gt;&gt;&gt; paths = process_downloaded_data(     ...     rnaseq_path=\"data/raw/expression.xena\",     ...     clinical_path=\"data/raw/survival.tsv\",     ...     phenotype_path=\"data/raw/phenotype.tsv\",     ...     cancer_type=\"KIRC\",     ...     output_dir=\"data/processed\",     ...     early_late=False     ... )</p> <pre><code>&gt;&gt;&gt; # Process KIRC data with binary early/late classification\n&gt;&gt;&gt; paths = process_downloaded_data(\n...     cancer_type=\"KIRC\",\n...     output_dir=\"data/processed\",\n...     early_late=True\n... )\n</code></pre> <p>References:     - TCGA Research Network: https://www.cancer.gov/tcga     - UCSC Xena Browser: https://xenabrowser.net/     - AJCC Cancer Staging Manual, 8<sup>th</sup> Edition</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def process_downloaded_data(\n        rnaseq_path: Path = \"data/raw/EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\",\n        clinical_path: Path = \"data/raw/Survival_SupplementalTable_S1_20171025_xena_sp\",\n        phenotype_path: Path = \"data/raw/TCGA_phenotype_denseDataOnlyDownload.tsv\",\n        cancer_type: str = \"KIRC\",\n        output_dir: Path = \"data/raw\",\n        early_late: bool = False) -&gt; Tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n    \"\"\"\n    Process TCGA Pan-Cancer Atlas data for a specific cancer type.\n\n    This function performs comprehensive data processing and quality control on TCGA\n    datasets, including cancer type filtering, sample type selection, stage harmonization,\n    and optional binary classification mapping. The processing pipeline follows TCGA\n    best practices for multi-omics data integration.\n\n    Processing Steps:\n        1. Load raw RNA-seq, clinical, and phenotype data from TCGA Xena Hub\n        2. Filter samples by cancer type (KIRC or BRCA)\n        3. Select primary tumor samples only (exclude metastatic/recurrent)\n        4. Remove ambiguous stage annotations (Stage X, discrepancies, missing)\n        5. Harmonize substages (e.g., Stage IA/IB/IC \u2192 Stage I)\n        6. Optionally map stages to binary early/late classification\n        7. Ensure consistency across all three datasets\n        8. Save processed data in CSV format\n\n    Args:\n        rnaseq_path: Path to RNA-seq expression matrix (genes \u00d7 samples).\n            Expected format: tab-delimited file with gene IDs as rows and\n            sample IDs (TCGA barcodes) as columns.\n        clinical_path: Path to clinical survival data.\n            Expected format: tab-delimited file with survival information,\n            stage annotations, and patient metadata.\n        phenotype_path: Path to phenotype annotations.\n            Expected format: tab-delimited file with sample type information\n            (Primary Tumor, Metastatic, etc.).\n        cancer_type: Cancer type abbreviation. Supported values:\n            - \"KIRC\": Kidney Renal Clear Cell Carcinoma\n            - \"BRCA\": Breast Invasive Carcinoma (filters to female patients only)\n        output_dir: Directory where processed CSV files will be saved.\n        early_late: If True, map AJCC stages to binary classification:\n            - \"early\": Stage I and Stage II\n            - \"late\": Stage III and Stage IV\n            If False, retain original stage granularity (I, II, III, IV).\n\n    Returns:\n        Tuple of Paths to processed files:\n            - rnaseq_path: Processed RNA-seq expression matrix\n            - clinical_path: Processed clinical annotations\n            - phenotype_path: Processed phenotype data\n\n    Raises:\n        FileNotFoundError: If input files do not exist\n        KeyError: If required columns are missing from input data\n        ValueError: If cancer_type is not supported\n\n    Notes:\n        - For BRCA, only female patients with ductal or lobular carcinomas are retained\n        - AJCC pathologic tumor stage is used as the primary staging system\n        - Substages (A, B, C) are collapsed to main stages for statistical power\n        - All three output datasets maintain consistent sample identifiers\n\n    Examples:\n        &gt;&gt;&gt; # Process KIRC data with 4-stage classification\n        &gt;&gt;&gt; paths = process_downloaded_data(\n        ...     rnaseq_path=\"data/raw/expression.xena\",\n        ...     clinical_path=\"data/raw/survival.tsv\",\n        ...     phenotype_path=\"data/raw/phenotype.tsv\",\n        ...     cancer_type=\"KIRC\",\n        ...     output_dir=\"data/processed\",\n        ...     early_late=False\n        ... )\n\n        &gt;&gt;&gt; # Process KIRC data with binary early/late classification\n        &gt;&gt;&gt; paths = process_downloaded_data(\n        ...     cancer_type=\"KIRC\",\n        ...     output_dir=\"data/processed\",\n        ...     early_late=True\n        ... )\n\n    References:\n        - TCGA Research Network: https://www.cancer.gov/tcga\n        - UCSC Xena Browser: https://xenabrowser.net/\n        - AJCC Cancer Staging Manual, 8th Edition\n    \"\"\"\n    # =========================================================================\n    # STEP 1: Load raw data from TCGA sources\n    # =========================================================================\n    logger.info(\"=\"*80)\n    logger.info(f\"Starting data processing pipeline for {cancer_type}\")\n    logger.info(\"=\"*80)\n\n    rnaseq = pd.read_table(rnaseq_path, index_col=0)\n    clinical = pd.read_csv(clinical_path, sep=\"\\t\", index_col=0)\n    pheno = pd.read_table(phenotype_path, sep=\"\\t\", index_col=0)\n\n    # Remove redundant _PATIENT column from clinical data\n    clinical.drop(\"_PATIENT\", axis=1, inplace=True, errors='ignore')\n\n    logger.info(\"\\n[INITIAL DATA SHAPES]\")\n    logger.info(f\"  RNA-seq:   {rnaseq.shape[0]:&gt;6,} genes \u00d7 {rnaseq.shape[1]:&gt;5,} samples\")\n    logger.info(f\"  Clinical:  {clinical.shape[0]:&gt;6,} patients \u00d7 {clinical.shape[1]:&gt;3,} features\")\n    logger.info(f\"  Phenotype: {pheno.shape[0]:&gt;6,} samples \u00d7 {pheno.shape[1]:&gt;3,} features\")\n\n    # =========================================================================\n    # STEP 2: Filter by cancer type\n    # =========================================================================\n    logger.info(f\"\\n[FILTERING BY CANCER TYPE: {cancer_type}]\")\n\n    if cancer_type == 'BRCA':\n        # For breast cancer, only include female patients\n        clinical = clinical[\n            (clinical[\"gender\"] == \"FEMALE\") &amp;\n            (clinical[\"cancer type abbreviation\"] == cancer_type)\n        ]\n        logger.info(f\"  Filter: Female patients with {cancer_type}\")\n    elif cancer_type == 'KIRC':\n        clinical = clinical[clinical[\"cancer type abbreviation\"] == cancer_type]\n        logger.info(f\"  Filter: Patients with {cancer_type}\")\n    else:\n        logger.warning(f\"  Cancer type '{cancer_type}' may not be fully supported\")\n        clinical = clinical[clinical[\"cancer type abbreviation\"] == cancer_type]\n\n    # Synchronize datasets to common samples\n    pheno = pheno[pheno.index.isin(clinical.index)]\n    rnaseq = rnaseq.loc[:, rnaseq.columns.isin(clinical.index)]\n    pheno = pheno[pheno.index.isin(rnaseq.columns)]\n    clinical = clinical[clinical.index.isin(rnaseq.columns)]\n\n    logger.info(f\"\\n  Post-filter shapes:\")\n    logger.info(f\"    RNA-seq:   {rnaseq.shape[0]:&gt;6,} genes \u00d7 {rnaseq.shape[1]:&gt;5,} samples\")\n    logger.info(f\"    Clinical:  {clinical.shape[0]:&gt;6,} patients\")\n    logger.info(f\"    Phenotype: {pheno.shape[0]:&gt;6,} samples\")\n\n    # =========================================================================\n    # STEP 3: Select primary tumor samples only\n    # =========================================================================\n    logger.info(\"\\n[FILTERING BY SAMPLE TYPE: Primary Tumor]\")\n\n    # Log sample type distribution before filtering\n    sample_type_counts = pheno[\"sample_type\"].value_counts()\n    logger.info(\"  Sample type distribution:\")\n    for sample_type, count in sample_type_counts.items():\n        logger.info(f\"    {sample_type:&lt;30} {count:&gt;5,} samples\")\n\n    pheno = pheno[pheno[\"sample_type\"] == \"Primary Tumor\"]\n    clinical = clinical[clinical.index.isin(pheno.index)]\n    rnaseq = rnaseq.loc[:, rnaseq.columns.isin(pheno.index)]\n\n    logger.info(f\"\\n  Retained {len(pheno):,} primary tumor samples\")\n    logger.info(f\"    RNA-seq:   {rnaseq.shape[0]:&gt;6,} genes \u00d7 {rnaseq.shape[1]:&gt;5,} samples\")\n    logger.info(f\"    Clinical:  {clinical.shape[0]:&gt;6,} patients\")\n    logger.info(f\"    Phenotype: {pheno.shape[0]:&gt;6,} samples\")\n\n    # =========================================================================\n    # STEP 4: Remove ambiguous stage annotations\n    # =========================================================================\n    logger.info(\"\\n[FILTERING BY STAGE QUALITY]\")\n\n    stages_remove = ['Stage X', '[Discrepancy]', np.nan]\n    initial_stage_counts = clinical[\"ajcc_pathologic_tumor_stage\"].value_counts(dropna=False)\n\n    logger.info(\"  Initial stage distribution:\")\n    _log_stage_table(initial_stage_counts)\n\n    # Filter out problematic stage annotations\n    clinical_redux = clinical[~clinical[\"ajcc_pathologic_tumor_stage\"].isin(stages_remove)]\n    removed_count = len(clinical) - len(clinical_redux)\n    logger.info(f\"\\n  Removed {removed_count} samples with ambiguous staging\")\n\n    # =========================================================================\n    # STEP 5: Harmonize substages (collapse A/B/C to main stage)\n    # =========================================================================\n    logger.info(\"\\n[HARMONIZING SUBSTAGES]\")\n\n    stages_available = clinical_redux[\"ajcc_pathologic_tumor_stage\"].unique()\n    has_substages = any(\n        str(stage).endswith((\"A\", \"B\", \"C\"))\n        for stage in stages_available\n        if pd.notna(stage)\n    )\n\n    if has_substages:\n        logger.info(\"  Substages detected (e.g., Stage IA, Stage IIB)\")\n        logger.info(\"  Collapsing substages to main stages for statistical power\")\n\n        # Collapse substages by removing trailing A/B/C letters\n        stages_clump = [\n            str(stage)[:-1] if str(stage).endswith((\"A\", \"B\", \"C\")) else stage\n            for stage in clinical_redux[\"ajcc_pathologic_tumor_stage\"]\n        ]\n        clinical_redux[\"ajcc_pathologic_tumor_stage\"] = stages_clump\n\n        post_clump_counts = clinical_redux[\"ajcc_pathologic_tumor_stage\"].value_counts().sort_index()\n        logger.info(\"\\n  Stage distribution after harmonization:\")\n        _log_stage_table(post_clump_counts)\n    else:\n        logger.info(\"  No substages detected; stage labels are already harmonized\")\n\n    # Synchronize datasets after stage filtering\n    pheno_redux = pheno[pheno.index.isin(clinical_redux.index)]\n    rnaseq_redux = rnaseq.loc[:, rnaseq.columns.isin(clinical_redux.index)]\n\n    # =========================================================================\n    # STEP 6: Additional cancer-specific filtering (BRCA only)\n    # =========================================================================\n    if cancer_type == 'BRCA':\n        logger.info(\"\\n[BRCA-SPECIFIC FILTERING: Ductal and Lobular Carcinomas]\")\n        ductal_lobular = ['Infiltrating Ductal Carcinoma', 'Infiltrating Lobular Carcinoma']\n        patients_ductal_lobular = clinical_redux.index[\n            clinical_redux[\"histological_type\"].isin(ductal_lobular)\n        ]\n\n        clinical_redux = clinical_redux[clinical_redux.index.isin(patients_ductal_lobular)]\n        pheno_redux = pheno_redux[pheno_redux.index.isin(patients_ductal_lobular)]\n        rnaseq_redux = rnaseq_redux.loc[:, rnaseq_redux.columns.isin(patients_ductal_lobular)]\n\n        logger.info(f\"  Retained {len(patients_ductal_lobular):,} ductal/lobular samples\")\n\n    # =========================================================================\n    # STEP 7: Optional binary classification mapping (early vs. late)\n    # =========================================================================\n    if early_late:\n        logger.info(\"\\n[MAPPING TO BINARY CLASSIFICATION: Early vs. Late]\")\n        logger.info(\"  Mapping scheme:\")\n        logger.info(\"    Early: Stage I, Stage II\")\n        logger.info(\"    Late:  Stage III, Stage IV\")\n\n        mapped_stages = map_stages_to_early_late(clinical_redux[\"ajcc_pathologic_tumor_stage\"])\n        valid_mask = mapped_stages.notna()\n\n        clinical_redux = clinical_redux.loc[valid_mask, :]\n        clinical_redux[\"ajcc_pathologic_tumor_stage\"] = mapped_stages[valid_mask]\n        pheno_redux = pheno_redux[pheno_redux.index.isin(clinical_redux.index)]\n        rnaseq_redux = rnaseq_redux.loc[:, rnaseq_redux.columns.isin(clinical_redux.index)]\n\n        final_stage_counts = clinical_redux[\"ajcc_pathologic_tumor_stage\"].value_counts().sort_index()\n        logger.info(\"\\n  Final binary classification distribution:\")\n        _log_stage_table(final_stage_counts)\n    else:\n        final_stage_counts = clinical_redux[\"ajcc_pathologic_tumor_stage\"].value_counts().sort_index()\n        logger.info(\"\\n  Final stage distribution (4-class):\")\n        _log_stage_table(final_stage_counts)\n\n    # =========================================================================\n    # STEP 8: Final statistics and data saving\n    # =========================================================================\n    logger.info(\"\\n[FINAL PROCESSED DATA SHAPES]\")\n    logger.info(f\"  RNA-seq:   {rnaseq_redux.shape[0]:&gt;6,} genes \u00d7 {rnaseq_redux.shape[1]:&gt;5,} samples\")\n    logger.info(f\"  Clinical:  {clinical_redux.shape[0]:&gt;6,} patients \u00d7 {clinical_redux.shape[1]:&gt;3,} features\")\n    logger.info(f\"  Phenotype: {pheno_redux.shape[0]:&gt;6,} samples \u00d7 {pheno_redux.shape[1]:&gt;3,} features\")\n\n    # Calculate and log filtering statistics\n    initial_samples = rnaseq.shape[1]\n    final_samples = rnaseq_redux.shape[1]\n    retention_rate = (final_samples / initial_samples) * 100 if initial_samples &gt; 0 else 0\n\n    logger.info(f\"\\n[FILTERING SUMMARY]\")\n    logger.info(f\"  Initial samples:  {initial_samples:&gt;5,}\")\n    logger.info(f\"  Final samples:    {final_samples:&gt;5,}\")\n    logger.info(f\"  Retention rate:   {retention_rate:&gt;5.1f}%\")\n    logger.info(f\"  Samples removed:  {initial_samples - final_samples:&gt;5,}\")\n\n    # Save processed data to disk\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    path_rnaseq = output_dir / f\"{cancer_type}_rnaseq.csv\"\n    path_clinical = output_dir / f\"{cancer_type}_clinical.csv\"\n    path_phenotype = output_dir / f\"{cancer_type}_phenotype.csv\"\n\n    logger.info(f\"\\n[SAVING PROCESSED DATA]\")\n    logger.info(f\"  Output directory: {output_dir}\")\n\n    rnaseq_redux.to_csv(path_rnaseq)\n    logger.info(f\"  \u2713 Saved RNA-seq:   {path_rnaseq.name}\")\n\n    clinical_redux.to_csv(path_clinical)\n    logger.info(f\"  \u2713 Saved clinical:  {path_clinical.name}\")\n\n    pheno_redux.to_csv(path_phenotype)\n    logger.info(f\"  \u2713 Saved phenotype: {path_phenotype.name}\")\n\n    logger.info(\"\\n\" + \"=\"*80)\n    logger.info(f\"Data processing completed successfully for {cancer_type}\")\n    logger.info(\"=\"*80 + \"\\n\")\n\n    return rnaseq_redux, clinical_redux, pheno_redux\n</code></pre>"},{"location":"api/dataset/#load_rnaseq_data","title":"load_rnaseq_data","text":"<p>Load RNA-seq expression data from a file.</p>"},{"location":"api/dataset/#renalprog.dataset.load_rnaseq_data","title":"load_rnaseq_data","text":"<pre><code>load_rnaseq_data(path: Path) -&gt; pd.DataFrame\n</code></pre> <p>Load RNA-seq data from CSV file.</p> <p>Args:     path: Path to RNA-seq CSV file (genes as rows, samples as columns)</p> <p>Returns:     DataFrame with RNA-seq data</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def load_rnaseq_data(path: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Load RNA-seq data from CSV file.\n\n    Args:\n        path: Path to RNA-seq CSV file (genes as rows, samples as columns)\n\n    Returns:\n        DataFrame with RNA-seq data\n    \"\"\"\n    logger.info(f\"Loading RNA-seq data from {path}\")\n    data = pd.read_csv(path, index_col=0)\n    logger.info(f\"Loaded RNA-seq data with shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/dataset/#load_clinical_data","title":"load_clinical_data","text":"<p>Load clinical and survival data from a file.</p>"},{"location":"api/dataset/#renalprog.dataset.load_clinical_data","title":"load_clinical_data","text":"<pre><code>load_clinical_data(\n    path: Path,\n    stage_column: str = \"ajcc_pathologic_tumor_stage\",\n    early_late=True,\n) -&gt; pd.Series\n</code></pre> <p>Load clinical metadata.</p> <p>Args:     path: Path to clinical data CSV file     stage_column: Name of column containing stage information</p> <p>Returns:     Series with clinical stages indexed by sample ID</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def load_clinical_data(path: Path, stage_column: str = \"ajcc_pathologic_tumor_stage\", early_late = True) -&gt; pd.Series:\n    \"\"\"\n    Load clinical metadata.\n\n    Args:\n        path: Path to clinical data CSV file\n        stage_column: Name of column containing stage information\n\n    Returns:\n        Series with clinical stages indexed by sample ID\n    \"\"\"\n    logger.info(f\"Loading clinical data from {path}\")\n    data = pd.read_csv(path, index_col=0)\n\n    if stage_column not in data.columns:\n        raise ValueError(f\"Stage column '{stage_column}' not found in clinical data\")\n\n    stages = data[stage_column]\n    logger.info(f\"Loaded clinical data with {len(stages)} samples\")\n\n    if early_late:\n        stages = map_stages_to_early_late(stages)\n        logger.info(\"Mapped stages to binary early/late classification\")\n    logger.info(f\"Stage distribution:\\n{stages.value_counts()}\")\n\n    return stages\n</code></pre>"},{"location":"api/dataset/#create_train_test_split","title":"create_train_test_split","text":"<p>Create stratified train/test splits of the data.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import load_rnaseq_data, load_clinical_data, create_train_test_split\nfrom pathlib import Path\n\n# Load the data\nrnaseq = load_rnaseq_data(Path(\"data/raw/KIRC_rnaseq.tsv\"))\nclinical = load_clinical_data(Path(\"data/raw/KIRC_clinical.tsv\"))\n\n# Create train/test split\ncreate_train_test_split(\n    rnaseq=rnaseq,\n    clinical=clinical,\n    test_size=0.2,\n    random_state=42,\n    output_dir=Path(\"data/interim/my_experiment\")\n)\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.create_train_test_split","title":"create_train_test_split","text":"<pre><code>create_train_test_split(\n    rnaseq_path: Path,\n    clinical_path: Path,\n    test_size: float = 0.2,\n    seed: int = 2023,\n    use_onehot: bool = True,\n    output_dir: Optional[Path] = None,\n) -&gt; Tuple[\n    pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, pd.DataFrame, pd.Series\n]\n</code></pre> <p>Create stratified train/test split of KIRC data.</p> <p>Args:     rnaseq_path: Path to RNA-seq CSV file     clinical_path: Path to clinical CSV file     test_size: Fraction of data to use for testing (default: 0.2)     seed: Random seed for reproducibility (default: 2023)     use_onehot: Whether to one-hot encode the labels (default: True)     output_dir: Optional directory to save split data</p> <p>Returns:     Tuple of (X_train, X_test, y_train, y_test, full_rnaseq, full_clinical)</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def create_train_test_split(\n    rnaseq_path: Path,\n    clinical_path: Path,\n    test_size: float = 0.2,\n    seed: int = 2023,\n    use_onehot: bool = True,\n    output_dir: Optional[Path] = None\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create stratified train/test split of KIRC data.\n\n    Args:\n        rnaseq_path: Path to RNA-seq CSV file\n        clinical_path: Path to clinical CSV file\n        test_size: Fraction of data to use for testing (default: 0.2)\n        seed: Random seed for reproducibility (default: 2023)\n        use_onehot: Whether to one-hot encode the labels (default: True)\n        output_dir: Optional directory to save split data\n\n    Returns:\n        Tuple of (X_train, X_test, y_train, y_test, full_rnaseq, full_clinical)\n    \"\"\"\n    set_seed(seed)\n\n    # Load data\n    rnaseq = load_rnaseq_data(rnaseq_path)\n    clinical = load_clinical_data(clinical_path)\n\n    # Ensure samples match between RNA-seq and clinical data\n    common_samples = rnaseq.columns.intersection(clinical.index)\n    if len(common_samples) &lt; len(rnaseq.columns):\n        logger.warning(\n            f\"Only {len(common_samples)} of {len(rnaseq.columns)} samples \"\n            f\"have clinical data. Filtering to common samples.\"\n        )\n\n    rnaseq = rnaseq[common_samples]\n    clinical = clinical[common_samples]\n\n    # Transpose to have samples as rows\n    rnaseq_t = rnaseq.T\n\n    # Prepare labels for stratification\n    if use_onehot:\n        # Get unique stages in sorted order for consistent encoding\n        categories = sorted(clinical.unique())\n        ohe = OneHotEncoder(\n            categories=[categories],\n            handle_unknown='ignore',\n            sparse_output=False,\n            dtype=np.int8\n        )\n        y = ohe.fit_transform(clinical.values.reshape(-1, 1))\n        logger.info(f\"One-hot encoded labels with {y.shape[1]} classes\")\n    else:\n        y = clinical.values\n\n    # Perform stratified split\n    X_train, X_test, y_train, y_test = train_test_split(\n        rnaseq_t,\n        y,\n        test_size=test_size,\n        stratify=y if use_onehot else clinical,\n        random_state=seed\n    )\n\n    logger.info(f\"Train set: {X_train.shape[0]} samples\")\n    logger.info(f\"Test set: {X_test.shape[0]} samples\")\n\n    # Save split data\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save split data\n        X_train.to_csv(output_dir / \"X_train.csv\")\n        X_test.to_csv(output_dir / \"X_test.csv\")\n\n        # Save labels WITH patient IDs as indices (matching X_train and X_test)\n        if use_onehot:\n            # Create DataFrame with patient IDs as index\n            y_train_df = pd.DataFrame(y_train, index=X_train.index)\n            y_test_df = pd.DataFrame(y_test, index=X_test.index)\n            y_train_df.to_csv(output_dir / \"y_train.csv\")\n            y_test_df.to_csv(output_dir / \"y_test.csv\")\n        else:\n            # Create Series with patient IDs as index\n            y_train_series = pd.Series(y_train, index=X_train.index)\n            y_test_series = pd.Series(y_test, index=X_test.index)\n            y_train_series.to_csv(output_dir / \"y_train.csv\")\n            y_test_series.to_csv(output_dir / \"y_test.csv\")\n\n        # Save full data for reference\n        rnaseq.to_csv(output_dir / \"data.csv\")\n        clinical.to_csv(output_dir / \"metadata.csv\")\n\n        # Save split statistics\n        _save_split_statistics(clinical, y_train, y_test, output_dir, use_onehot, categories if use_onehot else None)\n\n        logger.info(f\"Saved train/test split to {output_dir}\")\n\n    return X_train, X_test, y_train, y_test, rnaseq, clinical\n</code></pre>"},{"location":"api/dataset/#load_train_test_split","title":"load_train_test_split","text":"<p>Load previously saved train/test split data.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import load_train_test_split\nfrom pathlib import Path\n\n# Load existing split\ntrain_expr, test_expr, train_clin, test_clin = load_train_test_split(\n    Path(\"data/interim/my_experiment\")\n)\n\nprint(f\"Training samples: {len(train_expr)}\")\nprint(f\"Test samples: {len(test_expr)}\")\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.load_train_test_split","title":"load_train_test_split","text":"<pre><code>load_train_test_split(\n    split_dir: Path,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Load previously saved train/test split.</p> <p>Args:     split_dir: Directory containing saved split files</p> <p>Returns:     Tuple of (X_train, X_test, y_train, y_test)</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def load_train_test_split(split_dir: Path) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Load previously saved train/test split.\n\n    Args:\n        split_dir: Directory containing saved split files\n\n    Returns:\n        Tuple of (X_train, X_test, y_train, y_test)\n    \"\"\"\n    split_dir = Path(split_dir)\n\n    X_train = pd.read_csv(split_dir / \"X_train.csv\", index_col=0)\n    X_test = pd.read_csv(split_dir / \"X_test.csv\", index_col=0)\n    y_train = pd.read_csv(split_dir / \"y_train.csv\", index_col=0)\n    y_test = pd.read_csv(split_dir / \"y_test.csv\", index_col=0)\n\n    logger.info(f\"Loaded train/test split from {split_dir}\")\n    logger.info(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"api/dataset/#map_stages_to_early_late","title":"map_stages_to_early_late","text":"<p>Map cancer stages to binary early/late categories.</p>"},{"location":"api/dataset/#renalprog.dataset.map_stages_to_early_late","title":"map_stages_to_early_late","text":"<pre><code>map_stages_to_early_late(stages: Series) -&gt; pd.Series\n</code></pre> <p>Map detailed stages (I, II, III, IV) to binary early/late classification.</p> <p>Args:     stages: Series with stage labels (e.g., \"Stage I\", \"Stage II\", etc.)</p> <p>Returns:     Series with mapped stages (\"early\" or \"late\")</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def map_stages_to_early_late(stages: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Map detailed stages (I, II, III, IV) to binary early/late classification.\n\n    Args:\n        stages: Series with stage labels (e.g., \"Stage I\", \"Stage II\", etc.)\n\n    Returns:\n        Series with mapped stages (\"early\" or \"late\")\n    \"\"\"\n    stage_mapping = PreprocessingConfig.STAGE_MAPPING\n    mapped_stages = stages.map(stage_mapping)\n\n    # Check for unmapped values\n    unmapped = mapped_stages.isna() &amp; stages.notna()\n    if unmapped.any():\n        logger.warning(f\"Found {unmapped.sum()} unmapped stage values:\")\n        logger.warning(stages[unmapped].unique())\n\n    return mapped_stages\n</code></pre>"},{"location":"api/dataset/#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"api/dataset/#rna-seq-expression-data","title":"RNA-seq Expression Data","text":"<p>Expected format: - Rows: Genes (with gene symbols or Ensembl IDs) - Columns: Samples (patient IDs) - Values: Log2-transformed TPM or FPKM expression values</p> <pre><code># Example structure\n#              TCGA-A1-A0SB  TCGA-A1-A0SD  TCGA-A1-A0SE  ...\n# GENE_A       5.234         4.891         6.123         ...\n# GENE_B       2.456         2.789         2.634         ...\n# GENE_C       8.912         9.234         8.756         ...\n</code></pre>"},{"location":"api/dataset/#clinical-data","title":"Clinical Data","text":"<p>Expected columns: - <code>sample</code>: Patient ID matching expression columns - <code>OS</code>: Overall survival status (0=alive, 1=deceased) - <code>OS.time</code>: Overall survival time (days)</p> <p>Optional columns: - <code>age_at_initial_pathologic_diagnosis</code>: Age at diagnosis - <code>gender</code>: Patient gender - <code>tumor_stage</code>: Tumor stage</p> <pre><code># Example structure\n#    sample          OS  OS.time  age  gender  stage\n# 0  TCGA-A1-A0SB    1   1825     65   MALE    IV\n# 1  TCGA-A1-A0SD    0   2190     58   FEMALE  II\n</code></pre>"},{"location":"api/dataset/#traintest-splitting","title":"Train/Test Splitting","text":"<p>The module provides stratified splitting to maintain class balance:</p> <pre><code>from renalprog.dataset import (\n    load_rnaseq_data, \n    load_clinical_data, \n    create_train_test_split\n)\nfrom pathlib import Path\n\n# Load the data\nrnaseq = load_rnaseq_data(Path(\"data/raw/KIRC_rnaseq.tsv\"))\nclinical = load_clinical_data(Path(\"data/raw/KIRC_clinical.tsv\"))\n\n# Create stratified split preserving early/late survival distribution\ncreate_train_test_split(\n    rnaseq=rnaseq,\n    clinical=clinical,\n    test_size=0.2,  # 20% test set\n    random_state=42,  # For reproducibility\n    output_dir=Path(\"data/interim/my_split\")\n)\n\n# Load the split data\ntrain_expr, test_expr, train_clin, test_clin = load_train_test_split(\n    Path(\"data/interim/my_split\")\n)\n\n# Check class distribution\nimport pandas as pd\ntrain_dist = train_clin.value_counts(normalize=True)\ntest_dist = test_clin.value_counts(normalize=True)\n\nprint(\"Training set distribution:\")\nprint(train_dist)\nprint(\"\\nTest set distribution:\")\nprint(test_dist)\n</code></pre>"},{"location":"api/dataset/#data-preprocessing-pipeline","title":"Data Preprocessing Pipeline","text":"<p>The standard preprocessing pipeline:</p> <ol> <li>Download raw data from TCGA</li> <li>Filter by cancer type (e.g., KIRC)</li> <li>Filter low expression genes (see Features API)</li> <li>Remove outliers using Mahalanobis distance</li> <li>Create train/test split with stratification</li> <li>Normalize using MinMaxScaler (0-1 range)</li> <li>Save preprocessed data for modeling</li> </ol> <p>Complete Example:</p> <pre><code>from pathlib import Path\nfrom renalprog.dataset import (\n    download_data, \n    process_downloaded_data,\n    load_rnaseq_data,\n    load_clinical_data,\n    create_train_test_split,\n    load_train_test_split\n)\nfrom renalprog.features import filter_low_expression, detect_outliers_mahalanobis\n\n# Step 1: Download\nrnaseq_path, clinical_path, phenotype_path = download_data(\n    destination=Path(\"data/raw\")\n)\n\n# Step 2: Process for KIRC\nrnaseq, clinical, _ = process_downloaded_data(\n    rnaseq_path=rnaseq_path,\n    clinical_path=clinical_path,\n    phenotype_path=phenotype_path,\n    cancer_type=\"KIRC\",\n    output_dir=Path(\"data/raw\")\n)\n\n# Step 3: Filter low expression\nrnaseq_filtered = filter_low_expression(\n    rnaseq,\n    mean_threshold=0.5,\n    var_threshold=0.5\n)\n\n# Step 4: Remove outliers\nrnaseq_clean, outliers, _ = detect_outliers_mahalanobis(\n    rnaseq_filtered,\n    alpha=0.05\n)\n\n# Step 5: Create train/test split\nrnaseq_clean_path = Path(\"data/interim/rnaseq_clean.csv\")\nclinical_path = Path(\"data/raw/KIRC_clinical.tsv\")\nrnaseq_clean.to_csv(rnaseq_clean_path)\n\n# Load and split\nrnaseq_final = load_rnaseq_data(rnaseq_clean_path)\nclinical_final = load_clinical_data(clinical_path)\n\ncreate_train_test_split(\n    rnaseq=rnaseq_final,\n    clinical=clinical_final,\n    test_size=0.2,\n    random_state=42,\n    output_dir=Path(\"data/interim/20251218_experiment\")\n)\n</code></pre>"},{"location":"api/dataset/#see-also","title":"See Also","text":"<ul> <li>Features API - Gene filtering and outlier detection</li> <li>Preprocessing Tutorial - Step-by-step data preparation</li> <li>Configuration API - Data paths and preprocessing parameters</li> </ul>"},{"location":"api/enrichment/","title":"Enrichment Analysis","text":"<p>Pathway enrichment analysis using PyDESeq2 and GSEA.</p>"},{"location":"api/enrichment/#overview","title":"Overview","text":"<p>The enrichment module provides tools for:</p> <ol> <li>Differential Expression Analysis: Using PyDESeq2 to identify significantly changed genes</li> <li>Gene Set Enrichment Analysis (GSEA): Pathway-level analysis using GSEA CLI</li> <li>Pathway Visualization: Heatmap generation for pathway enrichment across trajectories</li> </ol>"},{"location":"api/enrichment/#main-classes","title":"Main Classes","text":""},{"location":"api/enrichment/#renalprog.enrichment.EnrichmentPipeline","title":"EnrichmentPipeline","text":"<pre><code>EnrichmentPipeline(\n    trajectory_dir: str,\n    output_dir: str,\n    cancer_type: str = \"kirc\",\n    data_dir: Optional[str] = None,\n    metadata_dir: Optional[str] = None,\n    control_data_dir: Optional[str] = None,\n    control_metadata_dir: Optional[str] = None,\n    gsea_path: str = \"./GSEA_4.3.2/gsea-cli.sh\",\n    pathways_file: str = \"data/external/ReactomePathways.gmt\",\n    n_threads: int = 4,\n)\n</code></pre> <p>Main pipeline for dynamic enrichment analysis using PyDESeq2 and GSEA.</p> <p>This class orchestrates the complete enrichment analysis workflow:</p> <ol> <li>PyDESeq2 Differential Expression Analysis</li> <li>Converts log2(RSEM+1) data back to integer RSEM counts</li> <li>Runs PyDESeq2 for each trajectory timepoint vs controls</li> <li> <p>Generates statistically valid log2FoldChange values</p> </li> <li> <p>GSEA Pathway Enrichment</p> </li> <li>Creates ranked gene lists (.rnk files) from DESeq2 results</li> <li>Executes GSEA in parallel with ReactomePathways gene sets</li> <li> <p>Collects pathway enrichment scores (NES, p-values, FDR)</p> </li> <li> <p>Result Processing and Visualization</p> </li> <li>Combines GSEA results across all trajectories and timepoints</li> <li>Generates pathway enrichment heatmaps</li> </ol> <p>IMPORTANT: This pipeline uses PyDESeq2 for proper differential expression. DO NOT bypass this with simple fold-change calculations.</p> <p>Initialize enrichment pipeline.</p> <p>Args:     trajectory_dir: Directory containing trajectory CSV files     output_dir: Output directory for enrichment results     cancer_type: Cancer type ('kirc', 'lobular', 'ductal')     data_dir: Path to preprocessed RNA-seq data     metadata_dir: Path to clinical metadata     control_data_dir: Path to control RNA-seq data     control_metadata_dir: Path to control metadata     gsea_path: Path to GSEA CLI tool     pathways_file: Path to pathways GMT file     n_threads: Number of parallel threads</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def __init__(\n    self,\n    trajectory_dir: str,\n    output_dir: str,\n    cancer_type: str = 'kirc',\n    data_dir: Optional[str] = None,\n    metadata_dir: Optional[str] = None,\n    control_data_dir: Optional[str] = None,\n    control_metadata_dir: Optional[str] = None,\n    gsea_path: str = './GSEA_4.3.2/gsea-cli.sh',\n    pathways_file: str = 'data/external/ReactomePathways.gmt',\n    n_threads: int = 4\n):\n    \"\"\"\n    Initialize enrichment pipeline.\n\n    Args:\n        trajectory_dir: Directory containing trajectory CSV files\n        output_dir: Output directory for enrichment results\n        cancer_type: Cancer type ('kirc', 'lobular', 'ductal')\n        data_dir: Path to preprocessed RNA-seq data\n        metadata_dir: Path to clinical metadata\n        control_data_dir: Path to control RNA-seq data\n        control_metadata_dir: Path to control metadata\n        gsea_path: Path to GSEA CLI tool\n        pathways_file: Path to pathways GMT file\n        n_threads: Number of parallel threads\n    \"\"\"\n    self.trajectory_dir = Path(trajectory_dir)\n    self.output_dir = Path(output_dir)\n    self.cancer_type = cancer_type\n    self.n_threads = n_threads\n    self.gsea_path = Path(gsea_path)\n    self.pathways_file = Path(pathways_file)\n\n    # Set default data paths if not provided\n    if data_dir is None:\n        # Find latest preprocessed data\n        data_dir = self._find_latest_preprocessed_data()\n    if metadata_dir is None:\n        metadata_dir = self._find_latest_preprocessed_metadata()\n    if control_data_dir is None:\n        control_data_dir = PATHS['processed'] / 'controls' / 'KIRC' / 'rnaseq_control.csv'\n    if control_metadata_dir is None:\n        control_metadata_dir = PATHS['processed'] / 'controls' / 'KIRC' / 'clinical_control.csv'\n\n    self.data_dir = Path(data_dir)\n    self.metadata_dir = Path(metadata_dir)\n    self.control_data_dir = Path(control_data_dir)\n    self.control_metadata_dir = Path(control_metadata_dir)\n\n    # Create output directories\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n    self.deseq_dir = self.output_dir / 'deseq'\n    self.gsea_dir = self.output_dir / 'gsea'\n    self.deseq_dir.mkdir(exist_ok=True)\n    self.gsea_dir.mkdir(exist_ok=True)\n\n    logger.info(f\"Initialized EnrichmentPipeline for {cancer_type}\")\n    logger.info(f\"  Trajectory dir: {self.trajectory_dir}\")\n    logger.info(f\"  Output dir: {self.output_dir}\")\n    logger.info(f\"  Threads: {self.n_threads}\")\n</code></pre>"},{"location":"api/enrichment/#renalprog.enrichment.EnrichmentPipeline-functions","title":"Functions","text":""},{"location":"api/enrichment/#renalprog.enrichment.EnrichmentPipeline.run","title":"run","text":"<pre><code>run(skip_deseq: bool = False, skip_gsea: bool = False, cleanup: bool = False)\n</code></pre> <p>Run the complete enrichment pipeline.</p> <p>Args:     skip_deseq: Skip DESeq processing (use if already completed)     skip_gsea: Skip GSEA analysis (use if already completed)     cleanup: Remove intermediate files after processing</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def run(\n    self,\n    skip_deseq: bool = False,\n    skip_gsea: bool = False,\n    cleanup: bool = False\n):\n    \"\"\"\n    Run the complete enrichment pipeline.\n\n    Args:\n        skip_deseq: Skip DESeq processing (use if already completed)\n        skip_gsea: Skip GSEA analysis (use if already completed)\n        cleanup: Remove intermediate files after processing\n    \"\"\"\n    logger.info(\"Starting enrichment analysis pipeline\")\n\n    # Step 1: Process trajectories for DESeq\n    if not skip_deseq:\n        logger.info(\"Step 1/4: Processing trajectories for DESeq...\")\n        self._run_deseq_processing()\n    else:\n        logger.info(\"Step 1/4: Skipping DESeq processing\")\n\n    # Step 2: Run GSEA in parallel\n    if not skip_gsea:\n        logger.info(\"Step 2/4: Running GSEA analysis...\")\n        self._run_gsea_parallel()\n    else:\n        logger.info(\"Step 2/4: Skipping GSEA analysis\")\n\n    # Step 3: Combine GSEA results\n    logger.info(\"Step 3/4: Combining GSEA results...\")\n    final_df = self._combine_gsea_results()\n\n    # Save final dataset\n    output_path = self.output_dir / 'trajectory_enrichment.csv'\n    final_df.to_csv(output_path, index=False)\n    logger.info(f\"Final enrichment dataset saved to: {output_path}\")\n    logger.info(f\"Dataset shape: {final_df.shape}\")\n\n    # Step 4: Generate pathway enrichment heatmap\n    logger.info(\"Step 4/4: Generating pathway enrichment heatmap...\")\n    try:\n        heatmap_data, heatmap_fig = generate_pathway_heatmap(\n            enrichment_df=final_df,\n            output_dir=self.output_dir\n        )\n        logger.info(f\"Heatmap generated with {heatmap_data.shape[0]} significant pathways\")\n    except Exception as e:\n        logger.error(f\"Failed to generate heatmap: {e}\", exc_info=True)\n        logger.warning(\"Continuing without heatmap...\")\n\n    # Cleanup if requested\n    if cleanup:\n        logger.info(\"Cleaning up intermediate files...\")\n        self._cleanup()\n\n    return final_df\n</code></pre>"},{"location":"api/enrichment/#renalprog.enrichment.EnrichmentPipeline._run_deseq_processing","title":"_run_deseq_processing","text":"<pre><code>_run_deseq_processing()\n</code></pre> <p>Process all trajectory files for DESeq analysis.</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def _run_deseq_processing(self):\n    \"\"\"Process all trajectory files for DESeq analysis.\"\"\"\n    # Find all trajectory CSV files\n    trajectory_files = list(self.trajectory_dir.glob('*.csv'))\n\n    if not trajectory_files:\n        raise ValueError(f\"No trajectory files found in {self.trajectory_dir}\")\n\n    logger.info(f\"Found {len(trajectory_files)} trajectory files\")\n\n    # Load data once\n    rnaseq_data, clinical_data, control_data, control_metadata, gene_list = self._load_data()\n\n    # Process files in parallel\n    with ProcessPoolExecutor(max_workers=self.n_threads) as executor:\n        futures = []\n        for traj_file in trajectory_files:\n            future = executor.submit(\n                process_trajectory_file,\n                traj_file=traj_file,\n                rnaseq_data=rnaseq_data,\n                clinical_data=clinical_data,\n                control_data=control_data,\n                control_metadata=control_metadata,\n                gene_list=gene_list,\n                output_dir=self.deseq_dir,\n                cancer_type=self.cancer_type,\n                gsea_path=self.gsea_path,\n                pathways_file=self.pathways_file\n            )\n            futures.append(future)\n\n        # Track progress\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"DESeq processing\"):\n            try:\n                future.result()\n            except Exception as e:\n                logger.error(f\"Error processing file: {e}\")\n\n    logger.info(\"DESeq processing complete\")\n\n    # Validate that files were created\n    rnk_files = list(self.deseq_dir.rglob('*.rnk'))\n    cmd_files = list(self.deseq_dir.glob('*.cmd'))\n\n    logger.info(f\"Created {len(rnk_files)} .rnk files\")\n    logger.info(f\"Created {len(cmd_files)} .cmd files\")\n\n    if len(rnk_files) == 0:\n        logger.error(\"No .rnk files were created during DESeq processing\")\n        raise ValueError(\"DESeq processing failed to create rank files\")\n\n    if len(cmd_files) == 0:\n        logger.error(\"No .cmd files were created during DESeq processing\")\n        raise ValueError(\"DESeq processing failed to create command files\")\n</code></pre>"},{"location":"api/enrichment/#renalprog.enrichment.EnrichmentPipeline._combine_gsea_results","title":"_combine_gsea_results","text":"<pre><code>_combine_gsea_results() -&gt; pd.DataFrame\n</code></pre> <p>Combine all GSEA results into a single dataset.</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def _combine_gsea_results(self) -&gt; pd.DataFrame:\n    \"\"\"Combine all GSEA results into a single dataset.\"\"\"\n    # Load pathways\n    pathways = load_pathways_from_gmt(self.pathways_file)\n\n    # Diagnostic: show directory structure\n    logger.info(f\"Scanning GSEA results in: {self.deseq_dir}\")\n    logger.info(\"Directory structure:\")\n    for item in self.deseq_dir.iterdir():\n        if item.is_dir():\n            logger.info(f\"  {item.name}/\")\n            for subitem in item.iterdir():\n                if subitem.is_dir():\n                    gsea_dirs = list(subitem.glob('gsea_tp*'))\n                    logger.info(f\"    {subitem.name}/ ({len(gsea_dirs)} gsea_tp* dirs)\")\n                    # Show deeper structure for debugging\n                    for gsea_dir in gsea_dirs[:2]:  # Show first 2\n                        subdirs = [d.name for d in gsea_dir.iterdir() if d.is_dir()]\n                        files = [f.name for f in gsea_dir.iterdir() if f.is_file()]\n                        logger.debug(f\"      {gsea_dir.name}/ subdirs={subdirs[:3]}, files={files[:3]}\")\n        else:\n            logger.info(f\"  {item.name}\")\n\n    # Find all trajectory directories (transition/patient structure)\n    # First, look for transition directories\n    transition_dirs = [d for d in self.deseq_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]\n\n    logger.info(f\"Found {len(transition_dirs)} transition directories\")\n\n    all_results = []\n    failed_trajectories = []\n    skipped_trajectories = []\n\n    for transition_dir in transition_dirs:\n        # Find patient directories within each transition\n        patient_dirs = [d for d in transition_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]\n        logger.info(f\"  Found {len(patient_dirs)} patient directories in {transition_dir.name}\")\n\n        for patient_dir in tqdm(patient_dirs, desc=f\"Processing {transition_dir.name}\"):\n            try:\n                result = process_trajectory_results(patient_dir, pathways)\n                if result is not None:\n                    all_results.append(result)\n                else:\n                    skipped_trajectories.append(str(patient_dir))\n                    logger.warning(f\"No results for {patient_dir}\")\n            except Exception as e:\n                failed_trajectories.append((str(patient_dir), str(e)))\n                logger.error(f\"Error processing {patient_dir}: {e}\", exc_info=True)\n\n    # Report statistics\n    logger.info(f\"Successfully processed: {len(all_results)} trajectories\")\n    logger.info(f\"Skipped (no GSEA dirs): {len(skipped_trajectories)} trajectories\")\n    logger.info(f\"Failed (exceptions): {len(failed_trajectories)} trajectories\")\n\n    if failed_trajectories:\n        logger.warning(\"Failed trajectories:\")\n        for path, error in failed_trajectories[:10]:  # Show first 10\n            logger.warning(f\"  {path}: {error}\")\n\n    if not all_results:\n        logger.error(f\"No GSEA results found in {self.deseq_dir}\")\n        logger.error(\"Directory structure should be: deseq_dir/transition/patient/gsea_tp*/\")\n        logger.error(f\"Transition dirs found: {len(transition_dirs)}\")\n        logger.error(f\"Total patient dirs scanned: {len(skipped_trajectories) + len(failed_trajectories)}\")\n\n        # Provide more specific error message\n        if failed_trajectories:\n            logger.error(f\"All trajectories failed with errors. First error: {failed_trajectories[0][1]}\")\n        elif skipped_trajectories:\n            logger.error(\"All trajectories were skipped (no gsea_tp* directories found)\")\n\n        raise ValueError(\"No GSEA results found to combine\")\n\n    # Combine all results\n    final_df = pd.concat(all_results, axis=0, ignore_index=True)\n\n    return final_df\n</code></pre>"},{"location":"api/enrichment/#functions","title":"Functions","text":""},{"location":"api/enrichment/#differential-expression","title":"Differential Expression","text":""},{"location":"api/enrichment/#renalprog.enrichment.run_deseq2_analysis","title":"run_deseq2_analysis","text":"<pre><code>run_deseq2_analysis(\n    sample_data: DataFrame,\n    control_data: DataFrame,\n    control_metadata: DataFrame,\n    gene_list: ndarray,\n    sample_name: str,\n    stage_transition: str,\n) -&gt; pd.Series\n</code></pre> <p>Perform DESeq2 differential expression analysis between sample and controls.</p> <p>This function properly: 1. Converts log2(RSEM+1) data back to RSEM integer counts 2. Runs PyDESeq2 analysis to get statistically valid log2FoldChange values 3. Returns ranked gene list for GSEA</p> <p>Args:     sample_data: Sample expression data (genes x 1) in log2(RSEM+1) format     control_data: Control expression data (genes x samples) in log2(RSEM+1) format     control_metadata: Control clinical metadata with stage information     gene_list: List of genes     sample_name: Name/ID of the sample     stage_transition: Stage transition label (e.g., 'early_to_late', 'I_to_II')</p> <p>Returns:     Series of log2FoldChange values sorted for GSEA input</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def run_deseq2_analysis(\n    sample_data: pd.DataFrame,\n    control_data: pd.DataFrame,\n    control_metadata: pd.DataFrame,\n    gene_list: np.ndarray,\n    sample_name: str,\n    stage_transition: str\n) -&gt; pd.Series:\n    \"\"\"\n    Perform DESeq2 differential expression analysis between sample and controls.\n\n    This function properly:\n    1. Converts log2(RSEM+1) data back to RSEM integer counts\n    2. Runs PyDESeq2 analysis to get statistically valid log2FoldChange values\n    3. Returns ranked gene list for GSEA\n\n    Args:\n        sample_data: Sample expression data (genes x 1) in log2(RSEM+1) format\n        control_data: Control expression data (genes x samples) in log2(RSEM+1) format\n        control_metadata: Control clinical metadata with stage information\n        gene_list: List of genes\n        sample_name: Name/ID of the sample\n        stage_transition: Stage transition label (e.g., 'early_to_late', 'I_to_II')\n\n    Returns:\n        Series of log2FoldChange values sorted for GSEA input\n    \"\"\"\n    # Debug logging\n    logger.debug(f\"DESeq2 analysis for {sample_name}\")\n    logger.debug(f\"  Sample data shape: {sample_data.shape}\")\n    logger.debug(f\"  Control data shape: {control_data.shape}\")\n    logger.debug(f\"  Gene list length: {len(gene_list)}\")\n\n    # 1. Convert from log2(RSEM+1) back to RSEM counts\n    # The preprocessed data is in log2(RSEM+1) format, so we reverse this transformation\n    # and round to integers as required by DESeq2\n    sample_counts = 2 ** sample_data.values.flatten() - 1\n    control_counts = 2 ** control_data.values - 1\n\n    # Clip negative values to 0 (can occur due to numerical precision or interpolation)\n    sample_counts = np.clip(sample_counts, 0, None)\n    control_counts = np.clip(control_counts, 0, None)\n\n    # Round to integers as required by DESeq2\n    sample_counts = np.round(sample_counts).astype(int)\n    control_counts = np.round(control_counts).astype(int)\n\n    # Ensure control_counts is 2D (genes x samples)\n    if control_counts.ndim == 1:\n        control_counts = control_counts[:, np.newaxis]\n\n    # Validate counts\n    if np.any(sample_counts &lt; 0):\n        logger.error(f\"Sample has negative counts after conversion: min={sample_counts.min()}\")\n        raise ValueError(\"Sample counts contain negative values after conversion\")\n    if np.any(control_counts &lt; 0):\n        logger.error(f\"Controls have negative counts after conversion: min={control_counts.min()}\")\n        raise ValueError(\"Control counts contain negative values after conversion\")\n\n    logger.debug(f\"  Sample counts: min={sample_counts.min()}, max={sample_counts.max()}, mean={sample_counts.mean():.1f}\")\n    logger.debug(f\"  Control counts: min={control_counts.min()}, max={control_counts.max()}, mean={control_counts.mean():.1f}\")\n\n    # Ensure sample_counts has the same number of genes as control_counts\n    if len(sample_counts) != control_counts.shape[0]:\n        raise ValueError(\n            f\"Sample has {len(sample_counts)} genes but controls have {control_counts.shape[0]} genes. \"\n            f\"Sample shape: {sample_data.shape}, Control shape: {control_data.shape}\"\n        )\n\n    # 2. Combine sample with controls\n    # Create count matrix: genes (rows) x samples (columns)\n    # sample_counts is 1D (n_genes,), control_counts is 2D (n_genes, n_controls)\n    # We need to reshape sample_counts to (n_genes, 1) to stack horizontally\n    counts_matrix = np.column_stack([sample_counts.reshape(-1, 1), control_counts])\n\n    counts_df = pd.DataFrame(\n        counts_matrix,\n        index=gene_list,\n        columns=[sample_name] + list(control_data.columns)\n    )\n\n    # 3. Create metadata DataFrame with condition labels\n    metadata_df = pd.DataFrame({\n        'condition': [stage_transition] + list(control_metadata['ajcc_pathologic_tumor_stage'].values)\n    }, index=counts_df.columns)\n\n    # 4. Run PyDESeq2 analysis\n    # Initialize DESeqDataSet\n    dds = DeseqDataSet(\n        counts=counts_df.T,  # DESeq2 expects samples as rows, genes as columns\n        metadata=metadata_df,\n        design_factors='condition',\n        refit_cooks=True,\n        quiet=True  # Suppress verbose output in parallel processing\n    )\n\n    # Fit dispersions and log-fold changes\n    dds.deseq2()\n\n    # Statistical analysis\n    stat_res = DeseqStats(\n        dds,\n        alpha=0.05,\n        cooks_filter=True,\n        independent_filter=True,\n        quiet=True\n    )\n    stat_res.summary()\n\n    # 5. Extract log2FoldChange values\n    results_df = stat_res.results_df\n\n    # Ensure we have log2FoldChange column\n    if 'log2FoldChange' not in results_df.columns:\n        raise ValueError(f\"DESeq2 results missing 'log2FoldChange' column. Available columns: {results_df.columns.tolist()}\")\n\n    # Extract log2FoldChange and sort by absolute value (for GSEA ranking)\n    log2fc = results_df['log2FoldChange'].copy()\n\n    # Replace NaN values with 0 (genes with no change or insufficient data)\n    log2fc = log2fc.fillna(0)\n\n    # Sort by absolute value descending (GSEA expects ranked list)\n    log2fc = log2fc.sort_values(ascending=False)\n\n    logger.debug(f\"  DESeq2 complete: {len(log2fc)} genes, range [{log2fc.min():.2f}, {log2fc.max():.2f}]\")\n\n    return log2fc\n</code></pre>"},{"location":"api/enrichment/#gsea-analysis","title":"GSEA Analysis","text":""},{"location":"api/enrichment/#renalprog.enrichment.run_gsea_command","title":"run_gsea_command","text":"<pre><code>run_gsea_command(cmd: str) -&gt; bool\n</code></pre> <p>Run a single GSEA command.</p> <p>Args:     cmd: GSEA command string</p> <p>Returns:     True if successful, False otherwise</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def run_gsea_command(cmd: str) -&gt; bool:\n    \"\"\"\n    Run a single GSEA command.\n\n    Args:\n        cmd: GSEA command string\n\n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    try:\n        result = subprocess.run(\n            cmd,\n            shell=True,\n            capture_output=True,\n            text=True,\n            timeout=600  # 10 minute timeout per command\n        )\n\n        if result.returncode != 0:\n            logger.error(f\"GSEA command failed with return code {result.returncode}\")\n            logger.error(f\"Command: {cmd[:200]}...\")  # Truncate long commands\n            logger.error(f\"STDERR: {result.stderr[:500]}\")  # First 500 chars\n            if result.stdout:\n                logger.error(f\"STDOUT: {result.stdout[:500]}\")\n            return False\n\n        # Check for Java errors in stdout (GSEA sometimes returns 0 even on error)\n        if result.stdout and (\"error\" in result.stdout.lower() or \"exception\" in result.stdout.lower()):\n            logger.warning(f\"GSEA command may have failed (found error in output)\")\n            logger.warning(f\"Command: {cmd[:200]}...\")\n            logger.warning(f\"Output: {result.stdout[:500]}\")  # First 500 chars\n            return False\n\n        # Check for GSEA-specific success indicators\n        if result.stdout and \"Enrichment score\" in result.stdout:\n            logger.debug(f\"GSEA command completed successfully\")\n            return True\n        elif result.stdout:\n            # Log a sample of output for debugging\n            logger.debug(f\"GSEA output sample: {result.stdout[:200]}\")\n\n        return True\n\n    except subprocess.TimeoutExpired:\n        logger.error(f\"GSEA command timed out after 600s\")\n        logger.error(f\"Command: {cmd[:200]}...\")\n        return False\n    except Exception as e:\n        logger.error(f\"Error running GSEA command: {e}\")\n        logger.error(f\"Command: {cmd[:200]}...\")\n        return False\n</code></pre>"},{"location":"api/enrichment/#visualization","title":"Visualization","text":""},{"location":"api/enrichment/#renalprog.enrichment.generate_pathway_heatmap","title":"generate_pathway_heatmap","text":"<pre><code>generate_pathway_heatmap(\n    enrichment_df: DataFrame,\n    output_dir: str,\n    fdr_threshold: float = 0.05,\n    colorbar: bool = True,\n    legend: bool = False,\n    yticks_fontsize: int = 12,\n    show: bool = False,\n) -&gt; Tuple[pd.DataFrame, Dict[str, matplotlib.figure.Figure]]\n</code></pre> <p>Generate multiple pathway enrichment heatmaps from GSEA results.</p> <p>This function creates several heatmaps showing the sum of NES (Normalized Enrichment Score) across all trajectories for each pathway at each timepoint:</p> <ol> <li>Top 50 most changing pathways (first vs last timepoint)</li> <li>Top 50 most upregulated pathways (average NES &gt; 0)</li> <li>Top 50 most downregulated pathways (average NES &lt; 0)</li> <li>Selected pathways (high-level Reactome + literature pathways)</li> </ol> <p>The heatmaps have: - Rows: Pathway names - Columns: Timepoints (pseudo-time from early to late) - Values: Sum of NES across all trajectories at each timepoint</p> <p>Args:     enrichment_df: DataFrame with columns [Patient, Idx, Transition, NAME, ES, NES, FDR q-val]     output_dir: Output directory for heatmap files     fdr_threshold: FDR q-value threshold for significance (default: 0.05)     colorbar: Whether to show colorbar (default: True)     legend: Whether to show legend (default: False)     yticks_fontsize: Font size for y-axis tick labels (default: 12)     show: Whether to display the plot (default: False)</p> <p>Returns:     Tuple of (heatmap_data, figures_dict):         - heatmap_data: DataFrame with summed NES values (pathways \u00d7 timepoints)         - figures_dict: Dictionary mapping figure names to Matplotlib Figure objects</p> <p>Example:     &gt;&gt;&gt; enrichment_df = pd.read_csv('trajectory_enrichment.csv')     &gt;&gt;&gt; heatmap_data, figs = generate_pathway_heatmap(     ...     enrichment_df=enrichment_df,     ...     output_dir='results/',     ...     fdr_threshold=0.05     ... )     &gt;&gt;&gt; print(f\"Generated {len(figs)} heatmaps\")</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def generate_pathway_heatmap(\n    enrichment_df: pd.DataFrame,\n    output_dir: str,\n    fdr_threshold: float = 0.05,\n    colorbar: bool = True,\n    legend: bool = False,\n    yticks_fontsize: int = 12,\n    show: bool = False\n) -&gt; Tuple[pd.DataFrame, Dict[str, 'matplotlib.figure.Figure']]:\n    \"\"\"\n    Generate multiple pathway enrichment heatmaps from GSEA results.\n\n    This function creates several heatmaps showing the sum of NES (Normalized Enrichment Score)\n    across all trajectories for each pathway at each timepoint:\n\n    1. Top 50 most changing pathways (first vs last timepoint)\n    2. Top 50 most upregulated pathways (average NES &gt; 0)\n    3. Top 50 most downregulated pathways (average NES &lt; 0)\n    4. Selected pathways (high-level Reactome + literature pathways)\n\n    The heatmaps have:\n    - Rows: Pathway names\n    - Columns: Timepoints (pseudo-time from early to late)\n    - Values: Sum of NES across all trajectories at each timepoint\n\n    Args:\n        enrichment_df: DataFrame with columns [Patient, Idx, Transition, NAME, ES, NES, FDR q-val]\n        output_dir: Output directory for heatmap files\n        fdr_threshold: FDR q-value threshold for significance (default: 0.05)\n        colorbar: Whether to show colorbar (default: True)\n        legend: Whether to show legend (default: False)\n        yticks_fontsize: Font size for y-axis tick labels (default: 12)\n        show: Whether to display the plot (default: False)\n\n    Returns:\n        Tuple of (heatmap_data, figures_dict):\n            - heatmap_data: DataFrame with summed NES values (pathways \u00d7 timepoints)\n            - figures_dict: Dictionary mapping figure names to Matplotlib Figure objects\n\n    Example:\n        &gt;&gt;&gt; enrichment_df = pd.read_csv('trajectory_enrichment.csv')\n        &gt;&gt;&gt; heatmap_data, figs = generate_pathway_heatmap(\n        ...     enrichment_df=enrichment_df,\n        ...     output_dir='results/',\n        ...     fdr_threshold=0.05\n        ... )\n        &gt;&gt;&gt; print(f\"Generated {len(figs)} heatmaps\")\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as mpatches\n    import matplotlib.colors as mcolors\n\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"Generating pathway enrichment heatmaps (FDR &lt; {fdr_threshold})...\")\n\n    # Define pathway lists\n    highest_pathways = [\n        \"Autophagy\",\n        \"Cell Cycle\",\n        \"Cell-Cell communication\",\n        \"Cellular responses to stimuli\",\n        \"Chromatin organization\",\n        \"Circadian Clock\",\n        \"DNA Repair\",\n        \"DNA Replication\",\n        \"Developmental Biology\",\n        \"Digestion and absorption\",\n        \"Disease\",\n        \"Drug ADME\",\n        \"Extracellular matrix organization\",\n        \"Gene expression (Transcription)\",\n        \"Hemostasis\",\n        \"Immune System\",\n        \"Metabolism\",\n        \"Metabolism of RNA\",\n        \"Metabolism of proteins\",\n        \"Muscle contraction\",\n        \"Neuronal System\",\n        \"Organelle biogenesis and maintenance\",\n        \"Programmed Cell Death\",\n        \"Protein localization\",\n        \"Reproduction\",\n        \"Sensory Perception\",\n        \"Signal Transduction\",\n        \"Transport of small molecules\",\n        \"Vesicle-mediated transport\",\n    ]\n\n    pathways_literature = [\n        # VHL/HIF pathway\n        \"CELLULAR RESPONSE TO HYPOXIA\",\n        \"OXYGEN-DEPENDENT PROLINE HYDROXYLATION OF HYPOXIA-INDUCIBLE FACTOR ALPHA\",\n        \"REGULATION OF GENE EXPRESSION BY HYPOXIA-INDUCIBLE FACTOR\",\n        # PI3K/AKT/MTOR Pathway\n        \"PI3K/AKT ACTIVATION\",\n        \"PI3K/AKT SIGNALING IN CANCER\",\n        \"MTOR SIGNALLING\",\n        # Warburg effect\n        \"TP53 REGULATES METABOLIC GENES\",\n        \"GLYCOLYSIS\",\n        \"GLUCOSE METABOLISM\",\n        # TCA/Krebs cycle\n        \"CITRIC ACID CYCLE (TCA CYCLE)\",\n        \"THE CITRIC ACID (TCA) CYCLE AND RESPIRATORY ELECTRON TRANSPORT\",\n        # Pentose phosphate pathway\n        \"NFE2L2 REGULATES PENTOSE PHOSPHATE PATHWAY GENES\",\n        \"PENTOSE PHOSPHATE PATHWAY\",\n        \"PENTOSE PHOSPHATE PATHWAY DISEASE\",\n        # Fatty Acid Metabolism\n        \"FATTY ACID METABOLISM\",\n        # Glutamine metabolism\n        \"GLUTAMATE AND GLUTAMINE METABOLISM\",\n        # EGFR\n        \"SIGNALING BY EGFR\",\n        \"SIGNALING BY EGFR IN CANCER\",\n        \"EGFR DOWNREGULATION\",\n        # TGF-\u03b2 signaling\n        \"SIGNALING BY TGF-BETA RECEPTOR COMPLEX\",\n        \"TGF-BETA RECEPTOR SIGNALING IN EMT (EPITHELIAL TO MESENCHYMAL TRANSITION)\",\n        \"SIGNALING BY TGF-BETA RECEPTOR COMPLEX IN CANCER\",\n        \"SIGNALING BY TGFB FAMILY MEMBERS\",\n        \"TGF-BETA RECEPTOR SIGNALING ACTIVATES SMADS\",\n        # Wnt/\u03b2-catenin pathway\n        \"BETA-CATENIN INDEPENDENT WNT SIGNALING\",\n        \"SIGNALING BY WNT\",\n        # SLIT-2-ROBO1 pathways\n        \"REGULATION OF EXPRESSION OF SLITS AND ROBOS\",\n        # DNA repair\n        \"DNA REPAIR\",\n        # Energy homeostasis\n        \"ION HOMEOSTASIS\",\n        # Apoptosis\n        \"APOPTOSIS\",\n        # Angiogenesis\n        \"SIGNALING BY VEGF\"\n    ]\n\n    # Step 1: Ensure numeric types for NES and FDR q-val\n    enrichment_df = enrichment_df.copy()\n    enrichment_df['NES'] = pd.to_numeric(enrichment_df['NES'], errors='coerce')\n    enrichment_df['FDR q-val'] = pd.to_numeric(enrichment_df['FDR q-val'], errors='coerce')\n    enrichment_df['Idx'] = pd.to_numeric(enrichment_df['Idx'], errors='coerce')\n\n    # Log any rows that had non-numeric values\n    invalid_nes = enrichment_df['NES'].isna().sum()\n    invalid_fdr = enrichment_df['FDR q-val'].isna().sum()\n    if invalid_nes &gt; 0:\n        logger.warning(f\"Found {invalid_nes} rows with non-numeric NES values (converted to NaN)\")\n    if invalid_fdr &gt; 0:\n        logger.warning(f\"Found {invalid_fdr} rows with non-numeric FDR q-val values (converted to NaN)\")\n\n    # Step 2: Filter by FDR threshold\n    significant = enrichment_df[enrichment_df['FDR q-val'] &lt; fdr_threshold].copy()\n\n    logger.info(f\"Found {significant.shape[0]} significant pathway enrichments (FDR &lt; {fdr_threshold})\")\n\n    if significant.empty:\n        logger.warning(\"No significant pathways found. Cannot generate heatmap.\")\n        # Return empty results\n        empty_df = pd.DataFrame()\n        empty_dict = {}\n        return empty_df, empty_dict\n\n    # Step 3: Group by Timepoint (Idx) and Pathway (NAME), sum NES across all trajectories\n    pathway_summary = significant.groupby(['Idx', 'NAME'])['NES'].sum().reset_index()\n\n    logger.info(f\"Aggregated results for {pathway_summary['Idx'].nunique()} timepoints \"\n                f\"and {pathway_summary['NAME'].nunique()} pathways\")\n\n    # Step 4: Pivot to create matrix (pathways \u00d7 timepoints)\n    heatmap_data = pathway_summary.pivot(\n        index='NAME',\n        columns='Idx',\n        values='NES'\n    ).fillna(0)  # Fill missing with 0\n\n    logger.info(f\"Full heatmap dimensions: {heatmap_data.shape[0]} pathways \u00d7 {heatmap_data.shape[1]} timepoints\")\n\n    # Save full summary data\n    summary_file = output_dir / 'pathway_nes_summary.csv'\n    heatmap_data.to_csv(summary_file)\n    logger.info(f\"Saved pathway NES summary to: {summary_file}\")\n\n    # Dictionary to store all figures\n    figures = {}\n\n    # Helper function to create and save a heatmap\n    def plot_heatmap_regulation(df_plot, unique_pathways, cmap_here='viridis',\n                               save_name=None, colorbar_title='Sum of NES'):\n        \"\"\"Plot heatmap following paper_figures.ipynb style\"\"\"\n        # Generate a range of locations for the ticks\n        tick_locations = range(len(unique_pathways))\n        z_min, z_max = df_plot.min().min(), df_plot.max().max()\n\n        # Make the range symmetric around 0\n        if z_min &lt; 0 and z_max &gt; 0:\n            abs_max = max(abs(z_min), abs(z_max))\n            z_min, z_max = -abs_max, abs_max\n            norm = mcolors.TwoSlopeNorm(vmin=z_min, vcenter=0, vmax=z_max)\n        else:\n            # If all values are positive or all negative, use regular normalization\n            norm = mcolors.Normalize(vmin=z_min, vmax=z_max)\n            logger.warning(f\"Cannot center colormap at zero: range [{z_min:.3f}, {z_max:.3f}] does not cross zero\")\n\n        fig, ax = plt.subplots(figsize=(30, 10))\n\n        # Make heatmap\n        cax = ax.imshow(df_plot.values, cmap=cmap_here, norm=norm, aspect='auto')\n\n        # Set the y-ticks\n        plt.yticks(tick_locations, unique_pathways, fontsize=yticks_fontsize)\n\n        # Set the x-ticks at specific positions\n        num_timepoints = df_plot.shape[1]\n        ax.set_xticks([0, num_timepoints - 1])\n        ax.set_xticklabels(['early', 'late'], fontsize=yticks_fontsize*1.33, rotation=45)\n        ax.set_xlabel('Pseudo-Time', fontsize=yticks_fontsize*1.33)\n\n        # Get x and y axis range\n        ymin, ymax = ax.get_ylim()\n        xmin, xmax = ax.get_xlim()\n\n        # Custom x and y ticks\n        x_custom = np.arange(xmin, xmax, step=1)\n        y_custom = np.arange(ymax, ymin, step=1)\n\n        # set minor ticks at custom locations:\n        ax.set_xticks(x_custom, minor=True)\n        ax.set_yticks(y_custom, minor=True)\n\n        # Add grid lines at both major and minor ticks\n        plt.grid(False, which='major')\n        plt.grid(True, which='minor', color='black', linestyle='-', linewidth=1)\n\n        # Remove ticks\n        ax.tick_params(axis='both', which='minor', length=0)\n\n        # Add colorbar\n        if colorbar:\n            cbar = plt.colorbar(cax, shrink=0.7)\n            cbar.set_label(colorbar_title, rotation=270, labelpad=20, fontsize=16)\n\n        # Add legend\n        if legend:\n            colors = np.append(plt.get_cmap(cmap_here)([0, 0.5, 1]), np.array([[1, 1, 1, 1]]), axis=0)\n            labels = ['Downregulated', 'No change', 'Upregulated', 'No data']\n            patches = [\n                mpatches.Patch(facecolor=colors[i], label=labels[i], edgecolor='black')\n                for i in range(len(labels))\n            ]\n            ax.legend(handles=patches, bbox_to_anchor=(1.05, 1),\n                      loc=2, borderaxespad=0., title='Regulation',\n                      fontsize=yticks_fontsize*2, title_fontsize=24)\n\n        # Save figures\n        if save_name:\n            plt.savefig(output_dir / f'{save_name}.pdf', bbox_inches='tight')\n            plt.savefig(output_dir / f'{save_name}.png', bbox_inches='tight', dpi=600)\n            plt.savefig(output_dir / f'{save_name}.svg', bbox_inches='tight',\n                       format='svg', transparent=True)\n            logger.info(f\"Saved heatmap to: {output_dir / save_name}.{{pdf,png,svg}}\")\n\n        if show:\n            plt.show()\n        else:\n            plt.close()\n\n        return fig\n\n    # 1. Top 50 most changing pathways (first vs last timepoint)\n    logger.info(\"Creating heatmap 1/5: Top 50 most changing pathways...\")\n    first_col = heatmap_data.columns[0]\n    last_col = heatmap_data.columns[-1]\n    change = (heatmap_data[last_col] - heatmap_data[first_col]).abs()\n    top_changing = change.nlargest(50).index.tolist()\n\n    df_top_changing = heatmap_data.loc[top_changing]\n    fig1 = plot_heatmap_regulation(\n        df_top_changing,\n        top_changing,\n        cmap_here='RdBu_r',\n        save_name='top50_most_changing_pathways',\n        colorbar_title='Sum of NES'\n    )\n    figures['top50_changing'] = fig1\n\n    # 2. Top 50 most upregulated pathways (average NES &gt; 0)\n    logger.info(\"Creating heatmap 2/5: Top 50 most upregulated pathways...\")\n    avg_nes = heatmap_data.mean(axis=1)\n    upregulated = avg_nes[avg_nes &gt; 0].nlargest(50).index.tolist()\n\n    df_upregulated = heatmap_data.loc[upregulated]\n    fig2 = plot_heatmap_regulation(\n        df_upregulated,\n        upregulated,\n        cmap_here='YlGn',\n        save_name='top50_most_upregulated_pathways',\n        colorbar_title='Sum of NES'\n    )\n    figures['top50_upregulated'] = fig2\n\n    # 3. Top 50 most downregulated pathways (average NES &lt; 0)\n    logger.info(\"Creating heatmap 3/5: Top 50 most downregulated pathways...\")\n    downregulated = avg_nes[avg_nes &lt; 0].nsmallest(50).index.tolist()\n\n    df_downregulated = heatmap_data.loc[downregulated]\n    fig3 = plot_heatmap_regulation(\n        df_downregulated,\n        downregulated,\n        cmap_here='YlOrBr',\n        save_name='top50_most_downregulated_pathways',\n        colorbar_title='Sum of NES'\n    )\n    figures['top50_downregulated'] = fig3\n\n    # 4. High-level pathways (29 pathways from Reactome highest level)\n    logger.info(\"Creating heatmap 4/5: High-level pathways...\")\n    available_highest = [p for p in highest_pathways if p in heatmap_data.index]\n\n    if available_highest:\n        df_highest = heatmap_data.loc[available_highest]\n        fig4 = plot_heatmap_regulation(\n            df_highest,\n            available_highest,\n            cmap_here='RdBu_r',\n            save_name='selected_pathways_highest_level',\n            colorbar_title='Sum of NES'\n        )\n        figures['selected_highest_level'] = fig4\n        logger.info(f\"Found {len(available_highest)}/{len(highest_pathways)} high-level pathways in data\")\n    else:\n        logger.warning(\"No high-level pathways found in the data\")\n\n    # 5. Literature pathways (33 pathways from literature review)\n    logger.info(\"Creating heatmap 5/5: Literature pathways...\")\n    available_literature = [p for p in pathways_literature if p in heatmap_data.index]\n\n    if available_literature:\n        df_literature = heatmap_data.loc[available_literature]\n        fig5 = plot_heatmap_regulation(\n            df_literature,\n            available_literature,\n            cmap_here='RdBu_r',\n            save_name='selected_pathways_literature',\n            colorbar_title='Sum of NES'\n        )\n        figures['selected_literature'] = fig5\n        logger.info(f\"Found {len(available_literature)}/{len(pathways_literature)} literature pathways in data\")\n    else:\n        logger.warning(\"No literature pathways found in the data\")\n\n    logger.info(f\"Pathway heatmap generation complete. Created {len(figures)} heatmaps.\")\n\n    return heatmap_data, figures\n</code></pre>"},{"location":"api/enrichment/#usage-examples","title":"Usage Examples","text":""},{"location":"api/enrichment/#running-complete-enrichment-pipeline","title":"Running Complete Enrichment Pipeline","text":"<pre><code>from renalprog.enrichment import EnrichmentPipeline\nfrom pathlib import Path\n\n# Initialize pipeline\npipeline = EnrichmentPipeline(\n    cancer_type=\"KIRC\",\n    trajectory_dir=Path(\"data/processed/trajectories\"),\n    output_dir=Path(\"data/processed/enrichment\"),\n    gsea_path=Path(\"./GSEA_4.3.2/gsea-cli.sh\"),\n    pathways_file=Path(\"data/external/ReactomePathways.gmt\"),\n    n_threads=8,\n    n_threads_per_deseq=8\n)\n\n# Run full pipeline\nresults = pipeline.run()\n</code></pre>"},{"location":"api/enrichment/#running-deseq2-analysis","title":"Running DESeq2 Analysis","text":"<pre><code>from renalprog.enrichment import run_deseq2_analysis\nimport pandas as pd\n\n# Load trajectory data\ntrajectory_data = pd.read_csv(\"trajectory_001.csv\", index_col=0)\ncontrol_data = pd.read_csv(\"control.csv\", index_col=0)\n\n# Run DESeq2\nresults_df = run_deseq2_analysis(\n    trajectory_samples=trajectory_data,\n    control_samples=control_data,\n    n_threads=8\n)\n\n# Results contain: log2FoldChange, pvalue, padj, etc.\nprint(results_df.head())\n</code></pre>"},{"location":"api/enrichment/#creating-rnk-files-for-gsea","title":"Creating RNK Files for GSEA","text":"<pre><code>from renalprog.enrichment import create_rnk_file\n\n# Create ranked gene list from DESeq2 results\nrnk_file = create_rnk_file(\n    deseq_results=results_df,\n    output_path=\"analysis/genes.rnk\"\n)\n</code></pre>"},{"location":"api/enrichment/#running-gsea","title":"Running GSEA","text":"<pre><code>from renalprog.enrichment import run_gsea_command\n\n# Run GSEA on ranked gene list\ngsea_output = run_gsea_command(\n    rnk_file=\"analysis/genes.rnk\",\n    gmt_file=\"data/external/ReactomePathways.gmt\",\n    output_dir=\"analysis/gsea_results\",\n    label=\"trajectory_001\",\n    gsea_path=\"./GSEA_4.3.2/gsea-cli.sh\"\n)\n</code></pre>"},{"location":"api/enrichment/#generating-pathway-heatmaps","title":"Generating Pathway Heatmaps","text":"<pre><code>from renalprog.enrichment import generate_pathway_heatmap\n\n# Generate heatmaps from enrichment results\nheatmap_data, figures = generate_pathway_heatmap(\n    enrichment_file=\"data/processed/enrichment/trajectory_enrichment.csv\",\n    output_dir=\"data/processed/enrichment\",\n    fdr_threshold=0.05,\n    n_timepoints=50\n)\n\n# figures contains:\n# - \"top_50_changing\": Most variable pathways\n# - \"top_50_upregulated\": Most upregulated pathways\n# - \"top_50_downregulated\": Most downregulated pathways\n# - \"high_level\": Reactome high-level pathways\n# - \"literature\": Literature-curated pathways\n</code></pre>"},{"location":"api/enrichment/#configuration","title":"Configuration","text":""},{"location":"api/enrichment/#enrichmentpipeline-parameters","title":"EnrichmentPipeline Parameters","text":"<ul> <li><code>cancer_type</code>: Cancer type identifier (e.g., \"KIRC\", \"BRCA\")</li> <li><code>trajectory_dir</code>: Directory containing trajectory CSV files</li> <li><code>output_dir</code>: Directory for output files</li> <li><code>gsea_path</code>: Path to GSEA CLI executable</li> <li><code>pathways_file</code>: Path to GMT file with pathway definitions</li> <li><code>n_threads</code>: Number of parallel threads for processing</li> <li><code>n_threads_per_deseq</code>: Number of threads per DESeq2 job</li> <li><code>memory_per_job_gb</code>: Memory limit per DESeq2 job (default: 12 GB)</li> <li><code>total_memory_gb</code>: Total available memory (default: 224 GB)</li> </ul>"},{"location":"api/enrichment/#gsea-parameters","title":"GSEA Parameters","text":"<ul> <li><code>nperm</code>: Number of permutations (default: 1000)</li> <li><code>set_min</code>: Minimum gene set size (default: 15)</li> <li><code>set_max</code>: Maximum gene set size (default: 500)</li> <li><code>scoring_scheme</code>: GSEA scoring method (default: \"weighted\")</li> <li><code>norm</code>: Normalization method (default: \"meandiv\")</li> </ul>"},{"location":"api/enrichment/#pathway-collections","title":"Pathway Collections","text":""},{"location":"api/enrichment/#high-level-reactome-pathways","title":"High-Level Reactome Pathways","text":"<p>29 top-level biological processes: - Autophagy - Cell Cycle - DNA Repair - Immune System - Metabolism - Signal Transduction - And more...</p>"},{"location":"api/enrichment/#literature-curated-pathways","title":"Literature-Curated Pathways","text":"<p>33 pathways from literature review: - VHL/HIF pathway - PI3K/AKT/MTOR pathway - Warburg effect - TCA cycle - And more...</p>"},{"location":"api/enrichment/#output-files","title":"Output Files","text":""},{"location":"api/enrichment/#deseq2-results","title":"DESeq2 Results","text":"<ul> <li><code>{trajectory_id}_deseq_results.csv</code>: Complete DESeq2 output</li> <li><code>{trajectory_id}.rnk</code>: Ranked gene list for GSEA</li> </ul>"},{"location":"api/enrichment/#gsea-results","title":"GSEA Results","text":"<ul> <li><code>{trajectory_id}/gsea_report_for_na_pos_{timestamp}.tsv</code>: Positive enrichment</li> <li><code>{trajectory_id}/gsea_report_for_na_neg_{timestamp}.tsv</code>: Negative enrichment</li> <li><code>{trajectory_id}/ranked_gene_list_{timestamp}.tsv</code>: Ranked genes with scores</li> </ul>"},{"location":"api/enrichment/#combined-results","title":"Combined Results","text":"<ul> <li><code>trajectory_enrichment.csv</code>: All GSEA results combined</li> <li><code>pathway_heatmap_*.png/pdf/svg</code>: Pathway heatmap visualizations</li> </ul>"},{"location":"api/enrichment/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/enrichment/#memory-management","title":"Memory Management","text":"<p>DESeq2 analysis is memory-intensive. The pipeline automatically: - Limits concurrent jobs based on available memory - Allocates memory per job (default: 12 GB) - Monitors memory usage</p> <p>For large datasets: <pre><code>pipeline = EnrichmentPipeline(\n    ...,\n    n_threads=8,  # Reduce parallelism\n    n_threads_per_deseq=4,  # Reduce threads per job\n    memory_per_job_gb=16  # Increase memory per job\n)\n</code></pre></p>"},{"location":"api/enrichment/#cpu-utilization","title":"CPU Utilization","text":"<ul> <li>DESeq2 jobs run in parallel (up to <code>n_threads</code>)</li> <li>Each job uses <code>n_threads_per_deseq</code> threads</li> <li>Total CPU usage \u2248 <code>n_threads</code> \u00d7 <code>n_threads_per_deseq</code></li> </ul> <p>Recommended settings: - Small dataset (&lt;100 trajectories): <code>n_threads=8</code>, <code>n_threads_per_deseq=8</code> - Large dataset (&gt;100 trajectories): <code>n_threads=4</code>, <code>n_threads_per_deseq=12</code></p>"},{"location":"api/enrichment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/enrichment/#out-of-memory-errors","title":"Out of Memory Errors","text":"<pre><code># Reduce parallel jobs\npipeline = EnrichmentPipeline(..., n_threads=4)\n\n# Increase memory per job\npipeline = EnrichmentPipeline(..., memory_per_job_gb=20)\n</code></pre>"},{"location":"api/enrichment/#gsea-not-found","title":"GSEA Not Found","text":"<p>Ensure GSEA is installed and path is correct: <pre><code># Test GSEA\n./GSEA_4.3.2/gsea-cli.sh --help\n</code></pre></p>"},{"location":"api/enrichment/#no-results-generated","title":"No Results Generated","text":"<p>Check logs for errors: <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p>"},{"location":"api/enrichment/#see-also","title":"See Also","text":"<ul> <li>Enrichment Analysis Tutorial</li> <li>GSEA Installation Guide</li> <li>PyDESeq2 Documentation</li> </ul>"},{"location":"api/features/","title":"Features API","text":"<p>The <code>features</code> module provides functions for feature engineering and quality control of gene expression data.</p>"},{"location":"api/features/#overview","title":"Overview","text":"<p>This module includes:</p> <ul> <li>Low expression gene filtering</li> <li>Mahalanobis distance outlier detection</li> <li>Gene clustering with tsfresh</li> <li>Feature extraction for trajectory analysis</li> </ul>"},{"location":"api/features/#core-functions","title":"Core Functions","text":""},{"location":"api/features/#filter_low_expression","title":"filter_low_expression","text":"<p>Filter genes with low or invariant expression across samples.</p> <p>Example Usage:</p> <pre><code>import pandas as pd\nfrom renalprog.features import filter_low_expression\n\n# Load raw expression data (genes \u00d7 samples)\nrnaseq = pd.read_csv(\"data/raw/KIRC_rnaseq.tsv\", sep=\"\\t\", index_col=0)\nprint(f\"Original: {rnaseq.shape[0]} genes\")\n\n# Filter low expression genes\nfiltered = filter_low_expression(\n    rnaseq,\n    mean_threshold=0.5,      # Minimum mean expression\n    var_threshold=0.5,       # Minimum variance\n    min_sample_fraction=0.2  # Maximum fraction of zero values\n)\nprint(f\"Filtered: {filtered.shape[0]} genes\")\n</code></pre> <p>Filtering Criteria:</p> <ol> <li>Zero expression threshold: Remove genes with &gt;20% samples at zero</li> <li>Mean expression threshold: Keep genes with mean \u2265 0.5</li> <li>Variance threshold: Keep genes with variance \u2265 0.5</li> </ol>"},{"location":"api/features/#renalprog.features.filter_low_expression","title":"filter_low_expression","text":"<pre><code>filter_low_expression(\n    data: DataFrame,\n    mean_threshold: float = 0.5,\n    var_threshold: float = 0.5,\n    min_sample_fraction: float = 0.2,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter out genes with low expression across samples.</p> <p>Args:     data: DataFrame with genes as rows and samples as columns     mean_threshold: Minimum expression value to consider gene as expressed     var_threshold: Minimum variance value to consider gene as expressed     min_sample_fraction: Minimum fraction of non-expressed samples to filter gene</p> <p>Returns:     Filtered DataFrame with only genes meeting expression criteria</p> Source code in <code>renalprog/features.py</code> <pre><code>def filter_low_expression(\n    data: pd.DataFrame,\n    mean_threshold: float = 0.5,\n    var_threshold: float = 0.5,\n    min_sample_fraction: float = 0.2\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out genes with low expression across samples.\n\n    Args:\n        data: DataFrame with genes as rows and samples as columns\n        mean_threshold: Minimum expression value to consider gene as expressed\n        var_threshold: Minimum variance value to consider gene as expressed\n        min_sample_fraction: Minimum fraction of non-expressed samples to filter gene\n\n    Returns:\n        Filtered DataFrame with only genes meeting expression criteria\n    \"\"\"\n    logger.info(f\"Starting with {data.shape[0]} genes\")\n    logger.info(f\"Initial data shape: {data.shape}\")\n    # Remove lowly expressed genes\n    filtered_data = data[(data == 0).sum(axis=1) / data.shape[1] &lt;= min_sample_fraction]\n\n    filtered_data= filtered_data.iloc[\n        (np.mean(filtered_data, axis=1).values &gt;= mean_threshold) &amp;\n        (np.var(filtered_data, axis=1).values &gt;= var_threshold)]\n\n    n_removed = data.shape[0] - filtered_data.shape[0]\n    logger.info(f\"Removed {n_removed} lowly expressed genes\")\n    logger.info(f\"Retained {filtered_data.shape[0]} genes\")\n\n    return filtered_data\n</code></pre>"},{"location":"api/features/#detect_outliers_mahalanobis","title":"detect_outliers_mahalanobis","text":"<p>Detect and remove outlier samples using robust Mahalanobis distance.</p> <p>Example Usage:</p> <pre><code>import pandas as pd\nfrom renalprog.features import detect_outliers_mahalanobis\n\n# Load gene expression data (genes \u00d7 samples)\nrnaseq = pd.read_csv(\"data/interim/filtered_expression.csv\", index_col=0)\n\n# Detect outliers\ncleaned_data, outlier_ids, mahal_distances = detect_outliers_mahalanobis(\n    rnaseq,\n    alpha=0.05,           # Significance level\n    support_fraction=None,  # Auto-determine robust subset\n    transpose=True,       # Transpose to samples \u00d7 genes\n    seed=42\n)\n\nprint(f\"Original samples: {rnaseq.shape[1]}\")\nprint(f\"Outliers detected: {len(outlier_ids)}\")\nprint(f\"Clean samples: {cleaned_data.shape[1]}\")\nprint(f\"Outlier IDs: {outlier_ids}\")\n</code></pre> <p>How It Works:</p> <ol> <li>Minimum Covariance Determinant (MCD): Computes robust covariance estimate</li> <li>Mahalanobis Distance: Calculates distance of each sample from the robust center</li> <li>Chi-square Test: Identifies outliers exceeding chi-square threshold at significance level \u03b1</li> </ol> <p>Visualization:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import chi2\n\n# Plot Mahalanobis distances\nn_features = rnaseq.shape[0]\ncutoff = chi2.ppf(1 - 0.05, n_features)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.hist(mahal_distances, bins=50, edgecolor='black')\nplt.axvline(cutoff, color='red', linestyle='--', label=f'Cutoff (\u03b1=0.05)')\nplt.xlabel('Mahalanobis Distance')\nplt.ylabel('Count')\nplt.legend()\nplt.title('Distribution of Mahalanobis Distances')\n\nplt.subplot(1, 2, 2)\nplt.scatter(range(len(mahal_distances)), sorted(mahal_distances))\nplt.axhline(cutoff, color='red', linestyle='--')\nplt.xlabel('Sample Index (sorted)')\nplt.ylabel('Mahalanobis Distance')\nplt.title('Sorted Mahalanobis Distances')\nplt.tight_layout()\nplt.savefig('outlier_detection.png', dpi=300)\n</code></pre>"},{"location":"api/features/#renalprog.features.detect_outliers_mahalanobis","title":"detect_outliers_mahalanobis","text":"<pre><code>detect_outliers_mahalanobis(\n    data: DataFrame,\n    alpha: float = 0.05,\n    support_fraction: float = None,\n    transpose: bool = False,\n    seed: int = 2023,\n) -&gt; Tuple[pd.DataFrame, List[str], np.ndarray]\n</code></pre> <p>Detect and remove outlier samples using Mahalanobis distance.</p> <p>Uses Minimum Covariance Determinant (MCD) for robust covariance estimation, then identifies outliers based on Mahalanobis distance and chi-square distribution.</p> <p>Args:     data: DataFrame with samples as columns and genes as rows (will be transposed)     alpha: Significance level for chi-square test (default: 0.05)     support_fraction: Fraction of samples to use in MCD estimation (default: 0.75)     transpose: Whether to transpose data before processing (default: True)     seed: Random seed for reproducibility (default: 2023)</p> <p>Returns:     Tuple of:     - cleaned_data: DataFrame with outliers removed     - outlier_ids: List of outlier sample IDs     - mahalanobis_distances: Array of Mahalanobis distances for all samples</p> Source code in <code>renalprog/features.py</code> <pre><code>def detect_outliers_mahalanobis(\n    data: pd.DataFrame,\n    alpha: float = 0.05,\n    support_fraction: float = None,\n    transpose: bool = False,\n    seed: int = 2023\n) -&gt; Tuple[pd.DataFrame, List[str], np.ndarray]:\n    \"\"\"\n    Detect and remove outlier samples using Mahalanobis distance.\n\n    Uses Minimum Covariance Determinant (MCD) for robust covariance estimation,\n    then identifies outliers based on Mahalanobis distance and chi-square distribution.\n\n    Args:\n        data: DataFrame with samples as columns and genes as rows (will be transposed)\n        alpha: Significance level for chi-square test (default: 0.05)\n        support_fraction: Fraction of samples to use in MCD estimation (default: 0.75)\n        transpose: Whether to transpose data before processing (default: True)\n        seed: Random seed for reproducibility (default: 2023)\n\n    Returns:\n        Tuple of:\n        - cleaned_data: DataFrame with outliers removed\n        - outlier_ids: List of outlier sample IDs\n        - mahalanobis_distances: Array of Mahalanobis distances for all samples\n    \"\"\"\n    logger.info(f\"Detecting outliers with Mahalanobis distance (alpha={alpha})\")\n\n    # Transpose if needed (we need samples as rows)\n    if transpose:\n        data_for_mcd = data.T\n    else:\n        data_for_mcd = data.copy()\n    logger.info(f\"Data shape for MCD: {data_for_mcd.shape} (features x samples)\")\n    # Define chi-square cutoff\n    n_features = data_for_mcd.shape[0] # this is number of features (genes)\n    cutoff = chi2.ppf(1 - alpha, n_features - 1)\n    logger.info(f\"Chi-square cutoff (df={n_features}, alpha={alpha}): {cutoff:.2f}\")\n\n    # Minimum Covariance Determinant for robust covariance estimation\n    logger.info(\"Fitting MinCovDet estimator...\")\n    mcd = MinCovDet(support_fraction=support_fraction,random_state = seed)\n    mcd.fit(data_for_mcd) # fit with (n_samples, n_features)\n\n    # Calculate Mahalanobis distances\n    mahalanobis_distances = mcd.dist_\n\n    # Identify outliers\n    outlier_mask = mahalanobis_distances &gt; cutoff\n    outlier_indices = np.where(outlier_mask)[0]\n    outlier_ids = data_for_mcd.index[outlier_indices].tolist()\n\n    logger.info(f\"Detected {len(outlier_ids)} outlier samples\")\n    logger.info(f\"Outlier IDs: {outlier_ids[:10]}{'...' if len(outlier_ids) &gt; 10 else ''}\")\n\n    # Remove outliers\n    if transpose:\n        # Remove columns (samples) from original data\n        cleaned_data = data.drop(columns=outlier_ids)\n    else:\n        # Remove rows from original data\n        cleaned_data = data.drop(index=outlier_ids)\n\n    logger.info(f\"Cleaned data shape: {cleaned_data.shape}\")\n\n    return cleaned_data, outlier_ids, mahalanobis_distances\n</code></pre>"},{"location":"api/features/#quality-control-workflow","title":"Quality Control Workflow","text":"<p>Complete QC pipeline for gene expression data:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nfrom renalprog.features import filter_low_expression, detect_outliers_mahalanobis\nfrom renalprog.config import PreprocessingConfig\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef qc_pipeline(rnaseq_path: Path, output_dir: Path):\n    \"\"\"Run complete quality control pipeline.\"\"\"\n\n    # Load configuration\n    config = PreprocessingConfig()\n\n    # Load data\n    logger.info(\"Loading RNA-seq data...\")\n    rnaseq = pd.read_csv(rnaseq_path, sep=\"\\t\", index_col=0)\n    logger.info(f\"Initial shape: {rnaseq.shape}\")\n\n    # Filter low expression genes\n    logger.info(\"Filtering low expression genes...\")\n    rnaseq_filtered = filter_low_expression(\n        rnaseq,\n        mean_threshold=config.mean_threshold,\n        var_threshold=config.var_threshold,\n        min_sample_fraction=config.min_sample_fraction\n    )\n    logger.info(f\"After filtering: {rnaseq_filtered.shape}\")\n\n    # Detect and remove outliers\n    logger.info(\"Detecting outliers...\")\n    rnaseq_clean, outliers, distances = detect_outliers_mahalanobis(\n        rnaseq_filtered,\n        alpha=config.outlier_alpha,\n        seed=config.random_state\n    )\n    logger.info(f\"Outliers removed: {len(outliers)}\")\n    logger.info(f\"Final shape: {rnaseq_clean.shape}\")\n\n    # Save results\n    output_dir.mkdir(parents=True, exist_ok=True)\n    rnaseq_clean.to_csv(output_dir / \"expression_qc.csv\")\n\n    # Save QC report\n    qc_report = {\n        'initial_genes': rnaseq.shape[0],\n        'initial_samples': rnaseq.shape[1],\n        'filtered_genes': rnaseq_filtered.shape[0],\n        'genes_removed': rnaseq.shape[0] - rnaseq_filtered.shape[0],\n        'outlier_samples': len(outliers),\n        'outlier_ids': outliers,\n        'final_genes': rnaseq_clean.shape[0],\n        'final_samples': rnaseq_clean.shape[1]\n    }\n\n    pd.DataFrame([qc_report]).to_csv(output_dir / \"qc_report.csv\", index=False)\n\n    return rnaseq_clean, qc_report\n\n# Run pipeline\nif __name__ == \"__main__\":\n    rnaseq_clean, report = qc_pipeline(\n        rnaseq_path=Path(\"data/raw/KIRC_rnaseq.tsv\"),\n        output_dir=Path(\"data/interim/qc_results\")\n    )\n</code></pre>"},{"location":"api/features/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/features/#custom-filtering-criteria","title":"Custom Filtering Criteria","text":"<p>Define custom filtering thresholds for different datasets:</p> <pre><code>from renalprog.features import filter_low_expression\n\n# Strict filtering for high-quality datasets\nstrict_filtered = filter_low_expression(\n    rnaseq,\n    mean_threshold=1.0,\n    var_threshold=1.0,\n    min_sample_fraction=0.1\n)\n\n# Lenient filtering for smaller datasets\nlenient_filtered = filter_low_expression(\n    rnaseq,\n    mean_threshold=0.1,\n    var_threshold=0.1,\n    min_sample_fraction=0.3\n)\n</code></pre>"},{"location":"api/features/#outlier-detection-with-custom-parameters","title":"Outlier Detection with Custom Parameters","text":"<p>Adjust sensitivity of outlier detection:</p> <pre><code>from renalprog.features import detect_outliers_mahalanobis\n\n# Conservative (fewer outliers)\nconservative, outliers_con, _ = detect_outliers_mahalanobis(\n    rnaseq, alpha=0.01, support_fraction=0.8\n)\n\n# Liberal (more outliers)\nliberal, outliers_lib, _ = detect_outliers_mahalanobis(\n    rnaseq, alpha=0.10, support_fraction=0.6\n)\n\nprint(f\"Conservative: {len(outliers_con)} outliers\")\nprint(f\"Liberal: {len(outliers_lib)} outliers\")\n</code></pre>"},{"location":"api/features/#see-also","title":"See Also","text":"<ul> <li>Dataset API - Data loading and preparation</li> <li>Configuration API - Preprocessing configuration</li> <li>Data Requirements Tutorial - Data preparation guide</li> </ul>"},{"location":"api/models/","title":"Models API","text":"<p>The <code>modeling</code> module provides neural network architectures and training functions for variational autoencoders (VAEs).</p>"},{"location":"api/models/#overview","title":"Overview","text":"<p>This module includes:</p> <ul> <li>VAE architectures (standard, conditional, simple)</li> <li>Training and evaluation functions</li> <li>Loss functions (reconstruction, KL divergence)</li> <li>Checkpoint management</li> <li>Post-processing networks</li> </ul>"},{"location":"api/models/#model-architectures","title":"Model Architectures","text":""},{"location":"api/models/#vae","title":"VAE","text":"<p>Standard Variational Autoencoder with encoder-decoder architecture.</p> <p>Example Usage:</p> <pre><code>import torch\nfrom renalprog.modeling.train import VAE\n\n# Create VAE model\nmodel = VAE(\n    input_dim=20000,  # Number of genes\n    mid_dim=1024,     # Hidden layer size\n    features=128,     # Latent dimension\n    dropout=0.1\n)\n\n# Forward pass\nx = torch.randn(32, 20000)  # Batch of gene expression\nreconstruction, mu, log_var, z = model(x)\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.VAE","title":"VAE","text":"<pre><code>VAE(input_dim: int, mid_dim: int, features: int, output_layer=nn.ReLU)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Variational Autoencoder (VAE).</p> <p>Standard VAE implementation with encoder-decoder architecture and reparameterization trick for sampling from the latent space.</p> <p>Args:     input_dim: Dimension of input data (number of genes)     mid_dim: Dimension of hidden layer     features: Dimension of latent space     output_layer: Output activation function (default: nn.ReLU)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(self, input_dim: int, mid_dim: int, features: int,\n             output_layer=nn.ReLU):\n    super().__init__()\n    self.input_dim = input_dim\n    self.mid_dim = mid_dim\n    self.features = features\n    self.output_layer = output_layer\n\n    # Encoder: input -&gt; mid_dim -&gt; (mu, logvar)\n    self.encoder = nn.Sequential(\n        nn.Linear(in_features=input_dim, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=features * 2)\n    )\n\n    # Decoder: latent -&gt; mid_dim -&gt; reconstruction\n    self.decoder = nn.Sequential(\n        nn.Linear(in_features=features, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=input_dim),\n        output_layer()\n    )\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.VAE-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.VAE.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Forward pass through VAE.</p> <p>Args:     x: Input data (batch_size, input_dim)</p> <p>Returns:     Tuple of (reconstruction, mu, log_var, z)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor,\n                                              torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through VAE.\n\n    Args:\n        x: Input data (batch_size, input_dim)\n\n    Returns:\n        Tuple of (reconstruction, mu, log_var, z)\n    \"\"\"\n    # Encode\n    encoded = self.encoder(x)\n    mu_logvar = encoded.view(-1, 2, self.features)\n    mu = mu_logvar[:, 0, :]\n    log_var = mu_logvar[:, 1, :]\n\n    # Sample from latent space\n    z = self.reparametrize(mu, log_var)\n\n    # Decode\n    reconstruction = self.decoder(z)\n\n    return reconstruction, mu, log_var, z\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.VAE.reparametrize","title":"reparametrize","text":"<pre><code>reparametrize(mu: Tensor, log_var: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Reparameterization trick: sample from N(mu, var) using N(0,1).</p> <p>Args:     mu: Mean of the latent distribution     log_var: Log variance of the latent distribution</p> <p>Returns:     Sampled latent vector</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def reparametrize(self, mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Reparameterization trick: sample from N(mu, var) using N(0,1).\n\n    Args:\n        mu: Mean of the latent distribution\n        log_var: Log variance of the latent distribution\n\n    Returns:\n        Sampled latent vector\n    \"\"\"\n    if self.training:\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    else:\n        # During evaluation, return mean directly\n        return mu\n</code></pre>"},{"location":"api/models/#cvae","title":"CVAE","text":"<p>Conditional VAE that incorporates clinical covariates.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.train import CVAE\n\n# Create conditional VAE\nmodel = ConditionalVAE(\n    input_dim=20000,\n    mid_dim=1024,\n    features=128,\n    condition_dim=2,  # e.g., one-hot encoded stage\n    dropout=0.1\n)\n\n# Forward pass with condition\nx = torch.randn(32, 20000)\ncondition = torch.randn(32, 2)  # Clinical covariates\nreconstruction, mu, log_var, z = model(x, condition)\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.CVAE","title":"CVAE","text":"<pre><code>CVAE(\n    input_dim: int,\n    mid_dim: int,\n    features: int,\n    num_classes: int,\n    output_layer=nn.ReLU,\n)\n</code></pre> <p>               Bases: <code>VAE</code></p> <p>Conditional Variational Autoencoder.</p> <p>VAE that conditions on additional information (e.g., clinical data).</p> <p>Args:     input_dim: Dimension of input data     mid_dim: Dimension of hidden layer     features: Dimension of latent space     num_classes: Number of condition classes     output_layer: Output activation function</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(self, input_dim: int, mid_dim: int, features: int,\n             num_classes: int, output_layer=nn.ReLU):\n    super().__init__(input_dim, mid_dim, features, output_layer)\n    self.num_classes = num_classes\n\n    # Modified encoder: accepts input + condition\n    self.encoder = nn.Sequential(\n        nn.Linear(in_features=input_dim + num_classes, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=features * 2)\n    )\n\n    # Modified decoder: accepts latent + condition\n    self.decoder = nn.Sequential(\n        nn.Linear(in_features=features + num_classes, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=input_dim),\n    )\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.CVAE-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.CVAE.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor, condition: Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Forward pass through CVAE.</p> <p>Args:     x: Input data     condition: Conditioning information (one-hot encoded)</p> <p>Returns:     Tuple of (reconstruction, mu, log_var, z)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(self, x: torch.Tensor, condition: torch.Tensor) -&gt; Tuple[torch.Tensor,\n                                                                      torch.Tensor,\n                                                                      torch.Tensor,\n                                                                      torch.Tensor]:\n    \"\"\"Forward pass through CVAE.\n\n    Args:\n        x: Input data\n        condition: Conditioning information (one-hot encoded)\n\n    Returns:\n        Tuple of (reconstruction, mu, log_var, z)\n    \"\"\"\n    # Concatenate input with condition\n    x_cond = torch.cat([x, condition], dim=1)\n\n    # Encode\n    encoded = self.encoder(x_cond)\n    mu_logvar = encoded.view(-1, 2, self.features)\n    mu = mu_logvar[:, 0, :]\n    log_var = mu_logvar[:, 1, :]\n\n    # Sample\n    z = self.reparametrize(mu, log_var)\n\n    # Concatenate latent with condition\n    z_cond = torch.cat([z, condition], dim=1)\n\n    # Decode\n    reconstruction = self.decoder(z_cond)\n    reconstruction = self.output_layer()(reconstruction)\n\n    return reconstruction, mu, log_var, z\n</code></pre>"},{"location":"api/models/#ae","title":"AE","text":"<p>Simplified autoencoder without variational component.</p>"},{"location":"api/models/#renalprog.modeling.train.AE","title":"AE","text":"<pre><code>AE(input_dim: int, mid_dim: int, features: int, output_layer=nn.ReLU)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Standard Autoencoder (without variational inference).</p> <p>Similar architecture to VAE but without reparameterization trick.</p> <p>Args:     input_dim: Dimension of input data     mid_dim: Dimension of hidden layer     features: Dimension of latent space     output_layer: Output activation function</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(self, input_dim: int, mid_dim: int, features: int,\n             output_layer=nn.ReLU):\n    super().__init__()\n    self.input_dim = input_dim\n    self.mid_dim = mid_dim\n    self.features = features\n    self.output_layer = output_layer\n\n    self.encoder = nn.Sequential(\n        nn.Linear(in_features=input_dim, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=features)\n    )\n\n    self.decoder = nn.Sequential(\n        nn.Linear(in_features=features, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=input_dim),\n        output_layer()\n    )\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.AE-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.AE.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tuple[torch.Tensor, None, None, torch.Tensor]\n</code></pre> <p>Forward pass through AE.</p> <p>Args:     x: Input data</p> <p>Returns:     Tuple of (reconstruction, None, None, z)     None values for mu and logvar to maintain consistency with VAE</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, None, None, torch.Tensor]:\n    \"\"\"Forward pass through AE.\n\n    Args:\n        x: Input data\n\n    Returns:\n        Tuple of (reconstruction, None, None, z)\n        None values for mu and logvar to maintain consistency with VAE\n    \"\"\"\n    z = self.encoder(x)\n    reconstruction = self.decoder(z)\n    return reconstruction, None, None, z\n</code></pre>"},{"location":"api/models/#loss-functions","title":"Loss Functions","text":""},{"location":"api/models/#vae_loss","title":"vae_loss","text":"<p>Complete VAE loss combining reconstruction and KL divergence.</p>"},{"location":"api/models/#renalprog.modeling.train.vae_loss","title":"vae_loss","text":"<pre><code>vae_loss(\n    reconstruction: Tensor,\n    x: Tensor,\n    mu: Tensor,\n    log_var: Tensor,\n    beta: float = 1.0,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Calculate VAE loss: reconstruction loss + KL divergence.</p> <p>Args:     reconstruction: Reconstructed output     x: Original input     mu: Mean of latent distribution     log_var: Log variance of latent distribution     beta: Weight for KL divergence term (beta-VAE)</p> <p>Returns:     Tuple of (total_loss, reconstruction_loss, kl_divergence)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def vae_loss(reconstruction: torch.Tensor, x: torch.Tensor,\n             mu: torch.Tensor, log_var: torch.Tensor,\n             beta: float = 1.0) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Calculate VAE loss: reconstruction loss + KL divergence.\n\n    Args:\n        reconstruction: Reconstructed output\n        x: Original input\n        mu: Mean of latent distribution\n        log_var: Log variance of latent distribution\n        beta: Weight for KL divergence term (beta-VAE)\n\n    Returns:\n        Tuple of (total_loss, reconstruction_loss, kl_divergence)\n    \"\"\"\n    # Reconstruction loss (MSE)\n    recon_loss = nn.functional.mse_loss(reconstruction, x, reduction='sum')\n\n    # KL divergence: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n    # Total loss\n    total_loss = recon_loss + beta * kl_div\n\n    return total_loss, recon_loss, kl_div\n</code></pre>"},{"location":"api/models/#reconstruction_loss","title":"reconstruction_loss","text":"<p>MSE-based reconstruction loss.</p>"},{"location":"api/models/#renalprog.modeling.train.reconstruction_loss","title":"reconstruction_loss","text":"<pre><code>reconstruction_loss(\n    reconstruction: Tensor, x: Tensor, reduction: str = \"sum\"\n) -&gt; torch.Tensor\n</code></pre> <p>Calculate reconstruction loss (MSE).</p> <p>Args:     reconstruction: Reconstructed output     x: Original input     reduction: Reduction method ('sum' or 'mean')</p> <p>Returns:     Reconstruction loss</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def reconstruction_loss(reconstruction: torch.Tensor, x: torch.Tensor,\n                       reduction: str = 'sum') -&gt; torch.Tensor:\n    \"\"\"Calculate reconstruction loss (MSE).\n\n    Args:\n        reconstruction: Reconstructed output\n        x: Original input\n        reduction: Reduction method ('sum' or 'mean')\n\n    Returns:\n        Reconstruction loss\n    \"\"\"\n    return nn.functional.mse_loss(reconstruction, x, reduction=reduction)\n</code></pre>"},{"location":"api/models/#kl_divergence","title":"kl_divergence","text":"<p>KL divergence between latent distribution and prior.</p>"},{"location":"api/models/#renalprog.modeling.train.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(mu: Tensor, log_var: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Calculate KL divergence between approximate posterior and prior.</p> <p>Args:     mu: Mean of approximate posterior     log_var: Log variance of approximate posterior</p> <p>Returns:     KL divergence</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def kl_divergence(mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Calculate KL divergence between approximate posterior and prior.\n\n    Args:\n        mu: Mean of approximate posterior\n        log_var: Log variance of approximate posterior\n\n    Returns:\n        KL divergence\n    \"\"\"\n    return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n</code></pre>"},{"location":"api/models/#training-functions","title":"Training Functions","text":""},{"location":"api/models/#train_vae","title":"train_vae","text":"<p>Main training function for VAE models.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.train import train_vae\nfrom pathlib import Path\nimport pandas as pd\n\n# Load training data\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\n\n# Train VAE\nhistory, best_model, checkpoints = train_vae(\n    train_data=train_expr.values,\n    val_data=test_expr.values,\n    input_dim=train_expr.shape[1],\n    mid_dim=1024,\n    features=128,\n    output_dir=Path(\"models/my_vae\"),\n    n_epochs=100,\n    batch_size=32,\n    learning_rate=1e-3,\n    use_scheduler=True,\n    use_checkpoint=True,\n    early_stopping_patience=20\n)\n\nprint(f\"Final validation loss: {history['val_loss'][-1]:.4f}\")\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.train_vae","title":"train_vae","text":"<pre><code>train_vae(\n    X_train: ndarray,\n    X_test: ndarray,\n    y_train: Optional[ndarray] = None,\n    y_test: Optional[ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]\n</code></pre> <p>Train a VAE model with full checkpointing support.</p> <p>Args:     X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame     X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame     y_train: Optional training labels for CVAE     y_test: Optional test labels for CVAE     config: Training configuration     save_dir: Directory to save checkpoints     resume_from: Optional checkpoint path to resume training     force_cpu: Force CPU usage even if CUDA is available (for compatibility)</p> <p>Returns:     Tuple of (trained_model, training_history)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_vae(\n    X_train: np.ndarray,\n    X_test: np.ndarray,\n    y_train: Optional[np.ndarray] = None,\n    y_test: Optional[np.ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]:\n    \"\"\"Train a VAE model with full checkpointing support.\n\n    Args:\n        X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame\n        X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame\n        y_train: Optional training labels for CVAE\n        y_test: Optional test labels for CVAE\n        config: Training configuration\n        save_dir: Directory to save checkpoints\n        resume_from: Optional checkpoint path to resume training\n        force_cpu: Force CPU usage even if CUDA is available (for compatibility)\n\n    Returns:\n        Tuple of (trained_model, training_history)\n    \"\"\"\n    # Convert DataFrames to numpy arrays if needed\n    if hasattr(X_train, 'values'):  # Check if it's a DataFrame\n        X_train = X_train.values\n    if hasattr(X_test, 'values'):  # Check if it's a DataFrame\n        X_test = X_test.values\n    if y_train is not None and hasattr(y_train, 'values'):\n        y_train = y_train.values\n    if y_test is not None and hasattr(y_test, 'values'):\n        y_test = y_test.values\n\n    if config is None:\n        config = VAEConfig()\n        config.INPUT_DIM = X_train.shape[1]\n\n    set_seed(config.SEED)\n\n    # Setup save directory\n    if save_dir is None:\n        timestamp = datetime.now().strftime('%Y%m%d')\n        save_dir = Path(f\"models/{timestamp}_VAE_KIRC\")\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save config\n    save_model_config(config, save_dir / 'config.json')\n\n    # Setup device\n    device = get_device(force_cpu=force_cpu)\n    logger.info(f\"Using device: {device}\")\n\n    # Initialize model\n    model = VAE(\n        input_dim=config.INPUT_DIM,\n        mid_dim=config.MID_DIM,\n        features=config.LATENT_DIM,\n    ).to(device)\n\n    logger.info(f\"Model: VAE(input_dim={config.INPUT_DIM}, mid_dim={config.MID_DIM}, latent_dim={config.LATENT_DIM})\")\n    logger.info(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    # Setup optimizer\n    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n\n    # Setup checkpointer\n    checkpointer = ModelCheckpointer(\n        save_dir=save_dir,\n        monitor='val_loss',\n        mode='min',\n        save_freq=config.CHECKPOINT_FREQ,\n        keep_last_n=3,\n    )\n\n    # Resume from checkpoint if provided\n    start_epoch = 0\n    if resume_from is not None:\n        checkpoint_info = checkpointer.load_checkpoint(\n            resume_from, model, optimizer, device=str(device)\n        )\n        start_epoch = checkpoint_info['epoch'] + 1\n        logger.info(f\"Resuming training from epoch {start_epoch}\")\n\n    # Create dataloaders\n    train_loader = create_dataloader(X_train, y_train, config.BATCH_SIZE, shuffle=True)\n    test_loader = create_dataloader(X_test, y_test, config.BATCH_SIZE, shuffle=False)\n\n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_recon_loss': [],\n        'train_kl_loss': [],\n        'val_recon_loss': [],\n        'val_kl_loss': [],\n        'beta_schedule': [],  # Track beta values\n    }\n\n    # Setup beta annealing schedule\n    if config.USE_BETA_ANNEALING:\n        beta_schedule = frange_cycle_linear(\n            start=config.BETA_START,\n            stop=config.BETA,\n            n_epoch=config.EPOCHS,\n            n_cycle=config.BETA_CYCLES,\n            ratio=config.BETA_RATIO\n        )\n        logger.info(\n            f\"Using cyclical beta annealing: \"\n            f\"{config.BETA_START} -&gt; {config.BETA} over {config.BETA_CYCLES} cycles\"\n        )\n    else:\n        # Constant beta\n        beta_schedule = np.ones(config.EPOCHS) * config.BETA\n        logger.info(f\"Using constant beta: {config.BETA}\")\n\n    # Training loop\n    logger.info(f\"Starting training for {config.EPOCHS} epochs\")\n\n    # Add epoch progress bar\n    epoch_pbar = tqdm(range(start_epoch, config.EPOCHS), desc='Epochs', position=0)\n    for epoch in epoch_pbar:\n        # Get beta for this epoch from schedule\n        current_beta = beta_schedule[epoch]\n\n        # Train\n        train_metrics = train_epoch(model, train_loader, optimizer, device, config, beta=current_beta)\n\n        # Validate\n        val_metrics = evaluate_model(model, test_loader, device, config, beta=current_beta)\n\n        # Update history\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['train_recon_loss'].append(train_metrics['recon_loss'])\n        history['train_kl_loss'].append(train_metrics['kl_loss'])\n        history['val_recon_loss'].append(val_metrics['recon_loss'])\n        history['val_kl_loss'].append(val_metrics['kl_loss'])\n        history['beta_schedule'].append(float(current_beta))\n\n        # Update epoch progress bar\n        epoch_pbar.set_postfix({\n            'train_loss': f\"{train_metrics['loss']:.4f}\",\n            'val_loss': f\"{val_metrics['loss']:.4f}\",\n            'beta': f\"{current_beta:.3f}\"\n        })\n\n        # Log progress\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            logger.info(\n                f\"Epoch {epoch+1}/{config.EPOCHS} - \"\n                f\"train_loss: {train_metrics['loss']:.4f}, \"\n                f\"val_loss: {val_metrics['loss']:.4f}\"\n            )\n\n        # Combine metrics for checkpointing\n        current_metrics = {\n            'train_loss': train_metrics['loss'],\n            'val_loss': val_metrics['loss'],\n            'train_recon': train_metrics['recon_loss'],\n            'train_kl': train_metrics['kl_loss'],\n            'val_recon': val_metrics['recon_loss'],\n            'val_kl': val_metrics['kl_loss'],\n        }\n\n        # # Save periodic checkpoint\n        # if checkpointer.should_save_checkpoint(epoch):\n        #     checkpointer.save_checkpoint(\n        #         epoch, model, optimizer, current_metrics, config\n        #     )\n\n    # Save final model\n    checkpointer.save_checkpoint(\n        config.EPOCHS - 1, model, optimizer, current_metrics, config, is_final=True\n    )\n\n    logger.info(\"Training complete!\")\n\n    return model, history\n</code></pre>"},{"location":"api/models/#train_epoch","title":"train_epoch","text":"<p>Train the model for one epoch.</p>"},{"location":"api/models/#renalprog.modeling.train.train_epoch","title":"train_epoch","text":"<pre><code>train_epoch(\n    model: Module,\n    dataloader: DataLoader,\n    optimizer: Optimizer,\n    device: str,\n    config: VAEConfig,\n    beta: Optional[float] = None,\n) -&gt; Dict[str, float]\n</code></pre> <p>Train model for one epoch.</p> <p>Args:     model: VAE model     dataloader: Training DataLoader     optimizer: Optimizer     device: Device to use     config: Training configuration     beta: Beta value for this epoch (if None, uses config.BETA)</p> <p>Returns:     Dictionary with loss metrics</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_epoch(model: nn.Module, dataloader: torch.utils.data.DataLoader,\n               optimizer: torch.optim.Optimizer, device: str,\n               config: VAEConfig, beta: Optional[float] = None) -&gt; Dict[str, float]:\n    \"\"\"Train model for one epoch.\n\n    Args:\n        model: VAE model\n        dataloader: Training DataLoader\n        optimizer: Optimizer\n        device: Device to use\n        config: Training configuration\n        beta: Beta value for this epoch (if None, uses config.BETA)\n\n    Returns:\n        Dictionary with loss metrics\n    \"\"\"\n    if beta is None:\n        beta = config.BETA\n    model.train()\n    total_loss = 0.0\n    total_recon = 0.0\n    total_kl = 0.0\n\n    # Add progress bar\n    pbar = tqdm(dataloader, desc='Training', leave=False)\n    for batch in pbar:\n        if len(batch) == 2:\n            data, _ = batch\n        else:\n            data = batch[0]\n\n        data = data.to(device)\n\n        # Forward pass\n        optimizer.zero_grad()\n        reconstruction, mu, log_var, z = model(data)\n\n        # Calculate loss (use beta parameter instead of config.BETA)\n        loss, recon, kl = vae_loss(reconstruction, data, mu, log_var, beta)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate losses\n        total_loss += loss.item()\n        total_recon += recon.item()\n        total_kl += kl.item()\n\n        # Update progress bar\n        pbar.set_postfix({\n            'loss': f'{loss.item() / len(data):.4f}',\n            'recon': f'{recon.item() / len(data):.4f}',\n            'kl': f'{kl.item() / len(data):.4f}'\n        })\n\n    # Average losses\n    n_samples = len(dataloader.dataset)\n    metrics = {\n        'loss': total_loss / n_samples,\n        'recon_loss': total_recon / n_samples,\n        'kl_loss': total_kl / n_samples,\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/models/#evaluate_model","title":"evaluate_model","text":"<p>Evaluate model on validation/test data.</p>"},{"location":"api/models/#renalprog.modeling.train.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: Module,\n    dataloader: DataLoader,\n    device: str,\n    config: VAEConfig,\n    beta: Optional[float] = None,\n) -&gt; Dict[str, float]\n</code></pre> <p>Evaluate model on validation/test set.</p> <p>Args:     model: VAE model     dataloader: Validation DataLoader     device: Device to use     config: Training configuration     beta: Beta value for this epoch (if None, uses config.BETA)</p> <p>Returns:     Dictionary with loss metrics</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def evaluate_model(model: nn.Module, dataloader: torch.utils.data.DataLoader,\n                  device: str, config: VAEConfig, beta: Optional[float] = None) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model on validation/test set.\n\n    Args:\n        model: VAE model\n        dataloader: Validation DataLoader\n        device: Device to use\n        config: Training configuration\n        beta: Beta value for this epoch (if None, uses config.BETA)\n\n    Returns:\n        Dictionary with loss metrics\n    \"\"\"\n    if beta is None:\n        beta = config.BETA\n    model.eval()\n    total_loss = 0.0\n    total_recon = 0.0\n    total_kl = 0.0\n\n    with torch.no_grad():\n        # Add progress bar\n        pbar = tqdm(dataloader, desc='Validation', leave=False)\n        for batch in pbar:\n            if len(batch) == 2:\n                data, _ = batch\n            else:\n                data = batch[0]\n\n            data = data.to(device)\n\n            # Forward pass\n            reconstruction, mu, log_var, z = model(data)\n\n            # Calculate loss (use beta parameter instead of config.BETA)\n            loss, recon, kl = vae_loss(reconstruction, data, mu, log_var, beta)\n\n            # Accumulate losses\n            total_loss += loss.item()\n            total_recon += recon.item()\n            total_kl += kl.item()\n\n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f'{loss.item() / len(data):.4f}',\n                'recon': f'{recon.item() / len(data):.4f}',\n                'kl': f'{kl.item() / len(data):.4f}'\n            })\n\n    # Average losses\n    n_samples = len(dataloader.dataset)\n    metrics = {\n        'loss': total_loss / n_samples,\n        'recon_loss': total_recon / n_samples,\n        'kl_loss': total_kl / n_samples,\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/models/#train_vae_with_postprocessing","title":"train_vae_with_postprocessing","text":"<p>Train VAE and post-processing network together.</p>"},{"location":"api/models/#renalprog.modeling.train.train_vae_with_postprocessing","title":"train_vae_with_postprocessing","text":"<pre><code>train_vae_with_postprocessing(\n    X_train: ndarray,\n    X_test: ndarray,\n    vae_config: Optional[VAEConfig] = None,\n    reconstruction_network_dims: Optional[List[int]] = None,\n    reconstruction_epochs: int = 200,\n    reconstruction_lr: float = 0.0001,\n    batch_size_reconstruction: int = 8,\n    save_dir: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, nn.Module, Dict[str, list], Dict[str, list]]\n</code></pre> <p>Train VAE followed by postprocessing network (full pipeline).</p> <p>This implements the complete training pipeline as in train_vae.sh: 1. Train VAE on gene expression data 2. Get VAE reconstructions 3. Train NetworkReconstruction to adjust VAE output</p> <p>Args:     X_train: Training data (numpy array or pandas DataFrame)     X_test: Test data (numpy array or pandas DataFrame)     vae_config: VAE configuration     reconstruction_network_dims: Architecture for reconstruction network         If None, defaults to [input_dim, 4096, 1024, 4096, input_dim]     reconstruction_epochs: Epochs for training reconstruction network     reconstruction_lr: Learning rate for reconstruction network     save_dir: Directory to save models     force_cpu: Force CPU usage</p> <p>Returns:     Tuple of (vae_model, reconstruction_network, vae_history, reconstruction_history)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_vae_with_postprocessing(\n    X_train: np.ndarray,\n    X_test: np.ndarray,\n    vae_config: Optional[VAEConfig] = None,\n    reconstruction_network_dims: Optional[List[int]] = None,\n    reconstruction_epochs: int = 200,\n    reconstruction_lr: float = 1e-4,\n    batch_size_reconstruction:int = 8,\n    save_dir: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, nn.Module, Dict[str, list], Dict[str, list]]:\n    \"\"\"\n    Train VAE followed by postprocessing network (full pipeline).\n\n    This implements the complete training pipeline as in train_vae.sh:\n    1. Train VAE on gene expression data\n    2. Get VAE reconstructions\n    3. Train NetworkReconstruction to adjust VAE output\n\n    Args:\n        X_train: Training data (numpy array or pandas DataFrame)\n        X_test: Test data (numpy array or pandas DataFrame)\n        vae_config: VAE configuration\n        reconstruction_network_dims: Architecture for reconstruction network\n            If None, defaults to [input_dim, 4096, 1024, 4096, input_dim]\n        reconstruction_epochs: Epochs for training reconstruction network\n        reconstruction_lr: Learning rate for reconstruction network\n        save_dir: Directory to save models\n        force_cpu: Force CPU usage\n\n    Returns:\n        Tuple of (vae_model, reconstruction_network, vae_history, reconstruction_history)\n    \"\"\"\n    logger.info(\"Starting full VAE + postprocessing pipeline\")\n\n    # Convert DataFrames to numpy arrays if needed\n    if hasattr(X_train, 'values'):  # Check if it's a DataFrame\n        X_train = X_train.values\n    if hasattr(X_test, 'values'):  # Check if it's a DataFrame\n        X_test = X_test.values\n\n    # Setup\n    if save_dir is None:\n        timestamp = datetime.now().strftime('%Y%m%d')\n        save_dir = Path(f\"models/{timestamp}_VAE_with_reconstruction\")\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Step 1: Train VAE\n    logger.info(\"Step 1: Training VAE\")\n    vae_model, vae_history = train_vae(\n        X_train, X_test,\n        config=vae_config,\n        save_dir=save_dir / \"vae\",\n        force_cpu=force_cpu\n    )\n\n    # Step 2: Get VAE reconstructions\n    logger.info(\"Step 2: Getting VAE reconstructions\")\n    device = get_device(force_cpu=force_cpu)\n    vae_model.eval()\n\n    # CRITICAL: Normalize data before passing to VAE (same as during training)\n    # The VAE was trained on normalized [0,1] data, so inference must use the same scale\n    logger.info(\"Normalizing data for VAE inference (same as training)\")\n    scaler = MinMaxScaler()\n    X_train_normalized = scaler.fit_transform(X_train)\n    X_test_normalized = scaler.transform(X_test)\n    logger.info(f\"Data normalized: min={X_train_normalized.min():.4f}, max={X_train_normalized.max():.4f}\")\n\n    with torch.no_grad():\n        X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32).to(device)\n        X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n\n        train_recon_normalized, _, _, _ = vae_model(X_train_tensor)\n        test_recon_normalized, _, _, _ = vae_model(X_test_tensor)\n\n        # Denormalize VAE output to match original data scale\n        train_recon = scaler.inverse_transform(train_recon_normalized.cpu().numpy())\n        test_recon = scaler.inverse_transform(test_recon_normalized.cpu().numpy())\n\n    # Convert to DataFrames\n    train_indices = [f\"train_{i}\" for i in range(len(X_train))]\n    test_indices = [f\"test_{i}\" for i in range(len(X_test))]\n\n    all_recon = np.vstack([train_recon, test_recon])\n    all_original = np.vstack([X_train, X_test])\n    all_indices = train_indices + test_indices\n\n    df_reconstruction = pd.DataFrame(all_recon, index=all_indices)\n    df_original = pd.DataFrame(all_original, index=all_indices)\n\n    # Step 3: Train reconstruction network\n    logger.info(\"Step 3: Training reconstruction network\")\n    input_dim = X_train.shape[1]\n\n    if reconstruction_network_dims is None:\n        reconstruction_network_dims = [input_dim, 4096, 1024, 4096, input_dim]\n\n    network = NetworkReconstruction(reconstruction_network_dims)\n\n    network, loss_train, loss_test = train_reconstruction_network(\n        network=network,\n        vae_reconstructions=df_reconstruction,\n        original_data=df_original,\n        train_indices=train_indices,\n        test_indices=test_indices,\n        epochs=reconstruction_epochs,\n        lr=reconstruction_lr,\n        batch_size=batch_size_reconstruction,\n        device=str(device)\n    )\n\n    # Step 4: Save everything\n    logger.info(\"Step 4: Saving models and results\")\n\n    # Save reconstruction network\n    torch.save(network.state_dict(), save_dir / \"reconstruction_network.pth\")\n\n    # Save network dimensions\n    pd.DataFrame([reconstruction_network_dims],\n                 columns=['in_dim', 'layer1_dim', 'layer2_dim', 'layer3_dim', 'out_dim']\n    ).to_csv(save_dir / \"network_dims.csv\", index=False)\n\n    # Save losses\n    pd.DataFrame({'train_loss': loss_train, 'test_loss': loss_test}\n    ).to_csv(save_dir / \"reconstruction_losses.csv\", index=False)\n\n    # Plot losses using Plotly\n    from renalprog.plots import plot_reconstruction_losses\n    plot_reconstruction_losses(\n        loss_train, loss_test,\n        save_path=save_dir / \"reconstruction_losses\"\n    )\n\n    reconstruction_history = {\n        'train_loss': loss_train,\n        'test_loss': loss_test\n    }\n\n    logger.info(f\"Full pipeline complete! Models saved to {save_dir}\")\n\n    return vae_model, network, vae_history, reconstruction_history\n</code></pre>"},{"location":"api/models/#utility-functions","title":"Utility Functions","text":""},{"location":"api/models/#create_dataloader","title":"create_dataloader","text":"<p>Create PyTorch DataLoader from numpy arrays.</p>"},{"location":"api/models/#renalprog.modeling.train.create_dataloader","title":"create_dataloader","text":"<pre><code>create_dataloader(\n    X: ndarray,\n    y: Optional[ndarray] = None,\n    batch_size: int = 32,\n    shuffle: bool = True,\n) -&gt; torch.utils.data.DataLoader\n</code></pre> <p>Create DataLoader with MinMax normalization.</p> <p>Args:     X: Input data (samples x features)     y: Optional labels     batch_size: Batch size     shuffle: Whether to shuffle data</p> <p>Returns:     DataLoader</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def create_dataloader(X: np.ndarray, y: Optional[np.ndarray] = None,\n                     batch_size: int = 32, shuffle: bool = True) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Create DataLoader with MinMax normalization.\n\n    Args:\n        X: Input data (samples x features)\n        y: Optional labels\n        batch_size: Batch size\n        shuffle: Whether to shuffle data\n\n    Returns:\n        DataLoader\n    \"\"\"\n    # Normalize with MinMaxScaler\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Convert to tensors\n    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n\n    if y is not None:\n        y_tensor = torch.tensor(y, dtype=torch.float32)\n        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n    else:\n        dataset = torch.utils.data.TensorDataset(X_tensor)\n\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle\n    )\n\n    return dataloader\n</code></pre>"},{"location":"api/models/#frange_cycle_linear","title":"frange_cycle_linear","text":"<p>Generate cyclical annealing schedule for KL divergence.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.train import frange_cycle_linear\n\n# Create annealing schedule\nschedule = frange_cycle_linear(\n    n_iter=1000,\n    start=0.0,\n    stop=1.0,\n    n_cycle=4,\n    ratio=0.5\n)\n\n# Use in training loop\nfor i, beta in enumerate(schedule):\n    loss = reconstruction_loss + beta * kl_loss\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.frange_cycle_linear","title":"frange_cycle_linear","text":"<pre><code>frange_cycle_linear(\n    start: float,\n    stop: float,\n    n_epoch: int,\n    n_cycle: int = 4,\n    ratio: float = 0.5,\n) -&gt; np.ndarray\n</code></pre> <p>Generate a linear cyclical schedule for beta hyperparameter.</p> <p>This creates a cyclical annealing schedule where beta increases linearly from start to stop over a portion of each cycle (controlled by ratio), then stays constant at stop for the remainder of the cycle.</p> <p>Args:     start: Initial value of beta (typically 0.0)     stop: Final/maximum value of beta (typically 1.0)     n_epoch: Total number of epochs     n_cycle: Number of cycles (default: 4)     ratio: Ratio of cycle spent increasing beta (default: 0.5)            - 0.5 means half cycle increasing, half constant            - 1.0 means entire cycle increasing</p> <p>Returns:     Array of beta values for each epoch</p> <p>Example:     &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle     &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)     &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0     &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0     &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def frange_cycle_linear(\n    start: float,\n    stop: float,\n    n_epoch: int,\n    n_cycle: int = 4,\n    ratio: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a linear cyclical schedule for beta hyperparameter.\n\n    This creates a cyclical annealing schedule where beta increases linearly\n    from start to stop over a portion of each cycle (controlled by ratio),\n    then stays constant at stop for the remainder of the cycle.\n\n    Args:\n        start: Initial value of beta (typically 0.0)\n        stop: Final/maximum value of beta (typically 1.0)\n        n_epoch: Total number of epochs\n        n_cycle: Number of cycles (default: 4)\n        ratio: Ratio of cycle spent increasing beta (default: 0.5)\n               - 0.5 means half cycle increasing, half constant\n               - 1.0 means entire cycle increasing\n\n    Returns:\n        Array of beta values for each epoch\n\n    Example:\n        &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle\n        &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)\n        &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0\n    \"\"\"\n    L = np.ones(n_epoch) * stop  # Initialize all to stop value\n    period = n_epoch / n_cycle\n    step = (stop - start) / (period * ratio)  # Linear schedule\n\n    for c in range(n_cycle):\n        v, i = start, 0\n        while v &lt;= stop and (int(i + c * period) &lt; n_epoch):\n            L[int(i + c * period)] = v\n            v += step\n            i += 1\n\n    return L\n</code></pre>"},{"location":"api/models/#post-processing-network","title":"Post-Processing Network","text":""},{"location":"api/models/#networkreconstruction","title":"NetworkReconstruction","text":"<p>Neural network for refining VAE reconstructions.</p>"},{"location":"api/models/#renalprog.modeling.train.NetworkReconstruction","title":"NetworkReconstruction","text":"<pre><code>NetworkReconstruction(layer_dims: List[int])\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Deep neural network to adjust VAE reconstruction.</p> <p>This network is trained on top of VAE output to improve reconstruction quality by learning a mapping from VAE reconstruction to original data.</p> <p>Args:     layer_dims: List of layer dimensions [input_dim, hidden1, hidden2, ..., output_dim]</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(self, layer_dims: List[int]):\n    super().__init__()\n    layers = []\n    for i in range(len(layer_dims) - 1):\n        layers.append(nn.Linear(layer_dims[i], layer_dims[i + 1]))\n        if i &lt; len(layer_dims) - 2:  # Don't add ReLU after last layer\n            layers.append(nn.ReLU())\n    self.network = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.NetworkReconstruction-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.NetworkReconstruction.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through network.</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through network.\"\"\"\n    return self.network(x)\n</code></pre>"},{"location":"api/models/#train_reconstruction_network","title":"train_reconstruction_network","text":"<p>Train post-processing network.</p>"},{"location":"api/models/#renalprog.modeling.train.train_reconstruction_network","title":"train_reconstruction_network","text":"<pre><code>train_reconstruction_network(\n    network: Module,\n    vae_reconstructions: DataFrame,\n    original_data: DataFrame,\n    train_indices: List,\n    test_indices: List,\n    epochs: int = 200,\n    lr: float = 0.0001,\n    batch_size: int = 32,\n    device: str = \"cpu\",\n) -&gt; Tuple[nn.Module, List[float], List[float]]\n</code></pre> <p>Train reconstruction network to adjust VAE output.</p> <p>Args:     network: NetworkReconstruction model     vae_reconstructions: DataFrame with VAE reconstructions (samples x genes)     original_data: DataFrame with original gene expression (samples x genes)     train_indices: List of training sample indices     test_indices: List of test sample indices     epochs: Number of training epochs     lr: Learning rate     batch_size: Batch size     device: Device to use</p> <p>Returns:     Tuple of (trained_network, train_losses, test_losses)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_reconstruction_network(\n    network: nn.Module,\n    vae_reconstructions: pd.DataFrame,\n    original_data: pd.DataFrame,\n    train_indices: List,\n    test_indices: List,\n    epochs: int = 200,\n    lr: float = 1e-4,\n    batch_size: int = 32,\n    device: str = 'cpu',\n) -&gt; Tuple[nn.Module, List[float], List[float]]:\n    \"\"\"\n    Train reconstruction network to adjust VAE output.\n\n    Args:\n        network: NetworkReconstruction model\n        vae_reconstructions: DataFrame with VAE reconstructions (samples x genes)\n        original_data: DataFrame with original gene expression (samples x genes)\n        train_indices: List of training sample indices\n        test_indices: List of test sample indices\n        epochs: Number of training epochs\n        lr: Learning rate\n        batch_size: Batch size\n        device: Device to use\n\n    Returns:\n        Tuple of (trained_network, train_losses, test_losses)\n    \"\"\"\n    network = network.to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(network.parameters(), lr=lr)\n\n    # Create dataloaders\n    train_dataset = torch.utils.data.TensorDataset(\n        torch.tensor(vae_reconstructions.loc[train_indices].values, dtype=torch.float32),\n        torch.tensor(original_data.loc[train_indices].values, dtype=torch.float32)\n    )\n    test_dataset = torch.utils.data.TensorDataset(\n        torch.tensor(vae_reconstructions.loc[test_indices].values, dtype=torch.float32),\n        torch.tensor(original_data.loc[test_indices].values, dtype=torch.float32)\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False\n    )\n\n    loss_train = []\n    loss_test = []\n\n    logger.info(f\"Training reconstruction network for {epochs} epochs\")\n\n    # Add epoch progress bar\n    epoch_pbar = tqdm(range(epochs), desc='Reconstruction Network Training', position=0)\n    for epoch in epoch_pbar:\n        # Training\n        network.train()\n        running_loss = 0.0\n\n        # Add batch progress bar\n        train_pbar = tqdm(train_loader, desc='Train', leave=False, position=1)\n        for vae_recon, original in train_pbar:\n            vae_recon = vae_recon.to(device)\n            original = original.to(device)\n\n            optimizer.zero_grad()\n            output = network(vae_recon)\n            loss = criterion(output, original)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # Update batch progress bar\n            train_pbar.set_postfix({'batch_loss': f'{loss.item():.6f}'})\n\n        train_loss = running_loss / len(train_loader)\n        loss_train.append(train_loss)\n\n        # Validation\n        network.eval()\n        running_loss = 0.0\n        with torch.no_grad():\n            # Add validation batch progress bar\n            val_pbar = tqdm(test_loader, desc='Val', leave=False, position=1)\n            for vae_recon, original in val_pbar:\n                vae_recon = vae_recon.to(device)\n                original = original.to(device)\n\n                output = network(vae_recon)\n                loss = criterion(output, original)\n                running_loss += loss.item()\n\n                # Update validation progress bar\n                val_pbar.set_postfix({'batch_loss': f'{loss.item():.6f}'})\n\n        test_loss = running_loss / len(test_loader)\n        loss_test.append(test_loss)\n\n        # Update epoch progress bar with current metrics\n        epoch_pbar.set_postfix({\n            'train_loss': f'{train_loss:.6f}',\n            'test_loss': f'{test_loss:.6f}'\n        })\n\n        if (epoch + 1) % 20 == 0:\n            logger.info(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n\n    logger.info(\"Reconstruction network training complete\")\n    return network, loss_train, loss_test\n</code></pre>"},{"location":"api/models/#see-also","title":"See Also","text":"<ul> <li>Training API - Complete training pipeline</li> <li>Prediction API - Using trained models</li> <li>Configuration - Model hyperparameters</li> </ul>"},{"location":"api/plots/","title":"Plots API","text":"<p>Visualization functions for gene expression analysis, model training, and results presentation.</p>"},{"location":"api/plots/#overview","title":"Overview","text":"<p>The plots module provides publication-quality visualization for:</p> <ul> <li>Training history and loss curves</li> <li>Latent space representations</li> <li>Gene expression heatmaps</li> <li>Trajectories and pathways</li> <li>Confusion matrices</li> <li>Enrichment results</li> </ul>"},{"location":"api/plots/#core-plotting-functions","title":"Core Plotting Functions","text":""},{"location":"api/plots/#save_plot","title":"save_plot","text":"<p>Utility function for saving plots with consistent formatting.</p> <p>Example Usage:</p> <pre><code>from renalprog.plots import save_plot\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot([1, 2, 3], [4, 5, 6])\nax.set_title(\"My Plot\")\n\nsave_plot(\n    fig=fig,\n    output_path=Path(\"reports/figures/my_plot.png\"),\n    dpi=300,\n    bbox_inches='tight'\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.save_plot","title":"save_plot","text":"<pre><code>save_plot(\n    fig: Figure,\n    save_path: Union[str, Path],\n    formats: List[str] = [\"html\", \"png\", \"pdf\", \"svg\"],\n    width: int = DEFAULT_WIDTH,\n    height: int = DEFAULT_HEIGHT,\n) -&gt; None\n</code></pre> <p>Save plotly figure in multiple formats.</p> <p>Args:     fig: Plotly figure object     save_path: Base path for saving (without extension)     formats: List of formats to save ['html', 'png', 'pdf', 'svg']     width: Width in pixels for static formats     height: Height in pixels for static formats</p> Source code in <code>renalprog/plots.py</code> <pre><code>def save_plot(\n    fig: go.Figure,\n    save_path: Union[str, Path],\n    formats: List[str] = [\"html\", \"png\", \"pdf\", \"svg\"],\n    width: int = DEFAULT_WIDTH,\n    height: int = DEFAULT_HEIGHT,\n) -&gt; None:\n    \"\"\"\n    Save plotly figure in multiple formats.\n\n    Args:\n        fig: Plotly figure object\n        save_path: Base path for saving (without extension)\n        formats: List of formats to save ['html', 'png', 'pdf', 'svg']\n        width: Width in pixels for static formats\n        height: Height in pixels for static formats\n    \"\"\"\n    save_path = Path(save_path)\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Remove extension if present\n    base_path = save_path.with_suffix('')\n\n    for fmt in formats:\n        output_path = base_path.with_suffix(f'.{fmt}')\n        try:\n            if fmt == 'html':\n                fig.write_html(str(output_path))\n            elif fmt in ['png', 'pdf', 'svg']:\n                fig.write_image(str(output_path), width=width, height=height)\n            logger.info(f\"Saved plot to {output_path}\")\n        except Exception as e:\n            logger.warning(f\"Failed to save {fmt} format: {e}\")\n            if fmt in ['png', 'pdf', 'svg']:\n                logger.warning(\"Note: Static image export requires kaleido package: pip install kaleido\")\n</code></pre>"},{"location":"api/plots/#training-visualization","title":"Training Visualization","text":""},{"location":"api/plots/#plot_training_history","title":"plot_training_history","text":"<p>Visualize VAE training progress.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_training_history\nfrom pathlib import Path\n\n# After training\nhistory, model, checkpoints = train_vae(...)\n\n# Plot training curves\nplot_training_history(\n    history=history,\n    output_path=Path(\"reports/figures/training_history.png\"),\n    title=\"VAE Training Progress\"\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_training_history","title":"plot_training_history","text":"<pre><code>plot_training_history(\n    history: Dict[str, List[float]],\n    save_path: Optional[Path] = None,\n    title: str = \"Training History\",\n    log_scale: bool = False,\n) -&gt; go.Figure\n</code></pre> <p>Plot training and validation losses over epochs.</p> <p>Args:     history: Dictionary with 'train_loss' and 'val_loss' keys     save_path: Optional path to save figure     title: Plot title     log_scale: Whether to use log scale for y-axis</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_training_history(\n    history: Dict[str, List[float]],\n    save_path: Optional[Path] = None,\n    title: str = \"Training History\",\n    log_scale: bool = False,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot training and validation losses over epochs.\n\n    Args:\n        history: Dictionary with 'train_loss' and 'val_loss' keys\n        save_path: Optional path to save figure\n        title: Plot title\n        log_scale: Whether to use log scale for y-axis\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    epochs = list(range(1, len(history['train_loss']) + 1))\n\n    fig = go.Figure()\n\n    ## Total loss = KL + reconstruction\n\n    # Train loss\n    fig.add_trace(go.Scatter(\n        x=epochs,\n        y=history['train_loss'],\n        mode='lines',\n        name='Train Loss',\n        line=dict(color='#1f77b4', width=2),\n        marker=dict(size=4)\n    ))\n\n    # Validation loss\n    if 'val_loss' in history:\n        fig.add_trace(go.Scatter(\n            x=epochs,\n            y=history['val_loss'],\n            mode='lines',\n            name='Val Loss',\n            line=dict(color='#ff7f0e', width=2),\n            marker=dict(size=4)\n        ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='Epoch',\n        yaxis_title='Loss',\n        yaxis_type='log' if log_scale else 'linear',\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode='x unified'\n    )\n\n\n\n    # KL Divergence plot\n    fig_kl = go.Figure()\n    if 'train_kl_loss' in history and 'val_kl_loss' in history:\n        fig_kl.add_trace(go.Scatter(\n            x=epochs,\n            y=history['train_kl_loss'],\n            mode='lines',\n            name='Train KL Divergence',\n            line=dict(color='#1f77b4', width=2),\n            marker=dict(size=4)\n        ))\n        fig_kl.add_trace(go.Scatter(\n            x=epochs,\n            y=history['val_kl_loss'],\n            mode='lines',\n            name='Val KL Divergence',\n            line=dict(color='#ff7f0e', width=2),\n            marker=dict(size=4)\n        ))\n\n    # Reconstruction Loss plot\n    fig_rec = go.Figure()\n    if 'train_recon_loss' in history and 'val_recon_loss' in history:\n        fig_rec.add_trace(go.Scatter(\n            x=epochs,\n            y=history['train_recon_loss'],\n            mode='lines',\n            name='Train Reconstruction Loss',\n            line=dict(color='#1f77b4', width=2),\n            marker=dict(size=4)\n        ))\n        fig_rec.add_trace(go.Scatter(\n            x=epochs,\n            y=history['val_recon_loss'],\n            mode='lines',\n            name='Val Reconstruction Loss',\n            line=dict(color='#ff7f0e', width=2),\n            marker=dict(size=4)\n        ))\n\n    if save_path:\n        save_plot(fig, save_path / \"total_loss\")\n        save_plot(fig_kl, save_path / \"kl_divergence\")\n        save_plot(fig_rec, save_path / \"reconstruction_loss\")\n\n    return fig\n</code></pre>"},{"location":"api/plots/#plot_reconstruction_losses","title":"plot_reconstruction_losses","text":"<p>Compare reconstruction losses across samples.</p>"},{"location":"api/plots/#renalprog.plots.plot_reconstruction_losses","title":"plot_reconstruction_losses","text":"<pre><code>plot_reconstruction_losses(\n    loss_train: List[float],\n    loss_test: List[float],\n    save_path: Optional[Path] = None,\n    title: str = \"Reconstruction Network Losses\",\n) -&gt; go.Figure\n</code></pre> <p>Plot training and test losses for reconstruction network.</p> <p>Args:     loss_train: List of training losses     loss_test: List of test losses     save_path: Optional path to save figure     title: Plot title</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_reconstruction_losses(\n    loss_train: List[float],\n    loss_test: List[float],\n    save_path: Optional[Path] = None,\n    title: str = \"Reconstruction Network Losses\",\n) -&gt; go.Figure:\n    \"\"\"\n    Plot training and test losses for reconstruction network.\n\n    Args:\n        loss_train: List of training losses\n        loss_test: List of test losses\n        save_path: Optional path to save figure\n        title: Plot title\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    epochs = list(range(1, len(loss_train) + 1))\n\n    fig = go.Figure()\n\n    fig.add_trace(go.Scatter(\n        x=epochs,\n        y=loss_train,\n        mode='lines',\n        name='Train',\n        line=dict(color='#1f77b4', width=2)\n    ))\n\n    fig.add_trace(go.Scatter(\n        x=epochs,\n        y=loss_test,\n        mode='lines',\n        name='Test',\n        line=dict(color='#ff7f0e', width=2)\n    ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='Epoch',\n        yaxis_title='Loss',\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode='x unified'\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#plot_loss_landscape","title":"plot_loss_landscape","text":"<p>Visualize loss landscape.</p>"},{"location":"api/plots/#renalprog.plots.plot_loss_landscape","title":"plot_loss_landscape","text":"<pre><code>plot_loss_landscape(\n    loss_values: ndarray,\n    param1_values: ndarray,\n    param2_values: ndarray,\n    save_path: Optional[Path] = None,\n    title: str = \"Loss Landscape\",\n    param1_name: str = \"Parameter 1\",\n    param2_name: str = \"Parameter 2\",\n) -&gt; go.Figure\n</code></pre> <p>Plot 2D loss landscape for hyperparameter tuning.</p> <p>Args:     loss_values: 2D array of loss values     param1_values: Values for first parameter     param2_values: Values for second parameter     save_path: Optional path to save figure     title: Plot title     param1_name: Name of first parameter     param2_name: Name of second parameter</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_loss_landscape(\n    loss_values: np.ndarray,\n    param1_values: np.ndarray,\n    param2_values: np.ndarray,\n    save_path: Optional[Path] = None,\n    title: str = \"Loss Landscape\",\n    param1_name: str = \"Parameter 1\",\n    param2_name: str = \"Parameter 2\",\n) -&gt; go.Figure:\n    \"\"\"\n    Plot 2D loss landscape for hyperparameter tuning.\n\n    Args:\n        loss_values: 2D array of loss values\n        param1_values: Values for first parameter\n        param2_values: Values for second parameter\n        save_path: Optional path to save figure\n        title: Plot title\n        param1_name: Name of first parameter\n        param2_name: Name of second parameter\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    fig = go.Figure(data=go.Contour(\n        z=loss_values,\n        x=param1_values,\n        y=param2_values,\n        colorscale='RdYlBu_r',\n        contours=dict(\n            showlabels=True,\n            labelfont=dict(size=10, color='white')\n        ),\n        colorbar=dict(title='Loss')\n    ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=param1_name,\n        yaxis_title=param2_name,\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#latent-space-visualization","title":"Latent Space Visualization","text":""},{"location":"api/plots/#plot_latent_space","title":"plot_latent_space","text":"<p>2D visualization of latent representations.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_latent_space\nfrom renalprog.modeling.predict import apply_vae\nimport pandas as pd\nfrom pathlib import Path\n\n# Get latent representations\nresults = apply_vae(model, data, device='cuda')\nlatent = results['latent']\n\n# Load labels\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# Plot latent space colored by stage\nplot_latent_space(\n    latent=latent,\n    labels=clinical['stage'],\n    output_path=Path(\"reports/figures/latent_space_by_stage.png\"),\n    method='umap',  # or 'pca', 'tsne'\n    title=\"Latent Space Representation\"\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_latent_space","title":"plot_latent_space","text":"<pre><code>plot_latent_space(\n    latent_coords: ndarray,\n    labels: ndarray,\n    method: str = \"UMAP\",\n    save_path: Optional[Path] = None,\n    title: Optional[str] = None,\n    color_discrete_map: Optional[Dict] = None,\n    **kwargs\n) -&gt; go.Figure\n</code></pre> <p>Plot 2D latent space representation colored by labels.</p> <p>Args:     latent_coords: 2D array of latent coordinates (n_samples x 2)     labels: Array of labels for coloring points     method: Dimensionality reduction method name for title     save_path: Optional path to save figure     title: Optional custom title     color_discrete_map: Optional color mapping for labels     **kwargs: Additional arguments for scatter plot</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_latent_space(\n    latent_coords: np.ndarray,\n    labels: np.ndarray,\n    method: str = \"UMAP\",\n    save_path: Optional[Path] = None,\n    title: Optional[str] = None,\n    color_discrete_map: Optional[Dict] = None,\n    **kwargs\n) -&gt; go.Figure:\n    \"\"\"\n    Plot 2D latent space representation colored by labels.\n\n    Args:\n        latent_coords: 2D array of latent coordinates (n_samples x 2)\n        labels: Array of labels for coloring points\n        method: Dimensionality reduction method name for title\n        save_path: Optional path to save figure\n        title: Optional custom title\n        color_discrete_map: Optional color mapping for labels\n        **kwargs: Additional arguments for scatter plot\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    if latent_coords.shape[1] != 2:\n        raise ValueError(f\"Expected 2D coordinates, got shape {latent_coords.shape}\")\n\n    df = pd.DataFrame({\n        f'{method}_1': latent_coords[:, 0],\n        f'{method}_2': latent_coords[:, 1],\n        'Label': labels\n    })\n\n    fig = px.scatter(\n        df,\n        x=f'{method}_1',\n        y=f'{method}_2',\n        color='Label',\n        color_discrete_map=color_discrete_map,\n        title=title or f'{method} Latent Space Visualization',\n        template=DEFAULT_TEMPLATE,\n        **kwargs\n    )\n\n    fig.update_layout(\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        font=dict(size=12)\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#plot_umap_plotly","title":"plot_umap_plotly","text":"<p>Interactive UMAP visualization.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_umap_plotly\n\n# Create interactive plot\nfig = plot_umap_plotly(\n    latent=latent,\n    labels=clinical['stage'],\n    sample_names=clinical.index.tolist(),\n    title=\"Interactive Latent Space\"\n)\n\n# Save as HTML\nfig.write_html(\"reports/figures/latent_space_interactive.html\")\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_umap_plotly","title":"plot_umap_plotly","text":"<pre><code>plot_umap_plotly(\n    data,\n    clinical,\n    colors_dict,\n    shapes_dict=None,\n    n_components=2,\n    save_fig=False,\n    save_as=None,\n    seed=None,\n    title=\"UMAP\",\n    show=True,\n    marker_size=8,\n)\n</code></pre> <p>Plot UMAP of the data with Plotly using different colors for the different groups.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Features as rows and samples as columns (same as in plot_umap).</p> required <code>clinical</code> <code>Series</code> <p>Category per sample (index must match data.columns).</p> required <code>colors_dict</code> <code>dict</code> <p>Mapping {group_name: color_hex_or_name}.</p> required <code>shapes_dict</code> <p>Mapping {group_name: shape}.</p> <code>None</code> <code>n_components</code> <code>int</code> <p>2 or 3, by default 2.</p> <code>2</code> <code>save_fig</code> <code>bool</code> <p>If True, save HTML/PNG/PDF/SVG, by default False.</p> <code>False</code> <code>save_as</code> <code>str or None</code> <p>Base path (without extension) for saving, by default None.</p> <code>None</code> <code>seed</code> <code>int or None</code> <p>Random state for UMAP, by default None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title, by default 'UMAP'.</p> <code>'UMAP'</code> <code>show</code> <code>bool</code> <p>If True, display the plot, by default True.</p> <code>True</code> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_umap_plotly(\n    data,\n    clinical,\n    colors_dict,\n    shapes_dict=None,\n    n_components=2,\n    save_fig=False,\n    save_as=None,\n    seed=None,\n    title='UMAP',\n    show=True,\n    marker_size=8,\n):\n    \"\"\"\n    Plot UMAP of the data with Plotly using different colors for the different groups.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        Features as rows and samples as columns (same as in plot_umap).\n    clinical : pandas.Series\n        Category per sample (index must match data.columns).\n    colors_dict : dict\n        Mapping {group_name: color_hex_or_name}.\n    shapes_dict: dict\n        Mapping {group_name: shape}.\n    n_components : int, optional\n        2 or 3, by default 2.\n    save_fig : bool, optional\n        If True, save HTML/PNG/PDF/SVG, by default False.\n    save_as : str or None, optional\n        Base path (without extension) for saving, by default None.\n    seed : int or None, optional\n        Random state for UMAP, by default None.\n    title : str, optional\n        Plot title, by default 'UMAP'.\n    show : bool, optional\n        If True, display the plot, by default True.\n    \"\"\"\n\n    # Check number of samples is the first dimension of data:\n    if data.shape[0] != clinical.shape[0]:\n        data = data.T\n        if data.shape[0] != clinical.shape[0]:\n            raise ValueError(\"Data and clinical metadata must have the same number of samples\")\n\n\n    if n_components not in (2, 3):\n        raise ValueError(\"n_components must be 2 or 3 for plot_umap_plotly\")\n\n    today = datetime.now().strftime(\"%Y%m%d\")\n    if save_as is None:\n        suffix = \"UMAP\" if n_components == 2 else \"3D_UMAP\"\n\n    if seed is not None:\n        umap_ = umap.UMAP(n_components=n_components, random_state=seed)\n    else:\n        umap_ = umap.UMAP(n_components=n_components)\n\n    # data: samples x features\n    X_umap = umap_.fit_transform(data)\n    print(\"X_umap.shape\", X_umap.shape)\n\n    # Determine color and shape series from clinical\n    if isinstance(clinical, pd.DataFrame):\n        color_col = clinical.columns[0]\n        color_series = clinical[color_col]\n        # use second column for shapes if provided and shapes_dict is given\n        if shapes_dict is not None and clinical.shape[1] &gt;= 2:\n            shape_col = clinical.columns[1]\n            shape_series = clinical[shape_col]\n        else:\n            shape_series = None\n    elif isinstance(clinical, pd.Series):\n        color_series = clinical\n        shape_series = None\n    else:\n        raise ValueError(\"clinical must be a pandas Series or DataFrame\")\n    print(\"color_series.shape\", color_series.shape)\n\n    # Build plotting DataFrame\n    all_patients = data.index.tolist()\n    print(\"len(all_patients)\", len(all_patients))\n    print(\"color_series.loc[all_patients].values.shape\", color_series.loc[all_patients].values.shape)\n    df_plot = pd.DataFrame(\n        {\n            \"sample\": all_patients,\n            \"group\": color_series.loc[all_patients].values,\n            \"UMAP_1\": X_umap[:, 0],\n            \"UMAP_2\": X_umap[:, 1],\n        }\n    )\n    if n_components == 3:\n        df_plot[\"UMAP_3\"] = X_umap[:, 2]\n\n    # Attach shape column if available\n    if shape_series is not None:\n        df_plot[\"shape\"] = shape_series.loc[all_patients].values\n\n    # Build color sequence in the order of unique groups\n    unique_groups = df_plot[\"group\"].unique()\n    color_sequence = [colors_dict[g] for g in unique_groups]\n\n    # Prepare symbol mapping if shapes are used\n    symbol_map = None\n    if \"shape\" in df_plot.columns and shapes_dict is not None:\n        # convert common Matplotlib markers to Plotly symbols if needed\n        matplot_to_plotly = {\n            'o': 'circle', 's': 'square', '^': 'triangle-up', 'v': 'triangle-down',\n            'D': 'diamond', 'd': 'diamond-wide', 'X': 'x', 'x': 'x', '*': 'star',\n            '+': 'cross', 'p': 'pentagon', 'h': 'hexagon', 'H': 'hexagon2'\n        }\n        unique_shapes = df_plot[\"shape\"].unique()\n        symbol_map = {}\n        for sh in unique_shapes:\n            # get marker definition from shapes_dict; fallback to the value itself\n            marker = shapes_dict.get(sh, shapes_dict.get(str(sh), sh))\n            # translate matplotlib marker codes to plotly symbol names when possible\n            symbol = matplot_to_plotly.get(marker, marker)\n            symbol_map[sh] = symbol\n\n    # Create plotly figure with optional symbols\n    if n_components == 2:\n        fig = px.scatter(\n            df_plot,\n            x=\"UMAP_1\",\n            y=\"UMAP_2\",\n            color=\"group\",\n            color_discrete_sequence=color_sequence,\n            hover_name=\"sample\",\n            template=\"simple_white\",\n            width=800,\n            height=800,\n            symbol=\"shape\" if \"shape\" in df_plot.columns and symbol_map is not None else None,\n            symbol_map=symbol_map if symbol_map is not None else None,\n        )\n        fig.update_layout(\n            title=title,\n            xaxis_title=\"UMAP 1\",\n            yaxis_title=\"UMAP 2\",\n        )\n    else:\n        fig = px.scatter_3d(\n            df_plot,\n            x=\"UMAP_1\",\n            y=\"UMAP_2\",\n            z=\"UMAP_3\",\n            color=\"group\",\n            color_discrete_sequence=color_sequence,\n            hover_name=\"sample\",\n            template=\"simple_white\",\n            width=800,\n            height=800,\n            symbol=\"shape\" if \"shape\" in df_plot.columns and symbol_map is not None else None,\n            symbol_map=symbol_map if symbol_map is not None else None,\n        )\n        fig.update_layout(\n            title=title,\n            scene=dict(\n                xaxis_title=\"UMAP 1\",\n                yaxis_title=\"UMAP 2\",\n                zaxis_title=\"UMAP 3\",\n            ),\n        )\n    fig.update_traces(marker=dict(size=marker_size))\n    # Optional saving\n    if save_fig:\n        base_dir = os.path.dirname(save_as)\n        if base_dir and not os.path.exists(base_dir):\n            os.makedirs(base_dir, exist_ok=True)\n        # Save as HTML\n        fig.write_html(f\"{save_as}.html\")\n        # Save static images\n        for extension in ['png', 'pdf', 'svg']:\n            print(f\"Saved UMAP plotly figure to: {save_as}.{extension}\")\n            fig.write_image(f\"{save_as}.{extension}\", scale=2)\n    if show:\n        fig.show()\n</code></pre>"},{"location":"api/plots/#gene-expression-visualization","title":"Gene Expression Visualization","text":""},{"location":"api/plots/#plot_gene_expression_heatmap","title":"plot_gene_expression_heatmap","text":"<p>Heatmap of gene expression patterns.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_gene_expression_heatmap\nimport pandas as pd\n\n# Load expression data\nexpr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\n\n# Select top variable genes\nvar = expr.var(axis=0).sort_values(ascending=False)\ntop_genes = var.head(50).index\n\n# Plot heatmap\nplot_gene_expression_heatmap(\n    expression=expr[top_genes],\n    row_labels=expr.index.tolist(),\n    col_labels=top_genes.tolist(),\n    output_path=Path(\"reports/figures/expression_heatmap.png\"),\n    cluster_rows=True,\n    cluster_cols=True\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_gene_expression_heatmap","title":"plot_gene_expression_heatmap","text":"<pre><code>plot_gene_expression_heatmap(\n    data: DataFrame,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Heatmap\",\n    colorscale: str = DEFAULT_COLORSCALE,\n    show_labels: bool = True,\n    **kwargs\n) -&gt; go.Figure\n</code></pre> <p>Create a heatmap of gene expression data.</p> <p>Args:     data: DataFrame with genes as columns and samples as rows     save_path: Optional path to save figure     title: Plot title     colorscale: Plotly colorscale name     show_labels: Whether to show axis labels     **kwargs: Additional arguments for heatmap</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_gene_expression_heatmap(\n    data: pd.DataFrame,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Heatmap\",\n    colorscale: str = DEFAULT_COLORSCALE,\n    show_labels: bool = True,\n    **kwargs\n) -&gt; go.Figure:\n    \"\"\"\n    Create a heatmap of gene expression data.\n\n    Args:\n        data: DataFrame with genes as columns and samples as rows\n        save_path: Optional path to save figure\n        title: Plot title\n        colorscale: Plotly colorscale name\n        show_labels: Whether to show axis labels\n        **kwargs: Additional arguments for heatmap\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    fig = go.Figure(data=go.Heatmap(\n        z=data.values,\n        x=data.columns if show_labels else None,\n        y=data.index if show_labels else None,\n        colorscale=colorscale,\n        **kwargs\n    ))\n\n    fig.update_layout(\n        title=title,\n        template=DEFAULT_TEMPLATE,\n        width=max(DEFAULT_WIDTH, len(data.columns) * 10),\n        height=max(DEFAULT_HEIGHT, len(data) * 10),\n        xaxis_title='Genes' if show_labels else None,\n        yaxis_title='Samples' if show_labels else None\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#trajectory-visualization","title":"Trajectory Visualization","text":""},{"location":"api/plots/#plot_trajectory","title":"plot_trajectory","text":"<p>Visualize disease progression trajectory.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_trajectory\n\n# Plot single trajectory\nplot_trajectory(\n    trajectory=trajectories[0],  # Shape: (n_steps, n_genes)\n    feature_names=selected_genes,\n    output_path=Path(\"reports/figures/trajectory_001.png\"),\n    title=\"Disease Progression Trajectory\",\n    highlight_genes=['TP53', 'VEGFA', 'HIF1A']\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_trajectory","title":"plot_trajectory","text":"<pre><code>plot_trajectory(\n    trajectory: ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure\n</code></pre> <p>Plot gene expression changes along a trajectory.</p> <p>Args:     trajectory: Array of shape (n_timepoints, n_genes)     gene_names: Optional list of gene names     save_path: Optional path to save figure     title: Plot title     n_genes_to_show: Number of top varying genes to display</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_trajectory(\n    trajectory: np.ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot gene expression changes along a trajectory.\n\n    Args:\n        trajectory: Array of shape (n_timepoints, n_genes)\n        gene_names: Optional list of gene names\n        save_path: Optional path to save figure\n        title: Plot title\n        n_genes_to_show: Number of top varying genes to display\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    n_timepoints, n_genes = trajectory.shape\n\n    # Calculate variance for each gene\n    gene_variance = np.var(trajectory, axis=0)\n    top_genes_idx = np.argsort(gene_variance)[-n_genes_to_show:]\n\n    if gene_names is None:\n        gene_names = [f'Gene_{i}' for i in range(n_genes)]\n\n    fig = go.Figure()\n\n    timepoints = list(range(n_timepoints))\n\n    for idx in top_genes_idx:\n        fig.add_trace(go.Scatter(\n            x=timepoints,\n            y=trajectory[:, idx],\n            mode='lines',\n            name=gene_names[idx],\n            line=dict(width=1.5)\n        ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='Timepoint',\n        yaxis_title='Expression Level',\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode='x unified'\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#enrichment-visualization","title":"Enrichment Visualization","text":""},{"location":"api/plots/#plot_enrichment_results","title":"plot_enrichment_results","text":"<p>Visualize pathway enrichment results.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_enrichment_results\nimport pandas as pd\n\n# Load enrichment results\nenrichment = pd.read_csv(\"reports/enrichment/combined_results.csv\")\n\n# Plot top pathways\nplot_enrichment_results(\n    enrichment_df=enrichment,\n    output_path=Path(\"reports/figures/enrichment_barplot.png\"),\n    top_n=20,\n    metric='fdr',\n    title=\"Top Enriched Pathways\"\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_enrichment_results","title":"plot_enrichment_results","text":"<pre><code>plot_enrichment_results(\n    enrichment_df: DataFrame,\n    save_path: Optional[Path] = None,\n    title: str = \"Pathway Enrichment Analysis\",\n    top_n: int = 20,\n    p_value_col: str = \"p_value\",\n    pathway_col: str = \"pathway\",\n) -&gt; go.Figure\n</code></pre> <p>Plot enrichment analysis results.</p> <p>Args:     enrichment_df: DataFrame with enrichment results     save_path: Optional path to save figure     title: Plot title     top_n: Number of top pathways to show     p_value_col: Column name for p-values     pathway_col: Column name for pathway names</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_enrichment_results(\n    enrichment_df: pd.DataFrame,\n    save_path: Optional[Path] = None,\n    title: str = \"Pathway Enrichment Analysis\",\n    top_n: int = 20,\n    p_value_col: str = 'p_value',\n    pathway_col: str = 'pathway',\n) -&gt; go.Figure:\n    \"\"\"\n    Plot enrichment analysis results.\n\n    Args:\n        enrichment_df: DataFrame with enrichment results\n        save_path: Optional path to save figure\n        title: Plot title\n        top_n: Number of top pathways to show\n        p_value_col: Column name for p-values\n        pathway_col: Column name for pathway names\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    # Sort by p-value and take top N\n    df_sorted = enrichment_df.sort_values(p_value_col).head(top_n)\n\n    # Calculate -log10(p-value)\n    df_sorted['neg_log_p'] = -np.log10(df_sorted[p_value_col])\n\n    fig = go.Figure(go.Bar(\n        x=df_sorted['neg_log_p'],\n        y=df_sorted[pathway_col],\n        orientation='h',\n        marker=dict(\n            color=df_sorted['neg_log_p'],\n            colorscale='Reds',\n            showscale=True,\n            colorbar=dict(title='-log10(p-value)')\n        )\n    ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='-log10(p-value)',\n        yaxis_title='Pathway',\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=max(DEFAULT_HEIGHT, top_n * 25),\n        yaxis={'categoryorder': 'total ascending'}\n    )\n\n    if save_path:\n        save_plot(fig, save_path, height=max(DEFAULT_HEIGHT, top_n * 25))\n\n    return fig\n</code></pre>"},{"location":"api/plots/#classification-visualization","title":"Classification Visualization","text":""},{"location":"api/plots/#plot_confusion_matrix","title":"plot_confusion_matrix","text":"<p>Visualize classification performance.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# After classification\ny_true = test_labels\ny_pred = classifier.predict(test_features)\ncm = confusion_matrix(y_true, y_pred)\n\nplot_confusion_matrix(\n    confusion_matrix=cm,\n    class_names=['Non-progressing', 'Progressing'],\n    output_path=Path(\"reports/figures/confusion_matrix.png\"),\n    normalize=True\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_confusion_matrix","title":"plot_confusion_matrix","text":"<pre><code>plot_confusion_matrix(\n    confusion_matrix: ndarray,\n    class_names: List[str],\n    save_path: Optional[Path] = None,\n    title: str = \"Confusion Matrix\",\n    normalize: bool = False,\n) -&gt; go.Figure\n</code></pre> <p>Plot confusion matrix as heatmap.</p> <p>Args:     confusion_matrix: Square confusion matrix     class_names: List of class names     save_path: Optional path to save figure     title: Plot title     normalize: Whether to normalize by row</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_confusion_matrix(\n    confusion_matrix: np.ndarray,\n    class_names: List[str],\n    save_path: Optional[Path] = None,\n    title: str = \"Confusion Matrix\",\n    normalize: bool = False,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot confusion matrix as heatmap.\n\n    Args:\n        confusion_matrix: Square confusion matrix\n        class_names: List of class names\n        save_path: Optional path to save figure\n        title: Plot title\n        normalize: Whether to normalize by row\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    if normalize:\n        cm = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n        text = [[f'{confusion_matrix[i, j]}&lt;br&gt;({cm[i, j]:.2%})'\n                for j in range(len(class_names))]\n               for i in range(len(class_names))]\n    else:\n        cm = confusion_matrix\n        text = [[str(confusion_matrix[i, j])\n                for j in range(len(class_names))]\n               for i in range(len(class_names))]\n\n    fig = go.Figure(data=go.Heatmap(\n        z=cm,\n        x=class_names,\n        y=class_names,\n        text=text,\n        texttemplate='%{text}',\n        colorscale='Blues',\n        showscale=True\n    ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='Predicted',\n        yaxis_title='Actual',\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        yaxis={'autorange': 'reversed'}\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#pca-visualization","title":"PCA Visualization","text":""},{"location":"api/plots/#plot_pca_variance","title":"plot_pca_variance","text":"<p>Visualize PCA variance explained.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_pca_variance\nfrom sklearn.decomposition import PCA\n\n# Perform PCA\npca = PCA(n_components=50)\npca.fit(expression_data)\n\n# Plot variance explained\nplot_pca_variance(\n    pca=pca,\n    output_path=Path(\"reports/figures/pca_variance.png\"),\n    n_components=20\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_pca_variance","title":"plot_pca_variance","text":"<pre><code>plot_pca_variance(\n    explained_variance_ratio: ndarray,\n    save_path: Optional[Path] = None,\n    title: str = \"PCA Explained Variance\",\n    n_components: int = 20,\n) -&gt; go.Figure\n</code></pre> <p>Plot explained variance ratio from PCA.</p> <p>Args:     explained_variance_ratio: Array of explained variance ratios     save_path: Optional path to save figure     title: Plot title     n_components: Number of components to show</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_pca_variance(\n    explained_variance_ratio: np.ndarray,\n    save_path: Optional[Path] = None,\n    title: str = \"PCA Explained Variance\",\n    n_components: int = 20,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot explained variance ratio from PCA.\n\n    Args:\n        explained_variance_ratio: Array of explained variance ratios\n        save_path: Optional path to save figure\n        title: Plot title\n        n_components: Number of components to show\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    n_show = min(n_components, len(explained_variance_ratio))\n    components = list(range(1, n_show + 1))\n\n    # Individual variance\n    fig = make_subplots(\n        rows=1, cols=2,\n        subplot_titles=('Individual Explained Variance', 'Cumulative Explained Variance')\n    )\n\n    fig.add_trace(\n        go.Bar(\n            x=components,\n            y=explained_variance_ratio[:n_show],\n            name='Individual',\n            marker_color='#1f77b4'\n        ),\n        row=1, col=1\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=components,\n            y=np.cumsum(explained_variance_ratio[:n_show]),\n            mode='lines+markers',\n            name='Cumulative',\n            line=dict(color='#ff7f0e', width=2),\n            marker=dict(size=6)\n        ),\n        row=1, col=2\n    )\n\n    fig.update_xaxes(title_text='Principal Component', row=1, col=1)\n    fig.update_xaxes(title_text='Principal Component', row=1, col=2)\n    fig.update_yaxes(title_text='Explained Variance Ratio', row=1, col=1)\n    fig.update_yaxes(title_text='Cumulative Variance', row=1, col=2)\n\n    fig.update_layout(\n        title_text=title,\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH * 2,\n        height=DEFAULT_HEIGHT,\n        showlegend=False\n    )\n\n    if save_path:\n        save_plot(fig, save_path, width=DEFAULT_WIDTH * 2)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#complete-visualization-workflow","title":"Complete Visualization Workflow","text":"<pre><code>import torch\nimport pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE, train_vae\nfrom renalprog.modeling.predict import apply_vae, generate_trajectories\nfrom renalprog.plots import (\n    plot_training_history,\n    plot_latent_space,\n    plot_gene_expression_heatmap,\n    plot_trajectory,\n    plot_umap_plotly\n)\n\n# Create output directory\noutput_dir = Path(\"reports/figures\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# 1. Load data\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# 2. Train model and plot history\nhistory, model, checkpoints = train_vae(\n    train_data=train_expr.values,\n    val_data=test_expr.values,\n    input_dim=train_expr.shape[1],\n    mid_dim=1024,\n    features=128,\n    output_dir=Path(\"models/my_vae\"),\n    n_epochs=100\n)\n\nplot_training_history(\n    history=history,\n    output_path=output_dir / \"training_history.png\"\n)\n\n# 3. Encode to latent space and visualize\nresults = apply_vae(model, test_expr.values, device='cuda')\n\nplot_latent_space(\n    latent=results['latent'],\n    labels=clinical['stage'],\n    output_path=output_dir / \"latent_space_umap.png\",\n    method='umap'\n)\n\nplot_umap_plotly(\n    latent=results['latent'],\n    labels=clinical['stage'],\n    sample_names=clinical.index.tolist(),\n    title=\"Interactive Latent Space\"\n).write_html(output_dir / \"latent_space_interactive.html\")\n\n# 4. Plot expression heatmap\nvar = test_expr.var(axis=0).sort_values(ascending=False)\ntop_genes = var.head(50).index\n\nplot_gene_expression_heatmap(\n    expression=test_expr[top_genes],\n    row_labels=test_expr.index.tolist(),\n    col_labels=top_genes.tolist(),\n    output_path=output_dir / \"expression_heatmap.png\",\n    cluster_rows=True,\n    cluster_cols=True\n)\n\n# 5. Generate and plot trajectories\nearly_mask = clinical['stage'] == 'early'\nlate_mask = clinical['stage'] == 'late'\n\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=test_expr.values[early_mask],\n    end_data=test_expr.values[late_mask],\n    n_steps=50,\n    device='cuda'\n)\n\n# Plot first trajectory\nplot_trajectory(\n    trajectory=trajectories[0],\n    feature_names=top_genes.tolist(),\n    output_path=output_dir / \"trajectory_001.png\",\n    title=\"Disease Progression Trajectory\"\n)\n\nprint(f\"All figures saved to {output_dir}\")\n</code></pre>"},{"location":"api/plots/#customization","title":"Customization","text":"<p>All plotting functions accept matplotlib/plotly parameters for customization:</p> <pre><code>from renalprog.plots import plot_latent_space\n\nplot_latent_space(\n    latent=latent,\n    labels=labels,\n    output_path=output_path,\n    figsize=(12, 8),         # Custom figure size\n    cmap='viridis',          # Custom colormap\n    alpha=0.6,               # Transparency\n    s=50,                    # Point size\n    title=\"Custom Title\",\n    xlabel=\"Component 1\",\n    ylabel=\"Component 2\"\n)\n</code></pre>"},{"location":"api/plots/#publication-quality-figures","title":"Publication-Quality Figures","text":"<p>For publication:</p> <pre><code>import matplotlib.pyplot as plt\nplt.rcParams.update({\n    'font.size': 12,\n    'font.family': 'sans-serif',\n    'figure.dpi': 300,\n    'savefig.dpi': 300,\n    'savefig.bbox': 'tight',\n    'axes.labelsize': 14,\n    'axes.titlesize': 16,\n    'xtick.labelsize': 12,\n    'ytick.labelsize': 12,\n    'legend.fontsize': 12\n})\n\n# Then use plotting functions\nplot_latent_space(...)\n</code></pre>"},{"location":"api/plots/#see-also","title":"See Also","text":"<ul> <li>Training API - Generate training history</li> <li>Prediction API - Generate predictions to plot</li> <li>Trajectories API - Generate trajectories</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/prediction/","title":"Prediction API","text":"<p>Functions for applying trained VAE models to generate latent representations and trajectories.</p>"},{"location":"api/prediction/#overview","title":"Overview","text":"<p>This module provides:</p> <ul> <li>Apply trained VAE to encode data</li> <li>Generate disease progression trajectories</li> <li>Evaluate reconstruction quality</li> <li>Patient connectivity analysis</li> <li>Latent space interpolation</li> </ul>"},{"location":"api/prediction/#core-prediction-functions","title":"Core Prediction Functions","text":""},{"location":"api/prediction/#apply_vae","title":"apply_vae","text":"<p>Apply trained VAE model to encode gene expression data into latent space.</p> <p>Example Usage:</p> <pre><code>import torch\nimport pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE\nfrom renalprog.modeling.predict import apply_vae\n\n# Load model\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel.load_state_dict(torch.load(\"models/my_vae/best_model.pt\"))\n\n# Load test data\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\n\n# Apply VAE\nresults = apply_vae(\n    model=model,\n    data=test_expr.values,\n    device='cuda',\n    batch_size=32\n)\n\nlatent = results['latent']  # Latent representations\nreconstructed = results['reconstructed']  # Reconstructed expression\nprint(f\"Latent space shape: {latent.shape}\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.apply_vae","title":"apply_vae","text":"<pre><code>apply_vae(\n    model: Module, data: DataFrame, device: str = \"cpu\"\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <p>Apply VAE to data to get reconstruction and latent representation.</p> <p>Args:     model: Trained VAE model     data: Input data (samples x genes)     device: Device to run inference on</p> <p>Returns:     Tuple of (reconstruction, mu, logvar, z)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def apply_vae(\n    model: torch.nn.Module,\n    data: pd.DataFrame,\n    device: str = \"cpu\"\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply VAE to data to get reconstruction and latent representation.\n\n    Args:\n        model: Trained VAE model\n        data: Input data (samples x genes)\n        device: Device to run inference on\n\n    Returns:\n        Tuple of (reconstruction, mu, logvar, z)\n    \"\"\"\n    model.eval()\n    model = model.to(device)\n\n    # Convert to tensor\n    if isinstance(data, pd.DataFrame):\n        data_tensor = torch.tensor(data.values, dtype=torch.float32).to(device)\n    else:\n        data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n\n    with torch.no_grad():\n        reconstruction, mu, logvar, z = model(data_tensor)\n\n    # Convert back to numpy\n    reconstruction = reconstruction.cpu().numpy()\n    mu = mu.cpu().numpy()\n    logvar = logvar.cpu().numpy() if logvar is not None else None\n    z = z.cpu().numpy()\n\n    return reconstruction, mu, logvar, z\n</code></pre>"},{"location":"api/prediction/#trajectory-generation","title":"Trajectory Generation","text":""},{"location":"api/prediction/#generate_trajectories","title":"generate_trajectories","text":"<p>Generate disease progression trajectories by interpolating in latent space.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import generate_trajectories\n\n# Generate trajectories from early to late stage\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=early_stage_samples.values,\n    end_data=late_stage_samples.values,\n    n_steps=50,\n    interpolation='spherical',\n    device='cuda'\n)\n\n# trajectories shape: (n_samples, n_steps, n_genes)\nprint(f\"Generated {trajectories.shape[0]} trajectories\")\nprint(f\"Each with {trajectories.shape[1]} steps\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.generate_trajectories","title":"generate_trajectories","text":"<pre><code>generate_trajectories(\n    model: Module,\n    source_samples: DataFrame,\n    target_samples: DataFrame,\n    n_steps: int = 50,\n    method: str = \"linear\",\n    output_dir: Optional[Path] = None,\n    parallel: bool = False,\n    n_workers: Optional[int] = None,\n) -&gt; Dict[str, pd.DataFrame]\n</code></pre> <p>Generate synthetic trajectories between source and target patient samples.</p> <p>This function creates interpolated gene expression profiles in the latent space between pairs of patients at different cancer stages.</p> <p>Args:     model: Trained VAE model     source_samples: Source patient samples (early stage)     target_samples: Target patient samples (late stage)     n_steps: Number of interpolation steps     method: Interpolation method (\"linear\" or \"spherical\")     output_dir: Optional directory to save trajectories     parallel: Whether to use parallel processing     n_workers: Number of parallel workers (None = use all CPUs)</p> <p>Returns:     Dictionary mapping patient pairs to trajectory DataFrames</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def generate_trajectories(\n    model: torch.nn.Module,\n    source_samples: pd.DataFrame,\n    target_samples: pd.DataFrame,\n    n_steps: int = 50,\n    method: str = \"linear\",\n    output_dir: Optional[Path] = None,\n    parallel: bool = False,\n    n_workers: Optional[int] = None\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Generate synthetic trajectories between source and target patient samples.\n\n    This function creates interpolated gene expression profiles in the latent\n    space between pairs of patients at different cancer stages.\n\n    Args:\n        model: Trained VAE model\n        source_samples: Source patient samples (early stage)\n        target_samples: Target patient samples (late stage)\n        n_steps: Number of interpolation steps\n        method: Interpolation method (\"linear\" or \"spherical\")\n        output_dir: Optional directory to save trajectories\n        parallel: Whether to use parallel processing\n        n_workers: Number of parallel workers (None = use all CPUs)\n\n    Returns:\n        Dictionary mapping patient pairs to trajectory DataFrames\n    \"\"\"\n    logger.info(f\"Generating trajectories with {n_steps} steps using {method} interpolation\")\n\n    # TODO: Implement trajectory generation\n    # Migrate from src_deseq_and_gsea_NCSR/synthetic_data_generation.py\n\n    raise NotImplementedError(\n        \"generate_trajectories() needs implementation from \"\n        \"src_deseq_and_gsea_NCSR/synthetic_data_generation.py and \"\n        \"src/data/fun_interpol.py\"\n    )\n</code></pre>"},{"location":"api/prediction/#create_patient_connections","title":"create_patient_connections","text":"<p>Create optimal patient pairings for trajectory generation.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import create_patient_connections\n\n# Find optimal patient connections\nconnections = create_patient_connections(\n    latent_early=early_latent,\n    latent_late=late_latent,\n    method='closest',  # or 'random'\n    output_path=Path(\"data/processed/patient_connections.csv\")\n)\n\nprint(f\"Created {len(connections)} patient pairs\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.create_patient_connections","title":"create_patient_connections","text":"<pre><code>create_patient_connections(\n    data: DataFrame,\n    clinical: Series,\n    method: str = \"random\",\n    transition_type: str = \"early_to_late\",\n    n_connections: Optional[int] = None,\n    seed: int = 2023,\n) -&gt; pd.DataFrame\n</code></pre> <p>Create connections between patients for trajectory generation.</p> <p>Args:     data: Gene expression data     clinical: Clinical stage information     method: Method for creating connections (\"random\", \"nearest\", \"all\")     transition_type: Type of transition (\"early_to_late\", \"early_to_early\", etc.)     n_connections: Number of connections to create (None = all possible)     seed: Random seed</p> <p>Returns:     DataFrame with columns: source, target, transition</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def create_patient_connections(\n    data: pd.DataFrame,\n    clinical: pd.Series,\n    method: str = \"random\",\n    transition_type: str = \"early_to_late\",\n    n_connections: Optional[int] = None,\n    seed: int = 2023\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create connections between patients for trajectory generation.\n\n    Args:\n        data: Gene expression data\n        clinical: Clinical stage information\n        method: Method for creating connections (\"random\", \"nearest\", \"all\")\n        transition_type: Type of transition (\"early_to_late\", \"early_to_early\", etc.)\n        n_connections: Number of connections to create (None = all possible)\n        seed: Random seed\n\n    Returns:\n        DataFrame with columns: source, target, transition\n    \"\"\"\n    logger.info(f\"Creating patient connections: {transition_type} using {method} method\")\n\n    # TODO: Implement connection logic\n    # Migrate from notebooks/4_1_trajectories.ipynb\n\n    raise NotImplementedError(\n        \"create_patient_connections() needs implementation from \"\n        \"notebooks/4_1_trajectories.ipynb\"\n    )\n</code></pre>"},{"location":"api/prediction/#interpolate_latent_linear","title":"interpolate_latent_linear","text":"<p>Linear interpolation between latent representations.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.interpolate_latent_linear","title":"interpolate_latent_linear","text":"<pre><code>interpolate_latent_linear(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Linear interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_linear(\n    z_source: np.ndarray,\n    z_target: np.ndarray,\n    n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Linear interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array([\n        (1 - alpha) * z_source + alpha * z_target\n        for alpha in alphas\n    ])\n    return interpolated\n</code></pre>"},{"location":"api/prediction/#interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<p>Spherical (SLERP) interpolation between latent representations.</p> <p>Example:</p> <pre><code>from renalprog.modeling.predict import interpolate_latent_spherical\nimport numpy as np\n\nz_start = np.random.randn(10, 128)  # 10 samples, 128 latent dims\nz_end = np.random.randn(10, 128)\n\n# Spherical interpolation (better for normalized spaces)\ntrajectory = interpolate_latent_spherical(z_start, z_end, n_steps=50)\n# Shape: (10, 50, 128)\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<pre><code>interpolate_latent_spherical(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Spherical (SLERP) interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_spherical(\n    z_source: np.ndarray,\n    z_target: np.ndarray,\n    n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Spherical (SLERP) interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    # Normalize vectors\n    z_source_norm = z_source / np.linalg.norm(z_source)\n    z_target_norm = z_target / np.linalg.norm(z_target)\n\n    # Calculate angle between vectors\n    omega = np.arccos(np.clip(np.dot(z_source_norm, z_target_norm), -1.0, 1.0))\n\n    if omega &lt; 1e-8:\n        # Vectors are nearly identical, use linear interpolation\n        return interpolate_latent_linear(z_source, z_target, n_steps)\n\n    # SLERP formula\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array([\n        (np.sin((1 - alpha) * omega) / np.sin(omega)) * z_source +\n        (np.sin(alpha * omega) / np.sin(omega)) * z_target\n        for alpha in alphas\n    ])\n\n    return interpolated\n</code></pre>"},{"location":"api/prediction/#reconstruction-evaluation","title":"Reconstruction Evaluation","text":""},{"location":"api/prediction/#evaluate_reconstruction","title":"evaluate_reconstruction","text":"<p>Comprehensive evaluation of VAE reconstruction quality.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import evaluate_reconstruction\n\n# Evaluate reconstruction quality\nmetrics = evaluate_reconstruction(\n    model=model,\n    original_data=test_expr.values,\n    device='cuda',\n    output_dir=Path(\"reports/reconstruction_eval\")\n)\n\nprint(f\"MSE: {metrics['mse']:.4f}\")\nprint(f\"Pearson R: {metrics['pearson_mean']:.4f}\")\nprint(f\"Cosine similarity: {metrics['cosine_mean']:.4f}\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.evaluate_reconstruction","title":"evaluate_reconstruction","text":"<pre><code>evaluate_reconstruction(\n    real_data: DataFrame,\n    synthetic_data: DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n    metadata_path: Path = None,\n) -&gt; Tuple[pd.Series, pd.Series]\n</code></pre> <p>Comprehensive evaluation of reconstruction quality using SDMetrics.</p> <p>This function orchestrates a complete quality assessment of synthetic/reconstructed data by computing both diagnostic and quality metrics. It's the main entry point for evaluating VAE reconstructions or VAE+RecNet outputs.</p> <p>The evaluation includes: 1. Boundary Adherence: Do synthetic values stay within real data bounds? 2. Distribution Similarity: Do synthetic distributions match real distributions? 3. Quality Report: Overall assessment of column shapes and correlations</p> <p>Args:     real_data: Real gene expression data (samples \u00d7 genes)     synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)     save_path_data: Directory path to save all metric results (CSV, PKL)     save_path_figures: Optional directory path to save visualization plots     metadata_path: Path to CSV file used to extract metadata structure                    (typically the test set CSV file)</p> <p>Returns:     Tuple of (boundary_adherence_series, ks_complement_series):     - boundary_adherence_series: Series with boundary adherence scores per gene     - ks_complement_series: Series with KS Complement scores per gene</p> <p>Workflow:     1. Extract metadata from test CSV file     2. Compute diagnostic metrics (boundary adherence)     3. Compute quality metrics (KS complement + quality report)     4. Save all results and visualizations</p> <p>Output Files:     In save_path_data/:     - boundary_adherence_per_gene.csv: Per-gene boundary scores     - ks_complement_per_gene.csv: Per-gene distribution similarity     - quality_report.pkl: Full SDMetrics quality report object</p> <pre><code>In save_path_figures/ (if provided):\n- boundary_adherence_per_gene.{html,png,pdf,svg}\n- ks_complement_per_gene.{html,png,pdf,svg}\n</code></pre> <p>Interpretation:     - Higher scores are better for both metrics (range: 0.0 to 1.0)     - Boundary Adherence: Checks if synthetic data stays in valid ranges     - KS Complement: Checks if distributions match (more stringent)     - Good reconstruction: BA &gt; 0.95, KS &gt; 0.85     - Excellent reconstruction: BA &gt; 0.99, KS &gt; 0.90</p> <p>Example:     &gt;&gt;&gt; ba_scores, ks_scores = evaluate_reconstruction(     ...     real_data=X_test,     ...     synthetic_data=vae_reconstruction,     ...     save_path_data=\"results/vae_eval/\",     ...     save_path_figures=\"figures/vae_eval/\",     ...     metadata_path=\"data/X_test.csv\"     ... )     &gt;&gt;&gt; print(f\"Mean BA: {ba_scores.mean():.4f}, Mean KS: {ks_scores.mean():.4f}\")     Mean BA: 0.9823, Mean KS: 0.8756</p> <p>Note:     - Ensure real_data and synthetic_data have identical column names and order     - The metadata_path CSV should have the same structure as real_data     - This function is used by scripts/pipeline_steps/3_check_reconstruction.py     - Both DataFrames should have samples as rows and genes as columns</p> <p>Raises:     ValueError: If data shapes don't match or columns don't align     FileNotFoundError: If metadata_path doesn't exist</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def evaluate_reconstruction(\n    real_data: pd.DataFrame,\n    synthetic_data: pd.DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n    metadata_path: Path = None,\n) -&gt; Tuple[pd.Series, pd.Series]:\n    \"\"\"\n    Comprehensive evaluation of reconstruction quality using SDMetrics.\n\n    This function orchestrates a complete quality assessment of synthetic/reconstructed\n    data by computing both diagnostic and quality metrics. It's the main entry point\n    for evaluating VAE reconstructions or VAE+RecNet outputs.\n\n    The evaluation includes:\n    1. Boundary Adherence: Do synthetic values stay within real data bounds?\n    2. Distribution Similarity: Do synthetic distributions match real distributions?\n    3. Quality Report: Overall assessment of column shapes and correlations\n\n    Args:\n        real_data: Real gene expression data (samples \u00d7 genes)\n        synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)\n        save_path_data: Directory path to save all metric results (CSV, PKL)\n        save_path_figures: Optional directory path to save visualization plots\n        metadata_path: Path to CSV file used to extract metadata structure\n                       (typically the test set CSV file)\n\n    Returns:\n        Tuple of (boundary_adherence_series, ks_complement_series):\n        - boundary_adherence_series: Series with boundary adherence scores per gene\n        - ks_complement_series: Series with KS Complement scores per gene\n\n    Workflow:\n        1. Extract metadata from test CSV file\n        2. Compute diagnostic metrics (boundary adherence)\n        3. Compute quality metrics (KS complement + quality report)\n        4. Save all results and visualizations\n\n    Output Files:\n        In save_path_data/:\n        - boundary_adherence_per_gene.csv: Per-gene boundary scores\n        - ks_complement_per_gene.csv: Per-gene distribution similarity\n        - quality_report.pkl: Full SDMetrics quality report object\n\n        In save_path_figures/ (if provided):\n        - boundary_adherence_per_gene.{html,png,pdf,svg}\n        - ks_complement_per_gene.{html,png,pdf,svg}\n\n    Interpretation:\n        - Higher scores are better for both metrics (range: 0.0 to 1.0)\n        - Boundary Adherence: Checks if synthetic data stays in valid ranges\n        - KS Complement: Checks if distributions match (more stringent)\n        - Good reconstruction: BA &gt; 0.95, KS &gt; 0.85\n        - Excellent reconstruction: BA &gt; 0.99, KS &gt; 0.90\n\n    Example:\n        &gt;&gt;&gt; ba_scores, ks_scores = evaluate_reconstruction(\n        ...     real_data=X_test,\n        ...     synthetic_data=vae_reconstruction,\n        ...     save_path_data=\"results/vae_eval/\",\n        ...     save_path_figures=\"figures/vae_eval/\",\n        ...     metadata_path=\"data/X_test.csv\"\n        ... )\n        &gt;&gt;&gt; print(f\"Mean BA: {ba_scores.mean():.4f}, Mean KS: {ks_scores.mean():.4f}\")\n        Mean BA: 0.9823, Mean KS: 0.8756\n\n    Note:\n        - Ensure real_data and synthetic_data have identical column names and order\n        - The metadata_path CSV should have the same structure as real_data\n        - This function is used by scripts/pipeline_steps/3_check_reconstruction.py\n        - Both DataFrames should have samples as rows and genes as columns\n\n    Raises:\n        ValueError: If data shapes don't match or columns don't align\n        FileNotFoundError: If metadata_path doesn't exist\n    \"\"\"\n    logger.info(\"=\" * 80)\n    logger.info(\"RECONSTRUCTION QUALITY EVALUATION\")\n    logger.info(\"=\" * 80)\n    logger.info(f\"Real data shape: {real_data.shape}\")\n    logger.info(f\"Synthetic data shape: {synthetic_data.shape}\")\n    logger.info(f\"Saving results to: {save_path_data}\")\n    if save_path_figures:\n        logger.info(f\"Saving figures to: {save_path_figures}\")\n\n    # Validate inputs\n    if real_data.shape != synthetic_data.shape:\n        raise ValueError(\n            f\"Data shape mismatch: real {real_data.shape} vs synthetic {synthetic_data.shape}\"\n        )\n\n    if not all(real_data.columns == synthetic_data.columns):\n        raise ValueError(\"Column names must match between real and synthetic data\")\n\n    # Step 1: Extract metadata in SDMetrics format\n    logger.info(\"\\n[1/3] Extracting metadata...\")\n    metadata_sd = get_metadata(metadata_path)\n    logger.info(f\"Metadata extracted for {len(metadata_sd['columns'])} genes\")\n\n    # Step 2: Compute diagnostic metrics\n    logger.info(\"\\n[2/3] Computing diagnostic metrics...\")\n    df_ba = diagnostic_metrics(\n        real_data=real_data,\n        synthetic_data=synthetic_data,\n        save_path_data=save_path_data,\n        save_path_figures=save_path_figures\n    )\n\n    # Step 3: Compute quality metrics\n    logger.info(\"\\n[3/3] Computing quality metrics...\")\n    df_ks = quality_metrics(\n        real_data=real_data,\n        synthetic_data=synthetic_data,\n        metadata=metadata_sd,\n        save_path_data=save_path_data,\n        save_path_figures=save_path_figures\n    )\n\n    # Final summary\n    logger.info(\"\\n\" + \"=\" * 80)\n    logger.info(\"EVALUATION COMPLETE - SUMMARY\")\n    logger.info(\"=\" * 80)\n    logger.info(f\"Boundary Adherence - Mean: {df_ba.mean():.4f}, Median: {df_ba.median():.4f}\")\n    logger.info(f\"KS Complement      - Mean: {df_ks.mean():.4f}, Median: {df_ks.median():.4f}\")\n\n    # Quality assessment\n    ba_quality = \"Excellent\" if df_ba.mean() &gt; 0.99 else \"Good\" if df_ba.mean() &gt; 0.95 else \"Fair\" if df_ba.mean() &gt; 0.90 else \"Poor\"\n    ks_quality = \"Excellent\" if df_ks.mean() &gt; 0.90 else \"Good\" if df_ks.mean() &gt; 0.85 else \"Fair\" if df_ks.mean() &gt; 0.75 else \"Poor\"\n\n    logger.info(f\"Overall Assessment - Boundary: {ba_quality}, Distribution: {ks_quality}\")\n    logger.info(f\"Results saved to: {save_path_data}\")\n    logger.info(\"=\" * 80)\n\n    return df_ba, df_ks\n</code></pre>"},{"location":"api/prediction/#quality-metrics","title":"Quality Metrics","text":""},{"location":"api/prediction/#diagnostic_metrics","title":"diagnostic_metrics","text":"<p>Calculate diagnostic metrics for model evaluation.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.diagnostic_metrics","title":"diagnostic_metrics","text":"<pre><code>diagnostic_metrics(\n    real_data: DataFrame,\n    synthetic_data: DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n) -&gt; pd.Series\n</code></pre> <p>Calculate diagnostic metrics to assess synthetic data quality.</p> <p>This function computes the Boundary Adherence metric for each gene, which measures whether synthetic values respect the min/max boundaries of real data. This is a critical diagnostic to detect mode collapse or distribution shift.</p> <p>Args:     real_data: Real gene expression data (samples \u00d7 genes)     synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)     save_path_data: Directory path to save metric CSV results     save_path_figures: Optional directory path to save visualization plots</p> <p>Returns:     Series with boundary adherence scores per gene (index=gene, values=scores).     Scores range from 0.0 (worst) to 1.0 (best).</p> <p>Metric Details:     Boundary Adherence per Gene:     - 1.0 (best): All synthetic values are within [min, max] of real data     - 0.0 (worst): No synthetic values fall within real data boundaries     - Values between 0-1 indicate partial adherence</p> <p>Saves:     - {save_path_data}/boundary_adherence_per_gene.csv: Per-gene scores     - {save_path_figures}/boundary_adherence_per_gene.html: Interactive histogram     - {save_path_figures}/boundary_adherence_per_gene.{png,pdf,svg}: Static plots</p> <p>Example:     &gt;&gt;&gt; ba_scores = diagnostic_metrics(X_real, X_synthetic, \"results/\", \"figures/\")     &gt;&gt;&gt; print(f\"Mean adherence: {ba_scores.mean():.4f}\")     Mean adherence: 0.9823</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def diagnostic_metrics(\n    real_data: pd.DataFrame,\n    synthetic_data: pd.DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None\n) -&gt; pd.Series:\n    \"\"\"\n    Calculate diagnostic metrics to assess synthetic data quality.\n\n    This function computes the Boundary Adherence metric for each gene, which\n    measures whether synthetic values respect the min/max boundaries of real data.\n    This is a critical diagnostic to detect mode collapse or distribution shift.\n\n    Args:\n        real_data: Real gene expression data (samples \u00d7 genes)\n        synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)\n        save_path_data: Directory path to save metric CSV results\n        save_path_figures: Optional directory path to save visualization plots\n\n    Returns:\n        Series with boundary adherence scores per gene (index=gene, values=scores).\n        Scores range from 0.0 (worst) to 1.0 (best).\n\n    Metric Details:\n        Boundary Adherence per Gene:\n        - 1.0 (best): All synthetic values are within [min, max] of real data\n        - 0.0 (worst): No synthetic values fall within real data boundaries\n        - Values between 0-1 indicate partial adherence\n\n    Saves:\n        - {save_path_data}/boundary_adherence_per_gene.csv: Per-gene scores\n        - {save_path_figures}/boundary_adherence_per_gene.html: Interactive histogram\n        - {save_path_figures}/boundary_adherence_per_gene.{png,pdf,svg}: Static plots\n\n    Example:\n        &gt;&gt;&gt; ba_scores = diagnostic_metrics(X_real, X_synthetic, \"results/\", \"figures/\")\n        &gt;&gt;&gt; print(f\"Mean adherence: {ba_scores.mean():.4f}\")\n        Mean adherence: 0.9823\n    \"\"\"\n\n    logger.info(\"=\" * 60)\n    logger.info(\"DIAGNOSTIC METRICS: Boundary Adherence\")\n    logger.info(\"=\" * 60)\n    logger.info(f\"Evaluating {real_data.shape[1]} genes across {real_data.shape[0]} samples\")\n\n    # Calculate boundary adherence for each gene\n    # This measures what percentage of synthetic values fall within the\n    # [min, max] range observed in the real data\n    ba_dict = {}\n\n    for gene_i in tqdm(real_data.columns, desc='Computing Boundary Adherence'):\n        # Compute metric: % of synthetic values within [min, max] of real values\n        ba_i = BoundaryAdherence.compute(\n            real_data=real_data[gene_i],\n            synthetic_data=synthetic_data[gene_i]\n        )\n        ba_dict[gene_i] = ba_i\n\n    # Convert to Series for easy analysis\n    df_ba = pd.Series(ba_dict, name='boundary_adherence')\n\n    # Save results\n    output_csv = os.path.join(save_path_data, 'boundary_adherence_per_gene.csv')\n    df_ba.to_csv(output_csv)\n    logger.info(f\"Saved results to: {output_csv}\")\n\n    # Log summary statistics\n    logger.info(f\"Mean Boundary Adherence: {df_ba.mean():.4f}\")\n    logger.info(f\"Median Boundary Adherence: {df_ba.median():.4f}\")\n    logger.info(f\"Min Boundary Adherence: {df_ba.min():.4f}\")\n    logger.info(f\"Max Boundary Adherence: {df_ba.max():.4f}\")\n    logger.info(f\"Genes with perfect adherence (1.0): {(df_ba == 1.0).sum()}/{len(df_ba)} ({100*(df_ba == 1.0).sum()/len(df_ba):.1f}%)\")\n\n    # Generate visualizations if output directory provided\n    if save_path_figures is not None:\n        logger.info(\"Generating visualizations...\")\n\n        # Create interactive histogram\n        fig = px.histogram(\n            df_ba,\n            x='boundary_adherence',\n            nbins=50,\n            title='Distribution of Boundary Adherence Scores per Gene',\n            labels={'boundary_adherence': 'Boundary Adherence Score'},\n            template='plotly_white'\n        )\n        fig.update_layout(\n            xaxis_title='Boundary Adherence Score',\n            yaxis_title='Number of Genes',\n            showlegend=False\n        )\n\n        # Save in multiple formats\n        html_path = os.path.join(save_path_figures, 'boundary_adherence_per_gene.html')\n        fig.write_html(html_path)\n        logger.info(f\"  Saved interactive plot: {html_path}\")\n\n        for format_ext in ['png', 'pdf', 'svg']:\n            img_path = os.path.join(save_path_figures, f'boundary_adherence_per_gene.{format_ext}')\n            fig.write_image(img_path, scale=2)\n            logger.info(f\"  Saved {format_ext.upper()} plot: {img_path}\")\n\n    logger.info(\"Diagnostic metrics calculation complete\")\n    logger.info(\"=\" * 60)\n\n    return df_ba\n</code></pre>"},{"location":"api/prediction/#quality_metrics","title":"quality_metrics","text":"<p>Calculate quality metrics for generated trajectories.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.quality_metrics","title":"quality_metrics","text":"<pre><code>quality_metrics(\n    real_data: DataFrame,\n    synthetic_data: DataFrame,\n    metadata: Dict[str, Any],\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n) -&gt; pd.Series\n</code></pre> <p>Calculate quality metrics to assess synthetic data fidelity.</p> <p>This function computes two key metrics: 1. Quality Report: Overall assessment of column shapes and pair-wise trends 2. KS Complement: Per-gene similarity of marginal distributions</p> <p>These metrics evaluate how well the synthetic data captures the statistical properties of the real data, beyond just staying within boundaries.</p> <p>Args:     real_data: Real gene expression data (samples \u00d7 genes)     synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)     metadata: Metadata dictionary from get_metadata() for SDMetrics     save_path_data: Directory path to save metric results     save_path_figures: Optional directory path to save visualization plots</p> <p>Returns:     Series with KS Complement scores per gene (index=gene, values=scores).     Scores range from 0.0 (worst) to 1.0 (best).</p> <p>Metrics Details:     Column Shapes (in Quality Report):     - Measures overall distribution similarity per column     - Higher scores indicate better shape matching</p> <pre><code>Column Pair Trends (in Quality Report):\n- Measures correlation and relationship preservation\n- Higher scores indicate better trend matching\n\nKS Complement (per gene):\n- 1.0 (best): Real and synthetic distributions are identical\n- 0.0 (worst): Distributions are maximally different\n- Based on Kolmogorov-Smirnov test\n</code></pre> <p>Saves:     - {save_path_data}/quality_report.pkl: Full SDMetrics quality report     - {save_path_data}/ks_complement_per_gene.csv: Per-gene KS scores     - {save_path_figures}/ks_complement_per_gene.html: Interactive histogram     - {save_path_figures}/ks_complement_per_gene.{png,pdf,svg}: Static plots</p> <p>Example:     &gt;&gt;&gt; ks_scores = quality_metrics(X_real, X_synth, metadata, \"results/\", \"figs/\")     &gt;&gt;&gt; print(f\"Mean KS Complement: {ks_scores.mean():.4f}\")     Mean KS Complement: 0.8756</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def quality_metrics(\n    real_data: pd.DataFrame,\n    synthetic_data: pd.DataFrame,\n    metadata: Dict[str, Any],\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None\n) -&gt; pd.Series:\n    \"\"\"\n    Calculate quality metrics to assess synthetic data fidelity.\n\n    This function computes two key metrics:\n    1. Quality Report: Overall assessment of column shapes and pair-wise trends\n    2. KS Complement: Per-gene similarity of marginal distributions\n\n    These metrics evaluate how well the synthetic data captures the statistical\n    properties of the real data, beyond just staying within boundaries.\n\n    Args:\n        real_data: Real gene expression data (samples \u00d7 genes)\n        synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)\n        metadata: Metadata dictionary from get_metadata() for SDMetrics\n        save_path_data: Directory path to save metric results\n        save_path_figures: Optional directory path to save visualization plots\n\n    Returns:\n        Series with KS Complement scores per gene (index=gene, values=scores).\n        Scores range from 0.0 (worst) to 1.0 (best).\n\n    Metrics Details:\n        Column Shapes (in Quality Report):\n        - Measures overall distribution similarity per column\n        - Higher scores indicate better shape matching\n\n        Column Pair Trends (in Quality Report):\n        - Measures correlation and relationship preservation\n        - Higher scores indicate better trend matching\n\n        KS Complement (per gene):\n        - 1.0 (best): Real and synthetic distributions are identical\n        - 0.0 (worst): Distributions are maximally different\n        - Based on Kolmogorov-Smirnov test\n\n    Saves:\n        - {save_path_data}/quality_report.pkl: Full SDMetrics quality report\n        - {save_path_data}/ks_complement_per_gene.csv: Per-gene KS scores\n        - {save_path_figures}/ks_complement_per_gene.html: Interactive histogram\n        - {save_path_figures}/ks_complement_per_gene.{png,pdf,svg}: Static plots\n\n    Example:\n        &gt;&gt;&gt; ks_scores = quality_metrics(X_real, X_synth, metadata, \"results/\", \"figs/\")\n        &gt;&gt;&gt; print(f\"Mean KS Complement: {ks_scores.mean():.4f}\")\n        Mean KS Complement: 0.8756\n    \"\"\"\n    import os\n    from tqdm import tqdm\n    import plotly.express as px\n    from sdmetrics.reports.single_table import QualityReport\n    from sdmetrics.single_column import KSComplement\n\n    logger.info(\"=\" * 60)\n    logger.info(\"QUALITY METRICS: Distribution Similarity\")\n    logger.info(\"=\" * 60)\n\n    # Generate comprehensive quality report\n    # This evaluates:\n    # 1. Column Shapes: How well distributions match per gene\n    # 2. Column Pair Trends: How well correlations are preserved\n    logger.info(\"Generating SDMetrics Quality Report...\")\n    q_report = QualityReport()\n    q_report.generate(real_data, synthetic_data, metadata)\n\n    # Save quality report object for later analysis\n    report_path = os.path.join(save_path_data, 'quality_report.pkl')\n    q_report.save(report_path)\n    logger.info(f\"Saved quality report to: {report_path}\")\n\n    # Get overall quality score from the report\n    overall_score = q_report.get_score()\n    logger.info(f\"Overall Quality Score: {overall_score:.4f}\")\n\n    # Calculate KS Complement for each gene\n    # This measures similarity of marginal distributions (1D histograms)\n    # KS Complement = 1 - KS statistic, where KS statistic measures max difference between CDFs\n    logger.info(f\"Computing KS Complement for {real_data.shape[1]} genes...\")\n\n    ks_dict = {}\n    for gene_i in tqdm(real_data.columns, desc='Computing KS Complement'):\n        # KS Complement measures how similar the empirical cumulative distribution functions are\n        # Higher values mean the distributions are more similar\n        ks_i = KSComplement.compute(\n            real_data=real_data[gene_i],\n            synthetic_data=synthetic_data[gene_i]\n        )\n        ks_dict[gene_i] = ks_i\n\n    # Convert to Series for analysis\n    df_ks = pd.Series(ks_dict, name='ks_complement')\n\n    # Save results\n    output_csv = os.path.join(save_path_data, 'ks_complement_per_gene.csv')\n    df_ks.to_csv(output_csv)\n    logger.info(f\"Saved results to: {output_csv}\")\n\n    # Log summary statistics\n    logger.info(f\"Mean KS Complement: {df_ks.mean():.4f}\")\n    logger.info(f\"Median KS Complement: {df_ks.median():.4f}\")\n    logger.info(f\"Min KS Complement: {df_ks.min():.4f}\")\n    logger.info(f\"Max KS Complement: {df_ks.max():.4f}\")\n    logger.info(f\"Genes with KS &gt; 0.9: {(df_ks &gt; 0.9).sum()}/{len(df_ks)} ({100*(df_ks &gt; 0.9).sum()/len(df_ks):.1f}%)\")\n\n    # Generate visualizations if output directory provided\n    if save_path_figures is not None:\n        logger.info(\"Generating visualizations...\")\n\n        # Create interactive histogram\n        fig = px.histogram(\n            df_ks,\n            x='ks_complement',\n            nbins=50,\n            title='Distribution of KS Complement Scores per Gene',\n            labels={'ks_complement': 'KS Complement Score'},\n            template='plotly_white'\n        )\n        fig.update_layout(\n            xaxis_title='KS Complement Score (Distribution Similarity)',\n            yaxis_title='Number of Genes',\n            showlegend=False\n        )\n\n        # Add reference line at 0.9 (high quality threshold)\n        fig.add_vline(\n            x=0.9,\n            line_dash=\"dash\",\n            line_color=\"red\",\n            annotation_text=\"High Quality (0.9)\"\n        )\n\n        # Save in multiple formats\n        html_path = os.path.join(save_path_figures, 'ks_complement_per_gene.html')\n        fig.write_html(html_path)\n        logger.info(f\"  Saved interactive plot: {html_path}\")\n\n        for format_ext in ['png', 'pdf', 'svg']:\n            img_path = os.path.join(save_path_figures, f'ks_complement_per_gene.{format_ext}')\n            fig.write_image(img_path, scale=2)\n            logger.info(f\"  Saved {format_ext.upper()} plot: {img_path}\")\n\n    logger.info(\"Quality metrics calculation complete\")\n    logger.info(\"=\" * 60)\n\n    return df_ks\n</code></pre>"},{"location":"api/prediction/#trajectory-classification","title":"Trajectory Classification","text":""},{"location":"api/prediction/#classify_trajectories","title":"classify_trajectories","text":"<p>Classify disease progression trajectories as progressing vs. non-progressing.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import classify_trajectories\n\n# Train classifier on trajectories\nclassifier, metrics = classify_trajectories(\n    trajectories=trajectory_data,\n    labels=progression_labels,\n    output_dir=Path(\"models/trajectory_classifier\")\n)\n\nprint(f\"Classification accuracy: {metrics['accuracy']:.3f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.3f}\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.classify_trajectories","title":"classify_trajectories","text":"<pre><code>classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, DataFrame],\n    gene_subset: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Apply stage classifier to synthetic trajectories.</p> <p>Args:     classifier: Trained classifier model     trajectory_data: Dictionary of patient pair to trajectory DataFrames     gene_subset: Optional subset of genes to use for classification</p> <p>Returns:     DataFrame with classification results for each trajectory point</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, pd.DataFrame],\n    gene_subset: Optional[List[str]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply stage classifier to synthetic trajectories.\n\n    Args:\n        classifier: Trained classifier model\n        trajectory_data: Dictionary of patient pair to trajectory DataFrames\n        gene_subset: Optional subset of genes to use for classification\n\n    Returns:\n        DataFrame with classification results for each trajectory point\n    \"\"\"\n    logger.info(\"Classifying trajectory points\")\n\n    # TODO: Implement trajectory classification\n    # Migrate from notebooks/kirc_classification_trajectory.ipynb\n\n    raise NotImplementedError(\n        \"classify_trajectories() needs implementation from \"\n        \"notebooks/kirc_classification_trajectory.ipynb\"\n    )\n</code></pre>"},{"location":"api/prediction/#network-analysis","title":"Network Analysis","text":""},{"location":"api/prediction/#build_trajectory_network","title":"build_trajectory_network","text":"<p>Build network graph of patient trajectories.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.build_trajectory_network","title":"build_trajectory_network","text":"<pre><code>build_trajectory_network(\n    patient_links: DataFrame,\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]\n</code></pre> <p>Build trajectory network and find all complete disease progression paths.</p> <p>Constructs a directed graph from patient links and identifies all possible complete trajectories from root nodes (earliest stage patients not appearing as targets) to leaf nodes (latest stage patients not appearing as sources).</p> <p>Args:     patient_links: DataFrame with 'source' and 'target' columns from linking functions</p> <p>Returns:     Tuple of:     - network: Dict mapping each source patient to list of target patients     - trajectories: List of complete trajectories, where each trajectory is a                     list of patient IDs ordered from earliest to latest stage</p> <p>Network Structure:     - Adjacency list representation: {source: [target1, target2, ...]}     - Directed edges from earlier to later stages     - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)</p> <p>Trajectory Discovery:     - Uses depth-first search from root nodes     - Root nodes: Patients in 'source' but not in 'target' (stage I or early)     - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)     - Each trajectory represents a complete disease progression path</p> <p>Example:     &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)     &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")     &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")     &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")     Network has 500 nodes     Found 234 complete trajectories     Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']</p> <p>Trajectory Characteristics:     - Length varies based on how many stages the path spans     - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions     - Length 2 for early\u2192late progressions     - Patients can appear in multiple trajectories</p> <p>Note:     - Cycles are prevented during trajectory search     - All paths from root to leaf are enumerated     - Trajectories respect chronological disease progression</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def build_trajectory_network(\n    patient_links: pd.DataFrame\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]:\n    \"\"\"\n    Build trajectory network and find all complete disease progression paths.\n\n    Constructs a directed graph from patient links and identifies all possible\n    complete trajectories from root nodes (earliest stage patients not appearing\n    as targets) to leaf nodes (latest stage patients not appearing as sources).\n\n    Args:\n        patient_links: DataFrame with 'source' and 'target' columns from linking functions\n\n    Returns:\n        Tuple of:\n        - network: Dict mapping each source patient to list of target patients\n        - trajectories: List of complete trajectories, where each trajectory is a\n                        list of patient IDs ordered from earliest to latest stage\n\n    Network Structure:\n        - Adjacency list representation: {source: [target1, target2, ...]}\n        - Directed edges from earlier to later stages\n        - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)\n\n    Trajectory Discovery:\n        - Uses depth-first search from root nodes\n        - Root nodes: Patients in 'source' but not in 'target' (stage I or early)\n        - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)\n        - Each trajectory represents a complete disease progression path\n\n    Example:\n        &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)\n        &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")\n        &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")\n        &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")\n        Network has 500 nodes\n        Found 234 complete trajectories\n        Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']\n\n    Trajectory Characteristics:\n        - Length varies based on how many stages the path spans\n        - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions\n        - Length 2 for early\u2192late progressions\n        - Patients can appear in multiple trajectories\n\n    Note:\n        - Cycles are prevented during trajectory search\n        - All paths from root to leaf are enumerated\n        - Trajectories respect chronological disease progression\n    \"\"\"\n    logger.info(\"Building trajectory network from patient links\")\n\n    sources = patient_links['source']\n    targets = patient_links['target']\n\n    # Build network adjacency list\n    network = {}\n    for source, target in zip(sources, targets):\n        if source not in network:\n            network[source] = []\n        network[source].append(target)\n\n    logger.info(f\"Network built: {len(network)} source nodes\")\n\n    # Find root nodes (patients who are sources but never targets)\n    unique_sources = set(sources) - set(targets)\n    logger.info(f\"Found {len(unique_sources)} root nodes (earliest stage patients)\")\n\n    # Recursively find all trajectories from each root\n    def find_trajectories(start_node: str, visited: Optional[List[str]] = None) -&gt; List[List[str]]:\n        \"\"\"Depth-first search to find all paths from start_node to leaf nodes.\"\"\"\n        if visited is None:\n            visited = []\n\n        visited.append(start_node)\n\n        # If node has no outgoing edges, this is a leaf node - return path\n        if start_node not in network:\n            return [visited]\n\n        # Recursively explore all targets\n        trajectories = []\n        for target in network[start_node]:\n            if target not in visited:  # Avoid cycles\n                new_visited = visited.copy()\n                trajectories.extend(find_trajectories(target, new_visited))\n\n        return trajectories\n\n    # Find all trajectories starting from each root\n    all_trajectories = []\n\n    if len(unique_sources) == 0:\n        # No clear root nodes - this happens with early\u2192late transitions where\n        # patients can be both sources and targets. In this case, each source\u2192target\n        # pair is already a complete 2-patient trajectory.\n        logger.info(\"No root nodes found (typical for early\u2192late transitions).\")\n        logger.info(\"Using each source\u2192target pair as a complete trajectory.\")\n        for source, target in zip(sources, targets):\n            all_trajectories.append([source, target])\n    else:\n        # Standard case: multi-stage progressions (I\u2192II\u2192III\u2192IV)\n        for source in unique_sources:\n            all_trajectories.extend(find_trajectories(source))\n\n    logger.info(f\"Discovered {len(all_trajectories)} complete disease progression trajectories\")\n\n    # Log trajectory length statistics only if we have trajectories\n    if len(all_trajectories) &gt; 0:\n        traj_lengths = [len(t) for t in all_trajectories]\n        logger.info(f\"Trajectory lengths - Min: {min(traj_lengths)}, Max: {max(traj_lengths)}, \"\n                    f\"Mean: {np.mean(traj_lengths):.1f}\")\n    else:\n        logger.warning(\"No trajectories found!\")\n\n    return network, all_trajectories\n</code></pre>"},{"location":"api/prediction/#link_patients_closest","title":"link_patients_closest","text":"<p>Link patients using closest neighbor strategy.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.link_patients_closest","title":"link_patients_closest","text":"<pre><code>link_patients_closest(\n    transitions_df: DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients by selecting closest (or farthest) matches across stages.</p> <p>For each patient at a source stage, this function identifies the closest (or farthest) patient at the target stage, considering metadata constraints (gender, race). This creates one-to-one patient linkages that form the basis for trajectory construction.</p> <p>Args:     transitions_df: DataFrame from calculate_all_possible_transitions()                     containing all possible patient pairs with distances     start_with_first_stage: If True, build forward trajectories (early\u2192late)                             If False, build backward trajectories (late\u2192early)     early_late: If True, uses early/late groupings. If False, uses I-IV stages     closest: If True, connect closest patients. If False, connect farthest patients</p> <p>Returns:     DataFrame with selected patient links, containing one row per source patient     with their optimal target patient match. Includes all columns from transitions_df.</p> <p>Selection Strategy:     - Forward (start_with_first_stage=True): For each source, find optimal target     - Backward (start_with_first_stage=False): For each target, find optimal source     - Closest (closest=True): Minimum distance match     - Farthest (closest=False): Maximum distance match</p> <p>Metadata Stratification:     Links are selected independently within each combination of:     - Gender (MALE, FEMALE)     - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)     This ensures demographic consistency in trajectories.</p> <p>Example:     &gt;&gt;&gt; links = link_patients_closest(     ...     transitions_df=all_transitions,     ...     start_with_first_stage=True,     ...     closest=True     ... )     &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")     Created 234 patient links</p> <p>Note:     - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV     - Processes in reverse for backward: IV\u2192III\u2192II\u2192I     - Each patient appears at most once as a source in the result</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_closest(\n    transitions_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients by selecting closest (or farthest) matches across stages.\n\n    For each patient at a source stage, this function identifies the closest\n    (or farthest) patient at the target stage, considering metadata constraints\n    (gender, race). This creates one-to-one patient linkages that form the basis\n    for trajectory construction.\n\n    Args:\n        transitions_df: DataFrame from calculate_all_possible_transitions()\n                        containing all possible patient pairs with distances\n        start_with_first_stage: If True, build forward trajectories (early\u2192late)\n                                If False, build backward trajectories (late\u2192early)\n        early_late: If True, uses early/late groupings. If False, uses I-IV stages\n        closest: If True, connect closest patients. If False, connect farthest patients\n\n    Returns:\n        DataFrame with selected patient links, containing one row per source patient\n        with their optimal target patient match. Includes all columns from transitions_df.\n\n    Selection Strategy:\n        - Forward (start_with_first_stage=True): For each source, find optimal target\n        - Backward (start_with_first_stage=False): For each target, find optimal source\n        - Closest (closest=True): Minimum distance match\n        - Farthest (closest=False): Maximum distance match\n\n    Metadata Stratification:\n        Links are selected independently within each combination of:\n        - Gender (MALE, FEMALE)\n        - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)\n        This ensures demographic consistency in trajectories.\n\n    Example:\n        &gt;&gt;&gt; links = link_patients_closest(\n        ...     transitions_df=all_transitions,\n        ...     start_with_first_stage=True,\n        ...     closest=True\n        ... )\n        &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")\n        Created 234 patient links\n\n    Note:\n        - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV\n        - Processes in reverse for backward: IV\u2192III\u2192II\u2192I\n        - Each patient appears at most once as a source in the result\n    \"\"\"\n    logger.info(\"Linking patients by closest/farthest matches\")\n    logger.info(f\"Direction: {'Forward' if start_with_first_stage else 'Backward'}\")\n    logger.info(f\"Strategy: {'Closest' if closest else 'Farthest'}\")\n\n    # Define transition order based on direction\n    if start_with_first_stage and not early_late:\n        transitions_possible = ['1_to_2', '2_to_3', '3_to_4']\n    elif not start_with_first_stage and not early_late:\n        transitions_possible = ['3_to_4', '2_to_3', '1_to_2']\n    elif early_late:\n        transitions_possible = ['early_to_late']\n\n    # 0 for closest (smallest distance), -1 for farthest (largest distance)\n    idx = 0 if closest else -1\n\n    # Find closest/farthest patient for each source patient\n    closest_list = []\n    for transition_i in transitions_possible:\n        transition_df_i = transitions_df[transitions_df['transition'] == transition_i]\n\n        logger.info(f\"Processing transition {transition_i}: {len(transition_df_i)} pairs\")\n\n        # Iterate through all metadata combinations\n        for gender_i in ['FEMALE', 'MALE']:\n            df_gender_i = transition_df_i.query(f\"source_gender == '{gender_i}'\")\n\n            for race_i in ['ASIAN', 'BLACK OR AFRICAN AMERICAN', 'WHITE']:\n                df_race_i = df_gender_i.query(f\"source_race == '{race_i}'\")\n\n                if df_race_i.empty:\n                    continue\n\n                # Get unique patients to link\n                unique_sources = df_race_i['source'].unique()\n                unique_targets = df_race_i['target'].unique()\n                use_uniques = unique_sources if start_with_first_stage else unique_targets\n                use_column = 'source' if start_with_first_stage else 'target'\n\n                # Find closest/farthest match for each patient\n                for pat_i in use_uniques:\n                    pat_matches = df_race_i[df_race_i[use_column] == pat_i]\n                    if len(pat_matches) &gt; 0:\n                        # Sort by distance and select first (closest) or last (farthest)\n                        best_match = pat_matches.sort_values('distance').iloc[idx]\n                        closest_list.append(best_match)\n\n    # Convert to DataFrame\n    closest_df = pd.DataFrame(closest_list)\n    closest_df.reset_index(drop=True, inplace=True)\n\n    logger.info(f\"Created {len(closest_df)} patient links\")\n\n    return closest_df\n</code></pre>"},{"location":"api/prediction/#link_patients_random","title":"link_patients_random","text":"<p>Link patients randomly (for control/comparison).</p>"},{"location":"api/prediction/#renalprog.modeling.predict.link_patients_random","title":"link_patients_random","text":"<pre><code>link_patients_random(\n    results_df: DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients to multiple random targets at the next stage.</p> <p>Instead of linking each patient to only their closest match, this function randomly samples multiple patients at the next stage to link to each source patient. This creates a one-to-many mapping useful for generating multiple trajectory samples.</p> <p>Parameters:</p> Name Type Description Default <code>results_df</code> <code>DataFrame</code> <p>DataFrame with possible sources and targets, their metadata, and distance.</p> required <code>start_with_first_stage</code> <code>bool</code> <p>If True, initiate trajectories with first stage as sources. If False, initiate trajectories with last stage as sources.</p> <code>True</code> <code>link_next</code> <code>int</code> <p>Number of patients at next stage to randomly link to each patient of current stage.</p> <code>5</code> <code>transitions_possible</code> <code>list</code> <p>List of transitions to process (e.g., ['1_to_2', '2_to_3']). If None, defaults to ['early_to_late'].</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with randomly sampled patient links for each transition. Contains multiple rows per source patient (up to link_next).</p> Notes <ul> <li>Random sampling is primarily performed for WHITE race patients due to sample size</li> <li>If fewer than link_next targets are available, all available targets are selected</li> <li>Patients from other races are included with all their possible connections</li> <li>Empty DataFrame is returned if no WHITE patients are found</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_random(\n    results_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients to multiple random targets at the next stage.\n\n    Instead of linking each patient to only their closest match, this function randomly\n    samples multiple patients at the next stage to link to each source patient. This\n    creates a one-to-many mapping useful for generating multiple trajectory samples.\n\n    Parameters\n    ----------\n    results_df : pd.DataFrame\n        DataFrame with possible sources and targets, their metadata, and distance.\n    start_with_first_stage : bool, default=True\n        If True, initiate trajectories with first stage as sources.\n        If False, initiate trajectories with last stage as sources.\n    link_next : int, default=5\n        Number of patients at next stage to randomly link to each patient of current stage.\n    transitions_possible : list, optional\n        List of transitions to process (e.g., ['1_to_2', '2_to_3']).\n        If None, defaults to ['early_to_late'].\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with randomly sampled patient links for each transition.\n        Contains multiple rows per source patient (up to link_next).\n\n    Notes\n    -----\n    - Random sampling is primarily performed for WHITE race patients due to sample size\n    - If fewer than link_next targets are available, all available targets are selected\n    - Patients from other races are included with all their possible connections\n    - Empty DataFrame is returned if no WHITE patients are found\n    \"\"\"\n    # Set default transitions if not provided\n    if transitions_possible is None:\n        transitions_possible = ['early_to_late']\n\n    # Get unique genders and races\n    unique_genders = results_df['source_gender'].unique().tolist()\n    # Get unique races\n    unique_races = results_df['source_race'].unique().tolist()\n    if 'WHITE' in unique_races:\n        unique_races.remove('WHITE')\n    # transition:\n    samples = []\n    for transition_i in transitions_possible:\n        transition_df_i = results_df[results_df['transition'] == transition_i]\n        for gender_i in unique_genders:\n            df_samples_i = transition_df_i.query(\n                f\"source_gender == '{gender_i}' &amp; source_race == 'WHITE'\")  # we can only do this for the whites since these are the only ones with enough samples\n            if df_samples_i.empty:\n                print(f\"Warning: No WHITE patients found for gender {gender_i} in transition {transition_i}\")\n                continue\n            unique_sources_i = np.unique(df_samples_i['source']).tolist()\n            unique_targets_i = np.unique(df_samples_i['target']).tolist()\n            use_uniques = unique_sources_i if start_with_first_stage else unique_targets_i\n            use_source_target = 'source' if start_with_first_stage else 'target'\n            for pat_i in use_uniques:\n                sample_i = df_samples_i.loc[df_samples_i[use_source_target] == pat_i]\n                if len(sample_i) &gt;= link_next:\n                    sample_i = sample_i.sample(\n                        link_next)  # Sample a number of patients at next stage to link to each patient of current stage\n                else:\n                    sample_i = sample_i.sample(len(sample_i))  # Sample all available patients if less than link_next\n                samples.append(sample_i)\n\n    # Check if samples list is empty\n    if not samples:\n        print(\"Warning: No samples found for WHITE race. Returning empty DataFrame.\")\n        return pd.DataFrame(columns=results_df.columns)\n\n    # Turn samples into dataframe:\n    samples_df = pd.concat(samples)\n    # Add the rest of the races\n    if unique_races:\n        samples_df = pd.concat(\n            [\n                samples_df,\n                results_df[results_df['source_race'].isin(unique_races)]\n            ]\n        )\n    samples_df.reset_index(drop=True, inplace=True)\n    return samples_df\n</code></pre>"},{"location":"api/prediction/#dynamic-analysis","title":"Dynamic Analysis","text":""},{"location":"api/prediction/#dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<p>Perform pathway enrichment along trajectory timepoints.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<pre><code>dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Perform dynamic enrichment analysis on synthetic trajectories.</p> <p>This orchestrates: 1. DESeq2 analysis on each trajectory point 2. GSEA on differential expression results 3. Aggregation of enrichment across trajectories</p> <p>Args:     trajectory_dir: Directory containing trajectory CSV files     pathways_file: Path to pathway GMT file     output_dir: Directory to save results     cancer_type: Cancer type identifier</p> <p>Returns:     DataFrame with aggregated enrichment results</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform dynamic enrichment analysis on synthetic trajectories.\n\n    This orchestrates:\n    1. DESeq2 analysis on each trajectory point\n    2. GSEA on differential expression results\n    3. Aggregation of enrichment across trajectories\n\n    Args:\n        trajectory_dir: Directory containing trajectory CSV files\n        pathways_file: Path to pathway GMT file\n        output_dir: Directory to save results\n        cancer_type: Cancer type identifier\n\n    Returns:\n        DataFrame with aggregated enrichment results\n    \"\"\"\n    logger.info(f\"Running dynamic enrichment analysis for {cancer_type}\")\n\n    # TODO: Implement orchestration\n    # Migrate from src_deseq_and_gsea_NCSR/full_bash.sh and related scripts\n\n    raise NotImplementedError(\n        \"dynamic_enrichment_analysis() needs implementation. \"\n        \"Migrate orchestration from src_deseq_and_gsea_NCSR/full_bash.sh, \"\n        \"py_deseq.py, and trajectory_analysis.py\"\n    )\n</code></pre>"},{"location":"api/prediction/#calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<p>Calculate all possible patient-to-patient transitions.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<pre><code>calculate_all_possible_transitions(\n    data: DataFrame,\n    metadata_selection: DataFrame,\n    distance: str = \"wasserstein\",\n    early_late: bool = False,\n    negative_trajectory: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Calculate all possible patient-to-patient transitions for KIRC cancer.</p> <p>This function computes pairwise distances between all patients at consecutive (or same) cancer stages, considering metadata constraints. Only patients with matching gender and race are considered as potential trajectory pairs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Gene expression data with patients as columns.</p> required <code>metadata_selection</code> <code>DataFrame</code> <p>Clinical metadata with columns: histological_type, race, gender, stage.</p> required <code>distance</code> <code>(wasserstein, euclidean)</code> <p>Distance metric to use for calculating patient-to-patient distances.</p> <code>'wasserstein'</code> <code>early_late</code> <code>bool</code> <p>If True, uses early/late stage groupings. If False, uses I-IV stages.</p> <code>False</code> <code>negative_trajectory</code> <code>bool</code> <p>If True, generates same-stage transitions (negative controls). If False, generates progression transitions (positive trajectories).</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing all possible transitions with columns: - source, target: Patient IDs - source_gender, target_gender: Gender - source_race, target_race: Race - transition: Stage transition label (e.g., '1_to_2', 'early_to_late') - distance: Calculated distance between patients</p> <p>Sorted by gender, race, transition, and distance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance metric is not 'wasserstein' or 'euclidean'.</p> Notes <ul> <li>For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late</li> <li>For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late</li> <li>Only patients with identical gender and race are paired</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def calculate_all_possible_transitions(\n    data: pd.DataFrame,\n    metadata_selection: pd.DataFrame,\n    distance: str = 'wasserstein',\n    early_late: bool = False,\n    negative_trajectory: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate all possible patient-to-patient transitions for KIRC cancer.\n\n    This function computes pairwise distances between all patients at consecutive\n    (or same) cancer stages, considering metadata constraints. Only patients with\n    matching gender and race are considered as potential trajectory pairs.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Gene expression data with patients as columns.\n    metadata_selection : pd.DataFrame\n        Clinical metadata with columns: histological_type, race, gender, stage.\n    distance : {'wasserstein', 'euclidean'}, default='wasserstein'\n        Distance metric to use for calculating patient-to-patient distances.\n    early_late : bool, default=False\n        If True, uses early/late stage groupings. If False, uses I-IV stages.\n    negative_trajectory : bool, default=False\n        If True, generates same-stage transitions (negative controls).\n        If False, generates progression transitions (positive trajectories).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing all possible transitions with columns:\n        - source, target: Patient IDs\n        - source_gender, target_gender: Gender\n        - source_race, target_race: Race\n        - transition: Stage transition label (e.g., '1_to_2', 'early_to_late')\n        - distance: Calculated distance between patients\n\n        Sorted by gender, race, transition, and distance.\n\n    Raises\n    ------\n    ValueError\n        If distance metric is not 'wasserstein' or 'euclidean'.\n\n    Notes\n    -----\n    - For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late\n    - For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late\n    - Only patients with identical gender and race are paired\n    \"\"\"\n    # Select distance function\n    if distance == 'wasserstein':\n        from scipy.stats import wasserstein_distance\n        distance_fun = wasserstein_distance\n    elif distance == 'euclidean':\n        from scipy.spatial.distance import euclidean\n        distance_fun = euclidean\n    else:\n        raise ValueError('Distance function not implemented. Use either \"wasserstein\" or \"euclidean\".')\n\n    # Define stage transitions based on parameters\n    if early_late and not negative_trajectory:\n        possible_transitions = ['early_to_late']\n        stage_pairs = [['early', 'late']]\n    elif early_late and negative_trajectory:\n        possible_transitions = ['early_to_early', 'late_to_late']\n        stage_pairs = [['early', 'early'], ['late', 'late']]\n    elif not early_late and not negative_trajectory:\n        possible_transitions = ['1_to_2', '2_to_3', '3_to_4']\n        stage_pairs = [['I', 'II'], ['II', 'III'], ['III', 'IV']]\n    elif not early_late and negative_trajectory:\n        possible_transitions = ['1_to_1', '2_to_2', '3_to_3', '4_to_4']\n        stage_pairs = [['I', 'I'], ['II', 'II'], ['III', 'III'], ['IV', 'IV']]\n\n    # Calculate all possible transitions\n    results = []\n    for i_tr, transition in enumerate(possible_transitions):\n        source_target_stage = stage_pairs[i_tr]\n\n        # Iterate through all patient pairs at specified stages\n        for pat_i in metadata_selection.index[metadata_selection['stage'] == source_target_stage[0]]:\n            for pat_ii in metadata_selection.index[metadata_selection['stage'] == source_target_stage[1]]:\n                # Extract metadata for both patients\n                source_gender = metadata_selection.at[pat_i, 'gender']\n                target_gender = metadata_selection.at[pat_ii, 'gender']\n                source_race = metadata_selection.at[pat_i, 'race']\n                target_race = metadata_selection.at[pat_ii, 'race']\n\n                # Skip if metadata doesn't match (gender and race must match)\n                if not (source_race == target_race and source_gender == target_gender):\n                    continue\n\n                # Store transition information\n                results_i = {\n                    'source': pat_i,\n                    'target': pat_ii,\n                    'source_gender': source_gender,\n                    'target_gender': target_gender,\n                    'source_race': source_race,\n                    'target_race': target_race,\n                    'transition': transition,\n                    'distance': distance_fun(data[pat_i], data[pat_ii]),\n                }\n                results.append(results_i)\n\n    # Convert to DataFrame and sort\n    results_df = pd.DataFrame(results)\n    results_df.sort_values(\n        ['source_gender', 'target_gender', 'source_race', 'target_race',\n         'transition', 'distance'],\n        inplace=True,\n        ignore_index=True\n    )\n    return results_df\n</code></pre>"},{"location":"api/prediction/#metadata","title":"Metadata","text":""},{"location":"api/prediction/#get_metadata","title":"get_metadata","text":"<p>Extract metadata from model directory.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata(test_path: Path) -&gt; Dict[str, Any]\n</code></pre> <p>Extract metadata from test dataset in SDMetrics format.</p> <p>This function loads a CSV file and extracts its column metadata using SDMetrics' automatic detection. The metadata describes the structure and data types of the dataset, which is required for SDMetrics quality evaluation.</p> <p>Args:     test_path: Path to the CSV file containing the test dataset.                Can be a string or Path object.</p> <p>Returns:     Dictionary containing metadata with column names and data types.     Format: {'columns': {col_name: {'sdtype': type}}}</p> <p>Note:     - The CSV is loaded with index_col=0 to avoid treating the index as a feature     - Both real and synthetic data must share the same metadata structure     - This ensures SDMetrics can properly validate and compare the datasets</p> <p>Example:     &gt;&gt;&gt; metadata = get_metadata(\"data/X_test.csv\")     &gt;&gt;&gt; print(metadata['columns'].keys())     dict_keys(['gene1', 'gene2', ...])</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def get_metadata(test_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract metadata from test dataset in SDMetrics format.\n\n    This function loads a CSV file and extracts its column metadata using SDMetrics'\n    automatic detection. The metadata describes the structure and data types of\n    the dataset, which is required for SDMetrics quality evaluation.\n\n    Args:\n        test_path: Path to the CSV file containing the test dataset.\n                   Can be a string or Path object.\n\n    Returns:\n        Dictionary containing metadata with column names and data types.\n        Format: {'columns': {col_name: {'sdtype': type}}}\n\n    Note:\n        - The CSV is loaded with index_col=0 to avoid treating the index as a feature\n        - Both real and synthetic data must share the same metadata structure\n        - This ensures SDMetrics can properly validate and compare the datasets\n\n    Example:\n        &gt;&gt;&gt; metadata = get_metadata(\"data/X_test.csv\")\n        &gt;&gt;&gt; print(metadata['columns'].keys())\n        dict_keys(['gene1', 'gene2', ...])\n    \"\"\"\n    from pathlib import Path as pathlib_Path\n\n    logger.info(\"Extracting metadata for SDMetrics evaluation\")\n    logger.info(f\"Loading data from: {test_path}\")\n\n    # Convert to Path object for consistent handling across platforms\n    test_path = pathlib_Path(test_path)\n\n    # Load data using SDMetrics CSV handler\n    # CRITICAL: index_col=0 prevents the index from being treated as a feature column\n    # This would cause metadata mismatch errors if the index is included\n    connector = CSVHandler()\n    real_data = connector.read(\n        folder_name=str(test_path.parent),\n        file_names=[test_path.name],\n        read_csv_parameters={\n            'index_col': 0,        # Use first column as index, not as feature\n            'parse_dates': False,  # Don't parse dates (all numeric gene expression)\n            'encoding': 'latin-1'  # Standard encoding for CSV files\n        }\n    )\n\n    # Auto-detect metadata from the loaded data\n    metadata = Metadata.detect_from_dataframes(data=real_data)\n\n    # Extract table-specific metadata (removes wrapper structure)\n    # The key 'X_test' matches the filename without extension\n    metadata_use = metadata.to_dict()['tables']['X_test']\n\n    logger.info(f\"Extracted metadata for {len(metadata_use['columns'])} genes\")\n\n    return metadata_use\n</code></pre>"},{"location":"api/prediction/#complete-example","title":"Complete Example","text":"<pre><code>import torch\nimport pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE\nfrom renalprog.modeling.predict import (\n    apply_vae,\n    create_patient_connections,\n    generate_trajectories,\n    evaluate_reconstruction\n)\nfrom renalprog.plots import plot_latent_space, plot_trajectory\n\n# Load model and data\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel.load_state_dict(torch.load(\"models/my_vae/best_model.pt\"))\n\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# Encode data\ntrain_results = apply_vae(model, train_expr.values, device='cuda')\ntest_results = apply_vae(model, test_expr.values, device='cuda')\n\n# Visualize latent space\nplot_latent_space(\n    latent=test_results['latent'],\n    labels=clinical['stage'],\n    output_path=Path(\"reports/figures/latent_space.png\")\n)\n\n# Create patient connections\nearly_mask = clinical['stage'] == 'early'\nlate_mask = clinical['stage'] == 'late'\n\nconnections = create_patient_connections(\n    latent_early=test_results['latent'][early_mask],\n    latent_late=test_results['latent'][late_mask],\n    method='closest',\n    output_path=Path(\"data/processed/connections.csv\")\n)\n\n# Generate trajectories\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=test_expr.values[early_mask],\n    end_data=test_expr.values[late_mask],\n    n_steps=50,\n    interpolation='spherical',\n    device='cuda'\n)\n\n# Evaluate reconstruction\nmetrics = evaluate_reconstruction(\n    model=model,\n    original_data=test_expr.values,\n    device='cuda',\n    output_dir=Path(\"reports/reconstruction\")\n)\n\nprint(f\"Generated {len(trajectories)} trajectories\")\nprint(f\"Reconstruction MSE: {metrics['mse']:.4f}\")\n</code></pre>"},{"location":"api/prediction/#see-also","title":"See Also","text":"<ul> <li>Training API - Train VAE models</li> <li>Trajectories API - Trajectory analysis</li> <li>Models API - VAE architectures</li> </ul>"},{"location":"api/training/","title":"Training API","text":"<p>Complete training pipeline for VAE models with checkpointing and monitoring.</p>"},{"location":"api/training/#overview","title":"Overview","text":"<p>The training module provides high-level functions for:</p> <ul> <li>Complete VAE training workflow</li> <li>Automatic checkpointing</li> <li>Training history visualization</li> <li>Early stopping</li> <li>Learning rate scheduling</li> </ul>"},{"location":"api/training/#main-training-function","title":"Main Training Function","text":""},{"location":"api/training/#train_vae","title":"train_vae","text":"<p>The main training function that orchestrates the entire VAE training process.</p>"},{"location":"api/training/#renalprog.modeling.train.train_vae","title":"train_vae","text":"<pre><code>train_vae(\n    X_train: ndarray,\n    X_test: ndarray,\n    y_train: Optional[ndarray] = None,\n    y_test: Optional[ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]\n</code></pre> <p>Train a VAE model with full checkpointing support.</p> <p>Args:     X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame     X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame     y_train: Optional training labels for CVAE     y_test: Optional test labels for CVAE     config: Training configuration     save_dir: Directory to save checkpoints     resume_from: Optional checkpoint path to resume training     force_cpu: Force CPU usage even if CUDA is available (for compatibility)</p> <p>Returns:     Tuple of (trained_model, training_history)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_vae(\n    X_train: np.ndarray,\n    X_test: np.ndarray,\n    y_train: Optional[np.ndarray] = None,\n    y_test: Optional[np.ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]:\n    \"\"\"Train a VAE model with full checkpointing support.\n\n    Args:\n        X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame\n        X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame\n        y_train: Optional training labels for CVAE\n        y_test: Optional test labels for CVAE\n        config: Training configuration\n        save_dir: Directory to save checkpoints\n        resume_from: Optional checkpoint path to resume training\n        force_cpu: Force CPU usage even if CUDA is available (for compatibility)\n\n    Returns:\n        Tuple of (trained_model, training_history)\n    \"\"\"\n    # Convert DataFrames to numpy arrays if needed\n    if hasattr(X_train, 'values'):  # Check if it's a DataFrame\n        X_train = X_train.values\n    if hasattr(X_test, 'values'):  # Check if it's a DataFrame\n        X_test = X_test.values\n    if y_train is not None and hasattr(y_train, 'values'):\n        y_train = y_train.values\n    if y_test is not None and hasattr(y_test, 'values'):\n        y_test = y_test.values\n\n    if config is None:\n        config = VAEConfig()\n        config.INPUT_DIM = X_train.shape[1]\n\n    set_seed(config.SEED)\n\n    # Setup save directory\n    if save_dir is None:\n        timestamp = datetime.now().strftime('%Y%m%d')\n        save_dir = Path(f\"models/{timestamp}_VAE_KIRC\")\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save config\n    save_model_config(config, save_dir / 'config.json')\n\n    # Setup device\n    device = get_device(force_cpu=force_cpu)\n    logger.info(f\"Using device: {device}\")\n\n    # Initialize model\n    model = VAE(\n        input_dim=config.INPUT_DIM,\n        mid_dim=config.MID_DIM,\n        features=config.LATENT_DIM,\n    ).to(device)\n\n    logger.info(f\"Model: VAE(input_dim={config.INPUT_DIM}, mid_dim={config.MID_DIM}, latent_dim={config.LATENT_DIM})\")\n    logger.info(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    # Setup optimizer\n    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n\n    # Setup checkpointer\n    checkpointer = ModelCheckpointer(\n        save_dir=save_dir,\n        monitor='val_loss',\n        mode='min',\n        save_freq=config.CHECKPOINT_FREQ,\n        keep_last_n=3,\n    )\n\n    # Resume from checkpoint if provided\n    start_epoch = 0\n    if resume_from is not None:\n        checkpoint_info = checkpointer.load_checkpoint(\n            resume_from, model, optimizer, device=str(device)\n        )\n        start_epoch = checkpoint_info['epoch'] + 1\n        logger.info(f\"Resuming training from epoch {start_epoch}\")\n\n    # Create dataloaders\n    train_loader = create_dataloader(X_train, y_train, config.BATCH_SIZE, shuffle=True)\n    test_loader = create_dataloader(X_test, y_test, config.BATCH_SIZE, shuffle=False)\n\n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_recon_loss': [],\n        'train_kl_loss': [],\n        'val_recon_loss': [],\n        'val_kl_loss': [],\n        'beta_schedule': [],  # Track beta values\n    }\n\n    # Setup beta annealing schedule\n    if config.USE_BETA_ANNEALING:\n        beta_schedule = frange_cycle_linear(\n            start=config.BETA_START,\n            stop=config.BETA,\n            n_epoch=config.EPOCHS,\n            n_cycle=config.BETA_CYCLES,\n            ratio=config.BETA_RATIO\n        )\n        logger.info(\n            f\"Using cyclical beta annealing: \"\n            f\"{config.BETA_START} -&gt; {config.BETA} over {config.BETA_CYCLES} cycles\"\n        )\n    else:\n        # Constant beta\n        beta_schedule = np.ones(config.EPOCHS) * config.BETA\n        logger.info(f\"Using constant beta: {config.BETA}\")\n\n    # Training loop\n    logger.info(f\"Starting training for {config.EPOCHS} epochs\")\n\n    # Add epoch progress bar\n    epoch_pbar = tqdm(range(start_epoch, config.EPOCHS), desc='Epochs', position=0)\n    for epoch in epoch_pbar:\n        # Get beta for this epoch from schedule\n        current_beta = beta_schedule[epoch]\n\n        # Train\n        train_metrics = train_epoch(model, train_loader, optimizer, device, config, beta=current_beta)\n\n        # Validate\n        val_metrics = evaluate_model(model, test_loader, device, config, beta=current_beta)\n\n        # Update history\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['train_recon_loss'].append(train_metrics['recon_loss'])\n        history['train_kl_loss'].append(train_metrics['kl_loss'])\n        history['val_recon_loss'].append(val_metrics['recon_loss'])\n        history['val_kl_loss'].append(val_metrics['kl_loss'])\n        history['beta_schedule'].append(float(current_beta))\n\n        # Update epoch progress bar\n        epoch_pbar.set_postfix({\n            'train_loss': f\"{train_metrics['loss']:.4f}\",\n            'val_loss': f\"{val_metrics['loss']:.4f}\",\n            'beta': f\"{current_beta:.3f}\"\n        })\n\n        # Log progress\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            logger.info(\n                f\"Epoch {epoch+1}/{config.EPOCHS} - \"\n                f\"train_loss: {train_metrics['loss']:.4f}, \"\n                f\"val_loss: {val_metrics['loss']:.4f}\"\n            )\n\n        # Combine metrics for checkpointing\n        current_metrics = {\n            'train_loss': train_metrics['loss'],\n            'val_loss': val_metrics['loss'],\n            'train_recon': train_metrics['recon_loss'],\n            'train_kl': train_metrics['kl_loss'],\n            'val_recon': val_metrics['recon_loss'],\n            'val_kl': val_metrics['kl_loss'],\n        }\n\n        # # Save periodic checkpoint\n        # if checkpointer.should_save_checkpoint(epoch):\n        #     checkpointer.save_checkpoint(\n        #         epoch, model, optimizer, current_metrics, config\n        #     )\n\n    # Save final model\n    checkpointer.save_checkpoint(\n        config.EPOCHS - 1, model, optimizer, current_metrics, config, is_final=True\n    )\n\n    logger.info(\"Training complete!\")\n\n    return model, history\n</code></pre>"},{"location":"api/training/#key-features","title":"Key Features","text":""},{"location":"api/training/#automatic-checkpointing","title":"Automatic Checkpointing","text":"<p>The training function automatically saves:</p> <ul> <li>Model state dict</li> <li>Optimizer state</li> <li>Training history</li> <li>Configuration parameters</li> </ul> <p>Checkpoints are saved when: - Validation loss improves (best model) - At regular intervals (every <code>checkpoint_every</code> epochs) - After training completes (final model)</p>"},{"location":"api/training/#early-stopping","title":"Early Stopping","text":"<p>Training stops automatically if validation loss doesn't improve for <code>early_stopping_patience</code> epochs. This prevents overfitting and saves computation time.</p>"},{"location":"api/training/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>When <code>use_scheduler=True</code>, the learning rate is reduced when validation loss plateaus:</p> <pre><code>scheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=10,\n    verbose=True\n)\n</code></pre>"},{"location":"api/training/#cyclical-kl-annealing","title":"Cyclical KL Annealing","text":"<p>The KL divergence weight \u03b2 is gradually increased using cyclical annealing to prevent posterior collapse:</p>"},{"location":"api/training/#renalprog.modeling.train.frange_cycle_linear","title":"frange_cycle_linear","text":"<pre><code>frange_cycle_linear(\n    start: float,\n    stop: float,\n    n_epoch: int,\n    n_cycle: int = 4,\n    ratio: float = 0.5,\n) -&gt; np.ndarray\n</code></pre> <p>Generate a linear cyclical schedule for beta hyperparameter.</p> <p>This creates a cyclical annealing schedule where beta increases linearly from start to stop over a portion of each cycle (controlled by ratio), then stays constant at stop for the remainder of the cycle.</p> <p>Args:     start: Initial value of beta (typically 0.0)     stop: Final/maximum value of beta (typically 1.0)     n_epoch: Total number of epochs     n_cycle: Number of cycles (default: 4)     ratio: Ratio of cycle spent increasing beta (default: 0.5)            - 0.5 means half cycle increasing, half constant            - 1.0 means entire cycle increasing</p> <p>Returns:     Array of beta values for each epoch</p> <p>Example:     &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle     &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)     &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0     &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0     &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def frange_cycle_linear(\n    start: float,\n    stop: float,\n    n_epoch: int,\n    n_cycle: int = 4,\n    ratio: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a linear cyclical schedule for beta hyperparameter.\n\n    This creates a cyclical annealing schedule where beta increases linearly\n    from start to stop over a portion of each cycle (controlled by ratio),\n    then stays constant at stop for the remainder of the cycle.\n\n    Args:\n        start: Initial value of beta (typically 0.0)\n        stop: Final/maximum value of beta (typically 1.0)\n        n_epoch: Total number of epochs\n        n_cycle: Number of cycles (default: 4)\n        ratio: Ratio of cycle spent increasing beta (default: 0.5)\n               - 0.5 means half cycle increasing, half constant\n               - 1.0 means entire cycle increasing\n\n    Returns:\n        Array of beta values for each epoch\n\n    Example:\n        &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle\n        &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)\n        &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0\n    \"\"\"\n    L = np.ones(n_epoch) * stop  # Initialize all to stop value\n    period = n_epoch / n_cycle\n    step = (stop - start) / (period * ratio)  # Linear schedule\n\n    for c in range(n_cycle):\n        v, i = start, 0\n        while v &lt;= stop and (int(i + c * period) &lt; n_epoch):\n            L[int(i + c * period)] = v\n            v += step\n            i += 1\n\n    return L\n</code></pre>"},{"location":"api/training/#training-history","title":"Training History","text":"<p>The training function returns a dictionary with:</p> Key Description <code>train_loss</code> Training loss per epoch <code>val_loss</code> Validation loss per epoch <code>train_recon</code> Training reconstruction loss per epoch <code>val_recon</code> Validation reconstruction loss per epoch <code>train_kl</code> Training KL divergence per epoch <code>val_kl</code> Validation KL divergence per epoch <code>learning_rates</code> Learning rate per epoch"},{"location":"api/training/#complete-example","title":"Complete Example","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import train_vae\nfrom renalprog.plots import plot_training_history\nfrom renalprog.utils import set_seed, configure_logging\n\n# Configure\nconfigure_logging()\nset_seed(42)\n\n# Load data\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\n\n# Train VAE\nhistory, best_model, checkpoints = train_vae(\n    train_data=train_expr.values,\n    val_data=test_expr.values,\n    input_dim=train_expr.shape[1],\n    mid_dim=1024,\n    features=128,\n    output_dir=Path(\"models/my_vae\"),\n    n_epochs=100,\n    batch_size=32,\n    learning_rate=1e-3,\n    use_scheduler=True,\n    use_checkpoint=True,\n    checkpoint_every=10,\n    early_stopping_patience=20,\n    device='cuda'\n)\n\n# Plot results\nplot_training_history(\n    history,\n    output_path=Path(\"reports/figures/training_history.png\")\n)\n\n# Load best model for inference\nbest_model.eval()\nimport torch\nwith torch.no_grad():\n    reconstruction, mu, log_var, z = best_model(\n        torch.FloatTensor(test_expr.values).to(device)\n    )\n\nprint(f\"Best validation loss: {min(history['val_loss']):.4f}\")\nprint(f\"Final learning rate: {history['learning_rates'][-1]:.6f}\")\n</code></pre>"},{"location":"api/training/#checkpointing-api","title":"Checkpointing API","text":"<p>For manual checkpoint management:</p>"},{"location":"api/training/#renalprog.modeling.checkpointing","title":"checkpointing","text":"<p>Model checkpointing utilities for saving and loading training state.</p>"},{"location":"api/training/#renalprog.modeling.checkpointing-classes","title":"Classes","text":""},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer","title":"ModelCheckpointer","text":"<pre><code>ModelCheckpointer(\n    save_dir: Path,\n    monitor: str = \"val_loss\",\n    mode: str = \"min\",\n    save_freq: int = 0,\n    keep_last_n: int = 3,\n)\n</code></pre> <p>Handles saving and loading model checkpoints during training.</p> <p>Features: - Save best model based on validation metric - Save checkpoints every N epochs - Save final model after training - Save training history and configuration - Resume training from checkpoint</p> <p>Attributes:     save_dir: Directory to save checkpoints     monitor: Metric to monitor ('loss', 'val_loss', etc.)     mode: 'min' for loss, 'max' for accuracy     save_freq: Save checkpoint every N epochs (0 = only best)     keep_last_n: Keep only last N checkpoints (0 = keep all)</p> <p>Initialize checkpointer.</p> <p>Args:     save_dir: Directory to save checkpoints     monitor: Metric name to monitor     mode: 'min' to minimize metric, 'max' to maximize     save_freq: Save every N epochs (0 = only save best)     keep_last_n: Keep only N most recent checkpoints (0 = all)</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def __init__(\n    self,\n    save_dir: Path,\n    monitor: str = 'val_loss',\n    mode: str = 'min',\n    save_freq: int = 0,\n    keep_last_n: int = 3,\n):\n    \"\"\"Initialize checkpointer.\n\n    Args:\n        save_dir: Directory to save checkpoints\n        monitor: Metric name to monitor\n        mode: 'min' to minimize metric, 'max' to maximize\n        save_freq: Save every N epochs (0 = only save best)\n        keep_last_n: Keep only N most recent checkpoints (0 = all)\n    \"\"\"\n    self.save_dir = Path(save_dir)\n    self.save_dir.mkdir(parents=True, exist_ok=True)\n\n    self.monitor = monitor\n    self.mode = mode\n    self.save_freq = save_freq\n    self.keep_last_n = keep_last_n\n\n    # Track best metric\n    self.best_metric = float('inf') if mode == 'min' else float('-inf')\n    self.best_epoch = 0\n\n    # Track saved checkpoints for cleanup\n    self.checkpoint_history = []\n\n    logger.info(f\"ModelCheckpointer initialized: {save_dir}\")\n    logger.info(f\"Monitoring: {monitor} ({mode})\")\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer-functions","title":"Functions","text":""},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.get_best_checkpoint_path","title":"get_best_checkpoint_path","text":"<pre><code>get_best_checkpoint_path() -&gt; Optional[Path]\n</code></pre> <p>Get path to best model checkpoint.</p> <p>Returns:     Path to best model, or None if not saved yet</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def get_best_checkpoint_path(self) -&gt; Optional[Path]:\n    \"\"\"Get path to best model checkpoint.\n\n    Returns:\n        Path to best model, or None if not saved yet\n    \"\"\"\n    best_path = self.save_dir / 'best_model.pth'\n    return best_path if best_path.exists() else None\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.get_final_checkpoint_path","title":"get_final_checkpoint_path","text":"<pre><code>get_final_checkpoint_path() -&gt; Optional[Path]\n</code></pre> <p>Get path to final model checkpoint.</p> <p>Returns:     Path to final model, or None if not saved yet</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def get_final_checkpoint_path(self) -&gt; Optional[Path]:\n    \"\"\"Get path to final model checkpoint.\n\n    Returns:\n        Path to final model, or None if not saved yet\n    \"\"\"\n    final_path = self.save_dir / 'final_model.pth'\n    return final_path if final_path.exists() else None\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(\n    checkpoint_path: Path,\n    model: Module,\n    optimizer: Optional[Optimizer] = None,\n    device: str = \"cpu\",\n) -&gt; Dict[str, Any]\n</code></pre> <p>Load a checkpoint and restore model state.</p> <p>Args:     checkpoint_path: Path to checkpoint file     model: Model to load state into     optimizer: Optional optimizer to restore state     device: Device to map checkpoint to</p> <p>Returns:     Dictionary with checkpoint information (epoch, metrics, config)</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def load_checkpoint(\n    self,\n    checkpoint_path: Path,\n    model: nn.Module,\n    optimizer: Optional[Optimizer] = None,\n    device: str = 'cpu',\n) -&gt; Dict[str, Any]:\n    \"\"\"Load a checkpoint and restore model state.\n\n    Args:\n        checkpoint_path: Path to checkpoint file\n        model: Model to load state into\n        optimizer: Optional optimizer to restore state\n        device: Device to map checkpoint to\n\n    Returns:\n        Dictionary with checkpoint information (epoch, metrics, config)\n    \"\"\"\n    if not checkpoint_path.exists():\n        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n\n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    # Restore model state\n    model.load_state_dict(checkpoint['model_state_dict'])\n    logger.info(f\"Loaded model state from epoch {checkpoint['epoch']}\")\n\n    # Restore optimizer state if provided\n    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logger.info(\"Loaded optimizer state\")\n\n    # Return checkpoint info\n    return {\n        'epoch': checkpoint['epoch'],\n        'metrics': checkpoint.get('metrics', {}),\n        'config': checkpoint.get('config', {}),\n        'best_metric': checkpoint.get('best_metric', self.best_metric),\n    }\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    epoch: int,\n    model: Module,\n    optimizer: Optimizer,\n    metrics: Dict[str, float],\n    config: Any,\n    is_best: bool = False,\n    is_final: bool = False,\n) -&gt; None\n</code></pre> <p>Save a training checkpoint.</p> <p>Args:     epoch: Current epoch number     model: PyTorch model to save     optimizer: Optimizer state to save     metrics: Dictionary of current metrics     config: Training configuration object     is_best: Whether this is the best model so far     is_final: Whether this is the final model</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def save_checkpoint(\n    self,\n    epoch: int,\n    model: nn.Module,\n    optimizer: Optimizer,\n    metrics: Dict[str, float],\n    config: Any,\n    is_best: bool = False,\n    is_final: bool = False,\n) -&gt; None:\n    \"\"\"Save a training checkpoint.\n\n    Args:\n        epoch: Current epoch number\n        model: PyTorch model to save\n        optimizer: Optimizer state to save\n        metrics: Dictionary of current metrics\n        config: Training configuration object\n        is_best: Whether this is the best model so far\n        is_final: Whether this is the final model\n    \"\"\"\n    # Create checkpoint dictionary\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'metrics': metrics,\n        'config': self._config_to_dict(config),\n        'best_metric': self.best_metric,\n        'monitor': self.monitor,\n    }\n\n    # Determine checkpoint filename\n    if is_final:\n        filename = 'final_model.pth'\n    elif is_best:\n        filename = 'best_model.pth'\n    else:\n        filename = f'checkpoint_epoch_{epoch:04d}.pth'\n\n    checkpoint_path = self.save_dir / filename\n\n    # Save checkpoint\n    torch.save(checkpoint, checkpoint_path)\n    logger.info(f\"Saved checkpoint: {checkpoint_path}\")\n\n    # Track for cleanup\n    if not is_best and not is_final:\n        self.checkpoint_history.append(checkpoint_path)\n        self._cleanup_old_checkpoints()\n\n    # Save training history as JSON\n    if is_best or is_final:\n        self._save_history(metrics, epoch)\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.should_save_checkpoint","title":"should_save_checkpoint","text":"<pre><code>should_save_checkpoint(epoch: int) -&gt; bool\n</code></pre> <p>Determine if checkpoint should be saved this epoch.</p> <p>Args:     epoch: Current epoch number</p> <p>Returns:     True if checkpoint should be saved</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def should_save_checkpoint(self, epoch: int) -&gt; bool:\n    \"\"\"Determine if checkpoint should be saved this epoch.\n\n    Args:\n        epoch: Current epoch number\n\n    Returns:\n        True if checkpoint should be saved\n    \"\"\"\n    if self.save_freq == 0:\n        return False\n    return epoch % self.save_freq == 0\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.update_best","title":"update_best","text":"<pre><code>update_best(epoch: int, metric_value: float) -&gt; bool\n</code></pre> <p>Check if current metric is the best and update if so.</p> <p>Args:     epoch: Current epoch number     metric_value: Current metric value</p> <p>Returns:     True if this is a new best, False otherwise</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def update_best(self, epoch: int, metric_value: float) -&gt; bool:\n    \"\"\"Check if current metric is the best and update if so.\n\n    Args:\n        epoch: Current epoch number\n        metric_value: Current metric value\n\n    Returns:\n        True if this is a new best, False otherwise\n    \"\"\"\n    is_better = (\n        (self.mode == 'min' and metric_value &lt; self.best_metric) or\n        (self.mode == 'max' and metric_value &gt; self.best_metric)\n    )\n\n    if is_better:\n        self.best_metric = metric_value\n        self.best_epoch = epoch\n        logger.info(\n            f\"New best {self.monitor}: {metric_value:.6f} \"\n            f\"at epoch {epoch}\"\n        )\n        return True\n    return False\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing-functions","title":"Functions","text":""},{"location":"api/training/#renalprog.modeling.checkpointing.load_model_config","title":"load_model_config","text":"<pre><code>load_model_config(config_path: Path) -&gt; Dict[str, Any]\n</code></pre> <p>Load model configuration from JSON file.</p> <p>Args:     config_path: Path to JSON config file</p> <p>Returns:     Dictionary with configuration</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def load_model_config(config_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"Load model configuration from JSON file.\n\n    Args:\n        config_path: Path to JSON config file\n\n    Returns:\n        Dictionary with configuration\n    \"\"\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config not found: {config_path}\")\n\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n\n    logger.info(f\"Loaded config: {config_path}\")\n    return config\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.save_model_config","title":"save_model_config","text":"<pre><code>save_model_config(config: Any, save_path: Path) -&gt; None\n</code></pre> <p>Save model configuration to JSON file.</p> <p>Args:     config: Configuration object     save_path: Path to save JSON file</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def save_model_config(config: Any, save_path: Path) -&gt; None:\n    \"\"\"Save model configuration to JSON file.\n\n    Args:\n        config: Configuration object\n        save_path: Path to save JSON file\n    \"\"\"\n    config_dict = {}\n    if hasattr(config, '__dict__'):\n        config_dict = {\n            k: v for k, v in config.__dict__.items()\n            if not k.startswith('_') and not callable(v)\n        }\n\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(save_path, 'w') as f:\n        json.dump(config_dict, f, indent=2)\n\n    logger.info(f\"Saved config: {save_path}\")\n</code></pre>"},{"location":"api/training/#save_checkpoint","title":"save_checkpoint","text":"<p>Save model checkpoint with metadata.</p>"},{"location":"api/training/#renalprog.modeling.checkpointing.save_model_config","title":"save_model_config","text":"<pre><code>save_model_config(config: Any, save_path: Path) -&gt; None\n</code></pre> <p>Save model configuration to JSON file.</p> <p>Args:     config: Configuration object     save_path: Path to save JSON file</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def save_model_config(config: Any, save_path: Path) -&gt; None:\n    \"\"\"Save model configuration to JSON file.\n\n    Args:\n        config: Configuration object\n        save_path: Path to save JSON file\n    \"\"\"\n    config_dict = {}\n    if hasattr(config, '__dict__'):\n        config_dict = {\n            k: v for k, v in config.__dict__.items()\n            if not k.startswith('_') and not callable(v)\n        }\n\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(save_path, 'w') as f:\n        json.dump(config_dict, f, indent=2)\n\n    logger.info(f\"Saved config: {save_path}\")\n</code></pre>"},{"location":"api/training/#load_checkpoint","title":"load_checkpoint","text":"<p>Load model from checkpoint.</p>"},{"location":"api/training/#renalprog.modeling.checkpointing.load_model_config","title":"load_model_config","text":"<pre><code>load_model_config(config_path: Path) -&gt; Dict[str, Any]\n</code></pre> <p>Load model configuration from JSON file.</p> <p>Args:     config_path: Path to JSON config file</p> <p>Returns:     Dictionary with configuration</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def load_model_config(config_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"Load model configuration from JSON file.\n\n    Args:\n        config_path: Path to JSON config file\n\n    Returns:\n        Dictionary with configuration\n    \"\"\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config not found: {config_path}\")\n\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n\n    logger.info(f\"Loaded config: {config_path}\")\n    return config\n</code></pre>"},{"location":"api/training/#see-also","title":"See Also","text":"<ul> <li>Models API - VAE architectures</li> <li>Prediction API - Using trained models</li> <li>Configuration - Training hyperparameters</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/trajectories/","title":"Trajectories API","text":"<p>Functions for analyzing disease progression trajectories.</p>"},{"location":"api/trajectories/#overview","title":"Overview","text":"<p>The trajectories module provides analysis tools for:</p> <ul> <li>Trajectory network construction</li> <li>Patient connectivity analysis</li> <li>Temporal pathway enrichment</li> <li>Transition probability calculation</li> <li>Trajectory visualization</li> </ul>"},{"location":"api/trajectories/#trajectory-generation","title":"Trajectory Generation","text":""},{"location":"api/trajectories/#generate_trajectories","title":"generate_trajectories","text":"<p>Generate smooth disease progression trajectories.</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.generate_trajectories","title":"generate_trajectories","text":"<pre><code>generate_trajectories(\n    model: Module,\n    source_samples: DataFrame,\n    target_samples: DataFrame,\n    n_steps: int = 50,\n    method: str = \"linear\",\n    output_dir: Optional[Path] = None,\n    parallel: bool = False,\n    n_workers: Optional[int] = None,\n) -&gt; Dict[str, pd.DataFrame]\n</code></pre> <p>Generate synthetic trajectories between source and target patient samples.</p> <p>This function creates interpolated gene expression profiles in the latent space between pairs of patients at different cancer stages.</p> <p>Args:     model: Trained VAE model     source_samples: Source patient samples (early stage)     target_samples: Target patient samples (late stage)     n_steps: Number of interpolation steps     method: Interpolation method (\"linear\" or \"spherical\")     output_dir: Optional directory to save trajectories     parallel: Whether to use parallel processing     n_workers: Number of parallel workers (None = use all CPUs)</p> <p>Returns:     Dictionary mapping patient pairs to trajectory DataFrames</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def generate_trajectories(\n    model: torch.nn.Module,\n    source_samples: pd.DataFrame,\n    target_samples: pd.DataFrame,\n    n_steps: int = 50,\n    method: str = \"linear\",\n    output_dir: Optional[Path] = None,\n    parallel: bool = False,\n    n_workers: Optional[int] = None\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Generate synthetic trajectories between source and target patient samples.\n\n    This function creates interpolated gene expression profiles in the latent\n    space between pairs of patients at different cancer stages.\n\n    Args:\n        model: Trained VAE model\n        source_samples: Source patient samples (early stage)\n        target_samples: Target patient samples (late stage)\n        n_steps: Number of interpolation steps\n        method: Interpolation method (\"linear\" or \"spherical\")\n        output_dir: Optional directory to save trajectories\n        parallel: Whether to use parallel processing\n        n_workers: Number of parallel workers (None = use all CPUs)\n\n    Returns:\n        Dictionary mapping patient pairs to trajectory DataFrames\n    \"\"\"\n    logger.info(f\"Generating trajectories with {n_steps} steps using {method} interpolation\")\n\n    # TODO: Implement trajectory generation\n    # Migrate from src_deseq_and_gsea_NCSR/synthetic_data_generation.py\n\n    raise NotImplementedError(\n        \"generate_trajectories() needs implementation from \"\n        \"src_deseq_and_gsea_NCSR/synthetic_data_generation.py and \"\n        \"src/data/fun_interpol.py\"\n    )\n</code></pre>"},{"location":"api/trajectories/#network-construction","title":"Network Construction","text":""},{"location":"api/trajectories/#build_trajectory_network","title":"build_trajectory_network","text":"<p>Build directed graph of patient transitions.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import build_trajectory_network\nimport pandas as pd\nfrom pathlib import Path\n\n# Load patient connections\nconnections = pd.read_csv(\"data/processed/patient_connections.csv\")\n\n# Build network\nnetwork = build_trajectory_network(\n    connections=connections,\n    output_path=Path(\"data/processed/trajectory_network.graphml\")\n)\n\nprint(f\"Network has {network.number_of_nodes()} nodes\")\nprint(f\"Network has {network.number_of_edges()} edges\")\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.build_trajectory_network","title":"build_trajectory_network","text":"<pre><code>build_trajectory_network(\n    patient_links: DataFrame,\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]\n</code></pre> <p>Build trajectory network and find all complete disease progression paths.</p> <p>Constructs a directed graph from patient links and identifies all possible complete trajectories from root nodes (earliest stage patients not appearing as targets) to leaf nodes (latest stage patients not appearing as sources).</p> <p>Args:     patient_links: DataFrame with 'source' and 'target' columns from linking functions</p> <p>Returns:     Tuple of:     - network: Dict mapping each source patient to list of target patients     - trajectories: List of complete trajectories, where each trajectory is a                     list of patient IDs ordered from earliest to latest stage</p> <p>Network Structure:     - Adjacency list representation: {source: [target1, target2, ...]}     - Directed edges from earlier to later stages     - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)</p> <p>Trajectory Discovery:     - Uses depth-first search from root nodes     - Root nodes: Patients in 'source' but not in 'target' (stage I or early)     - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)     - Each trajectory represents a complete disease progression path</p> <p>Example:     &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)     &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")     &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")     &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")     Network has 500 nodes     Found 234 complete trajectories     Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']</p> <p>Trajectory Characteristics:     - Length varies based on how many stages the path spans     - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions     - Length 2 for early\u2192late progressions     - Patients can appear in multiple trajectories</p> <p>Note:     - Cycles are prevented during trajectory search     - All paths from root to leaf are enumerated     - Trajectories respect chronological disease progression</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def build_trajectory_network(\n    patient_links: pd.DataFrame\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]:\n    \"\"\"\n    Build trajectory network and find all complete disease progression paths.\n\n    Constructs a directed graph from patient links and identifies all possible\n    complete trajectories from root nodes (earliest stage patients not appearing\n    as targets) to leaf nodes (latest stage patients not appearing as sources).\n\n    Args:\n        patient_links: DataFrame with 'source' and 'target' columns from linking functions\n\n    Returns:\n        Tuple of:\n        - network: Dict mapping each source patient to list of target patients\n        - trajectories: List of complete trajectories, where each trajectory is a\n                        list of patient IDs ordered from earliest to latest stage\n\n    Network Structure:\n        - Adjacency list representation: {source: [target1, target2, ...]}\n        - Directed edges from earlier to later stages\n        - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)\n\n    Trajectory Discovery:\n        - Uses depth-first search from root nodes\n        - Root nodes: Patients in 'source' but not in 'target' (stage I or early)\n        - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)\n        - Each trajectory represents a complete disease progression path\n\n    Example:\n        &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)\n        &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")\n        &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")\n        &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")\n        Network has 500 nodes\n        Found 234 complete trajectories\n        Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']\n\n    Trajectory Characteristics:\n        - Length varies based on how many stages the path spans\n        - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions\n        - Length 2 for early\u2192late progressions\n        - Patients can appear in multiple trajectories\n\n    Note:\n        - Cycles are prevented during trajectory search\n        - All paths from root to leaf are enumerated\n        - Trajectories respect chronological disease progression\n    \"\"\"\n    logger.info(\"Building trajectory network from patient links\")\n\n    sources = patient_links['source']\n    targets = patient_links['target']\n\n    # Build network adjacency list\n    network = {}\n    for source, target in zip(sources, targets):\n        if source not in network:\n            network[source] = []\n        network[source].append(target)\n\n    logger.info(f\"Network built: {len(network)} source nodes\")\n\n    # Find root nodes (patients who are sources but never targets)\n    unique_sources = set(sources) - set(targets)\n    logger.info(f\"Found {len(unique_sources)} root nodes (earliest stage patients)\")\n\n    # Recursively find all trajectories from each root\n    def find_trajectories(start_node: str, visited: Optional[List[str]] = None) -&gt; List[List[str]]:\n        \"\"\"Depth-first search to find all paths from start_node to leaf nodes.\"\"\"\n        if visited is None:\n            visited = []\n\n        visited.append(start_node)\n\n        # If node has no outgoing edges, this is a leaf node - return path\n        if start_node not in network:\n            return [visited]\n\n        # Recursively explore all targets\n        trajectories = []\n        for target in network[start_node]:\n            if target not in visited:  # Avoid cycles\n                new_visited = visited.copy()\n                trajectories.extend(find_trajectories(target, new_visited))\n\n        return trajectories\n\n    # Find all trajectories starting from each root\n    all_trajectories = []\n\n    if len(unique_sources) == 0:\n        # No clear root nodes - this happens with early\u2192late transitions where\n        # patients can be both sources and targets. In this case, each source\u2192target\n        # pair is already a complete 2-patient trajectory.\n        logger.info(\"No root nodes found (typical for early\u2192late transitions).\")\n        logger.info(\"Using each source\u2192target pair as a complete trajectory.\")\n        for source, target in zip(sources, targets):\n            all_trajectories.append([source, target])\n    else:\n        # Standard case: multi-stage progressions (I\u2192II\u2192III\u2192IV)\n        for source in unique_sources:\n            all_trajectories.extend(find_trajectories(source))\n\n    logger.info(f\"Discovered {len(all_trajectories)} complete disease progression trajectories\")\n\n    # Log trajectory length statistics only if we have trajectories\n    if len(all_trajectories) &gt; 0:\n        traj_lengths = [len(t) for t in all_trajectories]\n        logger.info(f\"Trajectory lengths - Min: {min(traj_lengths)}, Max: {max(traj_lengths)}, \"\n                    f\"Mean: {np.mean(traj_lengths):.1f}\")\n    else:\n        logger.warning(\"No trajectories found!\")\n\n    return network, all_trajectories\n</code></pre>"},{"location":"api/trajectories/#generate_trajectory_data","title":"generate_trajectory_data","text":"<p>Generate complete trajectory dataset with metadata.</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.generate_trajectory_data","title":"generate_trajectory_data","text":"<pre><code>generate_trajectory_data(\n    vae_model: Module,\n    recnet_model: Optional[Module],\n    trajectory: List[str],\n    gene_data: DataFrame,\n    n_timepoints: int = 50,\n    interpolation_method: str = \"linear\",\n    device: str = \"cpu\",\n    save_path: Optional[Path] = None,\n    scaler: Optional[MinMaxScaler] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Generate synthetic gene expression data along a patient trajectory.</p> <p>Creates N interpolated time points between consecutive patients in a trajectory by performing interpolation in the VAE latent space, then decoding back to gene expression space. Optionally applies reconstruction network for refinement.</p> <p>Args:     vae_model: Trained VAE model for encoding/decoding     recnet_model: Optional reconstruction network for refining VAE output     trajectory: List of patient IDs in chronological progression order     gene_data: Gene expression DataFrame (genes \u00d7 patients)     n_timepoints: Number of interpolation points between each patient pair     interpolation_method: 'linear' or 'spherical' interpolation in latent space     device: Torch device for computation     save_path: Optional path to save trajectory CSV file     scaler: Pre-fitted MinMaxScaler from VAE training. If None, will fit on gene_data.</p> <p>Returns:     DataFrame with synthetic gene expression profiles for all time points.     Shape: (n_timepoints * (len(trajectory)-1), n_genes)     Index contains time point identifiers</p> <p>Workflow:     1. Extract gene expression for each patient in trajectory     2. Normalize using the SAME scaler used during VAE training     3. Encode each patient to VAE latent space     4. For each consecutive pair:        a. Interpolate in latent space (linear or spherical)        b. Decode interpolated points back to gene space        c. Optionally apply reconstruction network     5. Concatenate all segments into complete trajectory</p> <p>Interpolation Methods:     linear: Straight-line interpolation in latent space             z(t) = (1-t)*z_source + t*z_target</p> <pre><code>spherical: Spherical linear interpolation (SLERP)\n           Preserves magnitude, interpolates on hypersphere\n           Recommended for normalized latent spaces\n</code></pre> <p>Note:     CRITICAL: The scaler must be the same one used during VAE training.     Using a different scaler will produce incorrect latent representations.     If scaler=None, will fit on all gene_data (all patients), which approximates     the training distribution.</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def generate_trajectory_data(\n    vae_model: torch.nn.Module,\n    recnet_model: Optional[torch.nn.Module],\n    trajectory: List[str],\n    gene_data: pd.DataFrame,\n    n_timepoints: int = 50,\n    interpolation_method: str = 'linear',\n    device: str = 'cpu',\n    save_path: Optional[Path] = None,\n    scaler: Optional[MinMaxScaler] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate synthetic gene expression data along a patient trajectory.\n\n    Creates N interpolated time points between consecutive patients in a trajectory\n    by performing interpolation in the VAE latent space, then decoding back to\n    gene expression space. Optionally applies reconstruction network for refinement.\n\n    Args:\n        vae_model: Trained VAE model for encoding/decoding\n        recnet_model: Optional reconstruction network for refining VAE output\n        trajectory: List of patient IDs in chronological progression order\n        gene_data: Gene expression DataFrame (genes \u00d7 patients)\n        n_timepoints: Number of interpolation points between each patient pair\n        interpolation_method: 'linear' or 'spherical' interpolation in latent space\n        device: Torch device for computation\n        save_path: Optional path to save trajectory CSV file\n        scaler: Pre-fitted MinMaxScaler from VAE training. If None, will fit on gene_data.\n\n    Returns:\n        DataFrame with synthetic gene expression profiles for all time points.\n        Shape: (n_timepoints * (len(trajectory)-1), n_genes)\n        Index contains time point identifiers\n\n    Workflow:\n        1. Extract gene expression for each patient in trajectory\n        2. Normalize using the SAME scaler used during VAE training\n        3. Encode each patient to VAE latent space\n        4. For each consecutive pair:\n           a. Interpolate in latent space (linear or spherical)\n           b. Decode interpolated points back to gene space\n           c. Optionally apply reconstruction network\n        5. Concatenate all segments into complete trajectory\n\n    Interpolation Methods:\n        linear: Straight-line interpolation in latent space\n                z(t) = (1-t)*z_source + t*z_target\n\n        spherical: Spherical linear interpolation (SLERP)\n                   Preserves magnitude, interpolates on hypersphere\n                   Recommended for normalized latent spaces\n\n    Note:\n        CRITICAL: The scaler must be the same one used during VAE training.\n        Using a different scaler will produce incorrect latent representations.\n        If scaler=None, will fit on all gene_data (all patients), which approximates\n        the training distribution.\n    \"\"\"\n\n    logger.info(f\"Generating trajectory data for {len(trajectory)} patients\")\n    logger.info(f\"Interpolation: {n_timepoints} points \u00d7 {len(trajectory)-1} segments\")\n    logger.info(f\"Method: {interpolation_method}\")\n\n    # Set models to evaluation mode\n    vae_model.eval()\n    if recnet_model is not None:\n        recnet_model.eval()\n\n    vae_model = vae_model.to(device)\n    if recnet_model is not None:\n        recnet_model = recnet_model.to(device)\n\n    # Use provided scaler or fit new one on all gene data\n    if scaler is None:\n        logger.warning(\"No scaler provided - fitting new scaler on all gene data\")\n        logger.warning(\"This may not match VAE training normalization!\")\n        scaler = MinMaxScaler()\n        # gene_data is (genes \u00d7 patients), need (patients \u00d7 genes) for scaler\n        scaler.fit(gene_data.T.values)\n        logger.info(f\"Fitted scaler on {gene_data.shape[1]} patients\")\n    else:\n        logger.info(\"Using provided scaler from VAE training\")\n\n    # Select interpolation function\n    if interpolation_method == 'linear':\n        interp_func = interpolate_latent_linear\n    elif interpolation_method == 'spherical':\n        interp_func = interpolate_latent_spherical\n    else:\n        raise ValueError(f\"Unknown interpolation method: {interpolation_method}\")\n\n    # Generate synthetic data for each segment of the trajectory\n    all_segments = []\n\n    with torch.no_grad():\n        for i in range(len(trajectory) - 1):\n            source_patient = trajectory[i]\n            target_patient = trajectory[i + 1]\n\n            logger.info(f\"Segment {i+1}/{len(trajectory)-1}: {source_patient} \u2192 {target_patient}\")\n\n            # Get gene expression for source and target\n            # gene_data is (genes \u00d7 patients), so gene_data[patient] is a Series of gene values\n            source_expr = gene_data[source_patient].values.reshape(1, -1)  # (1, genes)\n            target_expr = gene_data[target_patient].values.reshape(1, -1)  # (1, genes)\n\n            # Normalize data using the provided scaler\n            # Scaler expects (n_samples, n_features) = (1, genes)\n            source_norm = scaler.transform(source_expr)  # (1, genes)\n            target_norm = scaler.transform(target_expr)  # (1, genes)\n\n            # Encode to latent space\n            source_tensor = torch.tensor(source_norm, dtype=torch.float32).to(device)\n            target_tensor = torch.tensor(target_norm, dtype=torch.float32).to(device)\n\n            _, _, _, z_source = vae_model(source_tensor)\n            _, _, _, z_target = vae_model(target_tensor)\n\n            # Interpolate in latent space\n            z_source_np = z_source.cpu().numpy().flatten()\n            z_target_np = z_target.cpu().numpy().flatten()\n\n            interpolated_z = interp_func(z_source_np, z_target_np, n_timepoints)\n\n            # Decode interpolated latent vectors\n            interpolated_z_tensor = torch.tensor(interpolated_z, dtype=torch.float32).to(device)\n            decoded = vae_model.decoder(interpolated_z_tensor)\n\n            # Denormalize using the same scaler\n            # decoded is (n_timepoints, genes), scaler expects (n_samples, n_features)\n            decoded_np = decoded.cpu().numpy()  # (n_timepoints, genes)\n            segment_data = scaler.inverse_transform(decoded_np)  # (n_timepoints, genes) - REAL SPACE\n\n            # Apply reconstruction network if provided\n            # CRITICAL: RecNet works on REAL SPACE data, not normalized!\n            if recnet_model is not None:\n                # Convert to tensor and apply RecNet directly to real space data\n                segment_tensor = torch.tensor(segment_data, dtype=torch.float32).to(device)\n                refined = recnet_model(segment_tensor)\n                segment_data = refined.cpu().numpy()  # (n_timepoints, genes) - REAL SPACE\n\n            all_segments.append(segment_data)\n\n    # Concatenate all segments\n    trajectory_data = np.vstack(all_segments)\n\n    # Create DataFrame\n    trajectory_df = pd.DataFrame(\n        trajectory_data,\n        columns=gene_data.index\n    )\n\n    # Create informative index\n    time_indices = []\n    for i in range(len(trajectory) - 1):\n        for t in range(n_timepoints):\n            time_indices.append(f\"{trajectory[i]}_to_{trajectory[i+1]}_t{t:03d}\")\n    trajectory_df.index = time_indices\n\n    logger.info(f\"Generated trajectory data: {trajectory_df.shape}\")\n\n    # Save if path provided\n    if save_path is not None:\n        save_path = Path(save_path)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        trajectory_df.to_csv(save_path)\n        logger.info(f\"Saved trajectory to: {save_path}\")\n\n    return trajectory_df\n</code></pre>"},{"location":"api/trajectories/#patient-connectivity","title":"Patient Connectivity","text":""},{"location":"api/trajectories/#create_patient_connections","title":"create_patient_connections","text":"<p>Create optimal patient pairings for trajectories.</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.create_patient_connections","title":"create_patient_connections","text":"<pre><code>create_patient_connections(\n    data: DataFrame,\n    clinical: Series,\n    method: str = \"random\",\n    transition_type: str = \"early_to_late\",\n    n_connections: Optional[int] = None,\n    seed: int = 2023,\n) -&gt; pd.DataFrame\n</code></pre> <p>Create connections between patients for trajectory generation.</p> <p>Args:     data: Gene expression data     clinical: Clinical stage information     method: Method for creating connections (\"random\", \"nearest\", \"all\")     transition_type: Type of transition (\"early_to_late\", \"early_to_early\", etc.)     n_connections: Number of connections to create (None = all possible)     seed: Random seed</p> <p>Returns:     DataFrame with columns: source, target, transition</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def create_patient_connections(\n    data: pd.DataFrame,\n    clinical: pd.Series,\n    method: str = \"random\",\n    transition_type: str = \"early_to_late\",\n    n_connections: Optional[int] = None,\n    seed: int = 2023\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create connections between patients for trajectory generation.\n\n    Args:\n        data: Gene expression data\n        clinical: Clinical stage information\n        method: Method for creating connections (\"random\", \"nearest\", \"all\")\n        transition_type: Type of transition (\"early_to_late\", \"early_to_early\", etc.)\n        n_connections: Number of connections to create (None = all possible)\n        seed: Random seed\n\n    Returns:\n        DataFrame with columns: source, target, transition\n    \"\"\"\n    logger.info(f\"Creating patient connections: {transition_type} using {method} method\")\n\n    # TODO: Implement connection logic\n    # Migrate from notebooks/4_1_trajectories.ipynb\n\n    raise NotImplementedError(\n        \"create_patient_connections() needs implementation from \"\n        \"notebooks/4_1_trajectories.ipynb\"\n    )\n</code></pre>"},{"location":"api/trajectories/#link_patients_closest","title":"link_patients_closest","text":"<p>Link patients using closest latent space neighbors.</p> <p>Example:</p> <pre><code>from renalprog.modeling.predict import link_patients_closest\nimport numpy as np\n\nearly_latent = np.random.randn(100, 128)\nlate_latent = np.random.randn(80, 128)\n\nconnections = link_patients_closest(\n    latent_early=early_latent,\n    latent_late=late_latent,\n    patient_ids_early=['E001', 'E002', ...],\n    patient_ids_late=['L001', 'L002', ...]\n)\n\n# Returns DataFrame with columns: early_patient, late_patient, distance\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.link_patients_closest","title":"link_patients_closest","text":"<pre><code>link_patients_closest(\n    transitions_df: DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients by selecting closest (or farthest) matches across stages.</p> <p>For each patient at a source stage, this function identifies the closest (or farthest) patient at the target stage, considering metadata constraints (gender, race). This creates one-to-one patient linkages that form the basis for trajectory construction.</p> <p>Args:     transitions_df: DataFrame from calculate_all_possible_transitions()                     containing all possible patient pairs with distances     start_with_first_stage: If True, build forward trajectories (early\u2192late)                             If False, build backward trajectories (late\u2192early)     early_late: If True, uses early/late groupings. If False, uses I-IV stages     closest: If True, connect closest patients. If False, connect farthest patients</p> <p>Returns:     DataFrame with selected patient links, containing one row per source patient     with their optimal target patient match. Includes all columns from transitions_df.</p> <p>Selection Strategy:     - Forward (start_with_first_stage=True): For each source, find optimal target     - Backward (start_with_first_stage=False): For each target, find optimal source     - Closest (closest=True): Minimum distance match     - Farthest (closest=False): Maximum distance match</p> <p>Metadata Stratification:     Links are selected independently within each combination of:     - Gender (MALE, FEMALE)     - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)     This ensures demographic consistency in trajectories.</p> <p>Example:     &gt;&gt;&gt; links = link_patients_closest(     ...     transitions_df=all_transitions,     ...     start_with_first_stage=True,     ...     closest=True     ... )     &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")     Created 234 patient links</p> <p>Note:     - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV     - Processes in reverse for backward: IV\u2192III\u2192II\u2192I     - Each patient appears at most once as a source in the result</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_closest(\n    transitions_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients by selecting closest (or farthest) matches across stages.\n\n    For each patient at a source stage, this function identifies the closest\n    (or farthest) patient at the target stage, considering metadata constraints\n    (gender, race). This creates one-to-one patient linkages that form the basis\n    for trajectory construction.\n\n    Args:\n        transitions_df: DataFrame from calculate_all_possible_transitions()\n                        containing all possible patient pairs with distances\n        start_with_first_stage: If True, build forward trajectories (early\u2192late)\n                                If False, build backward trajectories (late\u2192early)\n        early_late: If True, uses early/late groupings. If False, uses I-IV stages\n        closest: If True, connect closest patients. If False, connect farthest patients\n\n    Returns:\n        DataFrame with selected patient links, containing one row per source patient\n        with their optimal target patient match. Includes all columns from transitions_df.\n\n    Selection Strategy:\n        - Forward (start_with_first_stage=True): For each source, find optimal target\n        - Backward (start_with_first_stage=False): For each target, find optimal source\n        - Closest (closest=True): Minimum distance match\n        - Farthest (closest=False): Maximum distance match\n\n    Metadata Stratification:\n        Links are selected independently within each combination of:\n        - Gender (MALE, FEMALE)\n        - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)\n        This ensures demographic consistency in trajectories.\n\n    Example:\n        &gt;&gt;&gt; links = link_patients_closest(\n        ...     transitions_df=all_transitions,\n        ...     start_with_first_stage=True,\n        ...     closest=True\n        ... )\n        &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")\n        Created 234 patient links\n\n    Note:\n        - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV\n        - Processes in reverse for backward: IV\u2192III\u2192II\u2192I\n        - Each patient appears at most once as a source in the result\n    \"\"\"\n    logger.info(\"Linking patients by closest/farthest matches\")\n    logger.info(f\"Direction: {'Forward' if start_with_first_stage else 'Backward'}\")\n    logger.info(f\"Strategy: {'Closest' if closest else 'Farthest'}\")\n\n    # Define transition order based on direction\n    if start_with_first_stage and not early_late:\n        transitions_possible = ['1_to_2', '2_to_3', '3_to_4']\n    elif not start_with_first_stage and not early_late:\n        transitions_possible = ['3_to_4', '2_to_3', '1_to_2']\n    elif early_late:\n        transitions_possible = ['early_to_late']\n\n    # 0 for closest (smallest distance), -1 for farthest (largest distance)\n    idx = 0 if closest else -1\n\n    # Find closest/farthest patient for each source patient\n    closest_list = []\n    for transition_i in transitions_possible:\n        transition_df_i = transitions_df[transitions_df['transition'] == transition_i]\n\n        logger.info(f\"Processing transition {transition_i}: {len(transition_df_i)} pairs\")\n\n        # Iterate through all metadata combinations\n        for gender_i in ['FEMALE', 'MALE']:\n            df_gender_i = transition_df_i.query(f\"source_gender == '{gender_i}'\")\n\n            for race_i in ['ASIAN', 'BLACK OR AFRICAN AMERICAN', 'WHITE']:\n                df_race_i = df_gender_i.query(f\"source_race == '{race_i}'\")\n\n                if df_race_i.empty:\n                    continue\n\n                # Get unique patients to link\n                unique_sources = df_race_i['source'].unique()\n                unique_targets = df_race_i['target'].unique()\n                use_uniques = unique_sources if start_with_first_stage else unique_targets\n                use_column = 'source' if start_with_first_stage else 'target'\n\n                # Find closest/farthest match for each patient\n                for pat_i in use_uniques:\n                    pat_matches = df_race_i[df_race_i[use_column] == pat_i]\n                    if len(pat_matches) &gt; 0:\n                        # Sort by distance and select first (closest) or last (farthest)\n                        best_match = pat_matches.sort_values('distance').iloc[idx]\n                        closest_list.append(best_match)\n\n    # Convert to DataFrame\n    closest_df = pd.DataFrame(closest_list)\n    closest_df.reset_index(drop=True, inplace=True)\n\n    logger.info(f\"Created {len(closest_df)} patient links\")\n\n    return closest_df\n</code></pre>"},{"location":"api/trajectories/#link_patients_random","title":"link_patients_random","text":"<p>Link patients randomly (control method).</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.link_patients_random","title":"link_patients_random","text":"<pre><code>link_patients_random(\n    results_df: DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients to multiple random targets at the next stage.</p> <p>Instead of linking each patient to only their closest match, this function randomly samples multiple patients at the next stage to link to each source patient. This creates a one-to-many mapping useful for generating multiple trajectory samples.</p> <p>Parameters:</p> Name Type Description Default <code>results_df</code> <code>DataFrame</code> <p>DataFrame with possible sources and targets, their metadata, and distance.</p> required <code>start_with_first_stage</code> <code>bool</code> <p>If True, initiate trajectories with first stage as sources. If False, initiate trajectories with last stage as sources.</p> <code>True</code> <code>link_next</code> <code>int</code> <p>Number of patients at next stage to randomly link to each patient of current stage.</p> <code>5</code> <code>transitions_possible</code> <code>list</code> <p>List of transitions to process (e.g., ['1_to_2', '2_to_3']). If None, defaults to ['early_to_late'].</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with randomly sampled patient links for each transition. Contains multiple rows per source patient (up to link_next).</p> Notes <ul> <li>Random sampling is primarily performed for WHITE race patients due to sample size</li> <li>If fewer than link_next targets are available, all available targets are selected</li> <li>Patients from other races are included with all their possible connections</li> <li>Empty DataFrame is returned if no WHITE patients are found</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_random(\n    results_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients to multiple random targets at the next stage.\n\n    Instead of linking each patient to only their closest match, this function randomly\n    samples multiple patients at the next stage to link to each source patient. This\n    creates a one-to-many mapping useful for generating multiple trajectory samples.\n\n    Parameters\n    ----------\n    results_df : pd.DataFrame\n        DataFrame with possible sources and targets, their metadata, and distance.\n    start_with_first_stage : bool, default=True\n        If True, initiate trajectories with first stage as sources.\n        If False, initiate trajectories with last stage as sources.\n    link_next : int, default=5\n        Number of patients at next stage to randomly link to each patient of current stage.\n    transitions_possible : list, optional\n        List of transitions to process (e.g., ['1_to_2', '2_to_3']).\n        If None, defaults to ['early_to_late'].\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with randomly sampled patient links for each transition.\n        Contains multiple rows per source patient (up to link_next).\n\n    Notes\n    -----\n    - Random sampling is primarily performed for WHITE race patients due to sample size\n    - If fewer than link_next targets are available, all available targets are selected\n    - Patients from other races are included with all their possible connections\n    - Empty DataFrame is returned if no WHITE patients are found\n    \"\"\"\n    # Set default transitions if not provided\n    if transitions_possible is None:\n        transitions_possible = ['early_to_late']\n\n    # Get unique genders and races\n    unique_genders = results_df['source_gender'].unique().tolist()\n    # Get unique races\n    unique_races = results_df['source_race'].unique().tolist()\n    if 'WHITE' in unique_races:\n        unique_races.remove('WHITE')\n    # transition:\n    samples = []\n    for transition_i in transitions_possible:\n        transition_df_i = results_df[results_df['transition'] == transition_i]\n        for gender_i in unique_genders:\n            df_samples_i = transition_df_i.query(\n                f\"source_gender == '{gender_i}' &amp; source_race == 'WHITE'\")  # we can only do this for the whites since these are the only ones with enough samples\n            if df_samples_i.empty:\n                print(f\"Warning: No WHITE patients found for gender {gender_i} in transition {transition_i}\")\n                continue\n            unique_sources_i = np.unique(df_samples_i['source']).tolist()\n            unique_targets_i = np.unique(df_samples_i['target']).tolist()\n            use_uniques = unique_sources_i if start_with_first_stage else unique_targets_i\n            use_source_target = 'source' if start_with_first_stage else 'target'\n            for pat_i in use_uniques:\n                sample_i = df_samples_i.loc[df_samples_i[use_source_target] == pat_i]\n                if len(sample_i) &gt;= link_next:\n                    sample_i = sample_i.sample(\n                        link_next)  # Sample a number of patients at next stage to link to each patient of current stage\n                else:\n                    sample_i = sample_i.sample(len(sample_i))  # Sample all available patients if less than link_next\n                samples.append(sample_i)\n\n    # Check if samples list is empty\n    if not samples:\n        print(\"Warning: No samples found for WHITE race. Returning empty DataFrame.\")\n        return pd.DataFrame(columns=results_df.columns)\n\n    # Turn samples into dataframe:\n    samples_df = pd.concat(samples)\n    # Add the rest of the races\n    if unique_races:\n        samples_df = pd.concat(\n            [\n                samples_df,\n                results_df[results_df['source_race'].isin(unique_races)]\n            ]\n        )\n    samples_df.reset_index(drop=True, inplace=True)\n    return samples_df\n</code></pre>"},{"location":"api/trajectories/#transition-analysis","title":"Transition Analysis","text":""},{"location":"api/trajectories/#calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<p>Calculate metrics for all possible patient transitions.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import calculate_all_possible_transitions\n\n# Calculate all transitions\ntransitions = calculate_all_possible_transitions(\n    latent_early=early_latent,\n    latent_late=late_latent,\n    patient_ids_early=early_ids,\n    patient_ids_late=late_ids,\n    output_dir=Path(\"data/processed/transitions\")\n)\n\n# Analyze transition patterns\nprint(transitions.describe())\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<pre><code>calculate_all_possible_transitions(\n    data: DataFrame,\n    metadata_selection: DataFrame,\n    distance: str = \"wasserstein\",\n    early_late: bool = False,\n    negative_trajectory: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Calculate all possible patient-to-patient transitions for KIRC cancer.</p> <p>This function computes pairwise distances between all patients at consecutive (or same) cancer stages, considering metadata constraints. Only patients with matching gender and race are considered as potential trajectory pairs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Gene expression data with patients as columns.</p> required <code>metadata_selection</code> <code>DataFrame</code> <p>Clinical metadata with columns: histological_type, race, gender, stage.</p> required <code>distance</code> <code>(wasserstein, euclidean)</code> <p>Distance metric to use for calculating patient-to-patient distances.</p> <code>'wasserstein'</code> <code>early_late</code> <code>bool</code> <p>If True, uses early/late stage groupings. If False, uses I-IV stages.</p> <code>False</code> <code>negative_trajectory</code> <code>bool</code> <p>If True, generates same-stage transitions (negative controls). If False, generates progression transitions (positive trajectories).</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing all possible transitions with columns: - source, target: Patient IDs - source_gender, target_gender: Gender - source_race, target_race: Race - transition: Stage transition label (e.g., '1_to_2', 'early_to_late') - distance: Calculated distance between patients</p> <p>Sorted by gender, race, transition, and distance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance metric is not 'wasserstein' or 'euclidean'.</p> Notes <ul> <li>For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late</li> <li>For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late</li> <li>Only patients with identical gender and race are paired</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def calculate_all_possible_transitions(\n    data: pd.DataFrame,\n    metadata_selection: pd.DataFrame,\n    distance: str = 'wasserstein',\n    early_late: bool = False,\n    negative_trajectory: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate all possible patient-to-patient transitions for KIRC cancer.\n\n    This function computes pairwise distances between all patients at consecutive\n    (or same) cancer stages, considering metadata constraints. Only patients with\n    matching gender and race are considered as potential trajectory pairs.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Gene expression data with patients as columns.\n    metadata_selection : pd.DataFrame\n        Clinical metadata with columns: histological_type, race, gender, stage.\n    distance : {'wasserstein', 'euclidean'}, default='wasserstein'\n        Distance metric to use for calculating patient-to-patient distances.\n    early_late : bool, default=False\n        If True, uses early/late stage groupings. If False, uses I-IV stages.\n    negative_trajectory : bool, default=False\n        If True, generates same-stage transitions (negative controls).\n        If False, generates progression transitions (positive trajectories).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing all possible transitions with columns:\n        - source, target: Patient IDs\n        - source_gender, target_gender: Gender\n        - source_race, target_race: Race\n        - transition: Stage transition label (e.g., '1_to_2', 'early_to_late')\n        - distance: Calculated distance between patients\n\n        Sorted by gender, race, transition, and distance.\n\n    Raises\n    ------\n    ValueError\n        If distance metric is not 'wasserstein' or 'euclidean'.\n\n    Notes\n    -----\n    - For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late\n    - For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late\n    - Only patients with identical gender and race are paired\n    \"\"\"\n    # Select distance function\n    if distance == 'wasserstein':\n        from scipy.stats import wasserstein_distance\n        distance_fun = wasserstein_distance\n    elif distance == 'euclidean':\n        from scipy.spatial.distance import euclidean\n        distance_fun = euclidean\n    else:\n        raise ValueError('Distance function not implemented. Use either \"wasserstein\" or \"euclidean\".')\n\n    # Define stage transitions based on parameters\n    if early_late and not negative_trajectory:\n        possible_transitions = ['early_to_late']\n        stage_pairs = [['early', 'late']]\n    elif early_late and negative_trajectory:\n        possible_transitions = ['early_to_early', 'late_to_late']\n        stage_pairs = [['early', 'early'], ['late', 'late']]\n    elif not early_late and not negative_trajectory:\n        possible_transitions = ['1_to_2', '2_to_3', '3_to_4']\n        stage_pairs = [['I', 'II'], ['II', 'III'], ['III', 'IV']]\n    elif not early_late and negative_trajectory:\n        possible_transitions = ['1_to_1', '2_to_2', '3_to_3', '4_to_4']\n        stage_pairs = [['I', 'I'], ['II', 'II'], ['III', 'III'], ['IV', 'IV']]\n\n    # Calculate all possible transitions\n    results = []\n    for i_tr, transition in enumerate(possible_transitions):\n        source_target_stage = stage_pairs[i_tr]\n\n        # Iterate through all patient pairs at specified stages\n        for pat_i in metadata_selection.index[metadata_selection['stage'] == source_target_stage[0]]:\n            for pat_ii in metadata_selection.index[metadata_selection['stage'] == source_target_stage[1]]:\n                # Extract metadata for both patients\n                source_gender = metadata_selection.at[pat_i, 'gender']\n                target_gender = metadata_selection.at[pat_ii, 'gender']\n                source_race = metadata_selection.at[pat_i, 'race']\n                target_race = metadata_selection.at[pat_ii, 'race']\n\n                # Skip if metadata doesn't match (gender and race must match)\n                if not (source_race == target_race and source_gender == target_gender):\n                    continue\n\n                # Store transition information\n                results_i = {\n                    'source': pat_i,\n                    'target': pat_ii,\n                    'source_gender': source_gender,\n                    'target_gender': target_gender,\n                    'source_race': source_race,\n                    'target_race': target_race,\n                    'transition': transition,\n                    'distance': distance_fun(data[pat_i], data[pat_ii]),\n                }\n                results.append(results_i)\n\n    # Convert to DataFrame and sort\n    results_df = pd.DataFrame(results)\n    results_df.sort_values(\n        ['source_gender', 'target_gender', 'source_race', 'target_race',\n         'transition', 'distance'],\n        inplace=True,\n        ignore_index=True\n    )\n    return results_df\n</code></pre>"},{"location":"api/trajectories/#dynamic-enrichment","title":"Dynamic Enrichment","text":""},{"location":"api/trajectories/#dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<p>Perform pathway enrichment at each trajectory timepoint.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import dynamic_enrichment_analysis\nfrom pathlib import Path\n\n# Analyze pathway dynamics along trajectories\nenrichment_results = dynamic_enrichment_analysis(\n    trajectories=trajectory_gene_expression,  # Shape: (n_traj, n_steps, n_genes)\n    gene_names=gene_list,\n    pathway_file=Path(\"data/external/ReactomePathways.gmt\"),\n    output_dir=Path(\"reports/dynamic_enrichment\")\n)\n\n# Results contain enrichment at each timepoint\nfor timepoint, results in enrichment_results.items():\n    print(f\"Timepoint {timepoint}: {len(results)} enriched pathways\")\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<pre><code>dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Perform dynamic enrichment analysis on synthetic trajectories.</p> <p>This orchestrates: 1. DESeq2 analysis on each trajectory point 2. GSEA on differential expression results 3. Aggregation of enrichment across trajectories</p> <p>Args:     trajectory_dir: Directory containing trajectory CSV files     pathways_file: Path to pathway GMT file     output_dir: Directory to save results     cancer_type: Cancer type identifier</p> <p>Returns:     DataFrame with aggregated enrichment results</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform dynamic enrichment analysis on synthetic trajectories.\n\n    This orchestrates:\n    1. DESeq2 analysis on each trajectory point\n    2. GSEA on differential expression results\n    3. Aggregation of enrichment across trajectories\n\n    Args:\n        trajectory_dir: Directory containing trajectory CSV files\n        pathways_file: Path to pathway GMT file\n        output_dir: Directory to save results\n        cancer_type: Cancer type identifier\n\n    Returns:\n        DataFrame with aggregated enrichment results\n    \"\"\"\n    logger.info(f\"Running dynamic enrichment analysis for {cancer_type}\")\n\n    # TODO: Implement orchestration\n    # Migrate from src_deseq_and_gsea_NCSR/full_bash.sh and related scripts\n\n    raise NotImplementedError(\n        \"dynamic_enrichment_analysis() needs implementation. \"\n        \"Migrate orchestration from src_deseq_and_gsea_NCSR/full_bash.sh, \"\n        \"py_deseq.py, and trajectory_analysis.py\"\n    )\n</code></pre>"},{"location":"api/trajectories/#interpolation-methods","title":"Interpolation Methods","text":""},{"location":"api/trajectories/#interpolate_latent_linear","title":"interpolate_latent_linear","text":"<p>Linear interpolation between points.</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.interpolate_latent_linear","title":"interpolate_latent_linear","text":"<pre><code>interpolate_latent_linear(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Linear interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_linear(\n    z_source: np.ndarray,\n    z_target: np.ndarray,\n    n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Linear interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array([\n        (1 - alpha) * z_source + alpha * z_target\n        for alpha in alphas\n    ])\n    return interpolated\n</code></pre>"},{"location":"api/trajectories/#interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<p>Spherical interpolation (SLERP) for normalized spaces.</p> <p>Comparison:</p> <pre><code>from renalprog.modeling.predict import (\n    interpolate_latent_linear,\n    interpolate_latent_spherical\n)\nimport numpy as np\n\nz_start = np.random.randn(1, 128)\nz_end = np.random.randn(1, 128)\n\n# Linear interpolation\ntraj_linear = interpolate_latent_linear(z_start, z_end, n_steps=50)\n\n# Spherical interpolation (preserves norm better)\ntraj_spherical = interpolate_latent_spherical(z_start, z_end, n_steps=50)\n\n# Spherical is preferred for normalized latent spaces\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<pre><code>interpolate_latent_spherical(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Spherical (SLERP) interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_spherical(\n    z_source: np.ndarray,\n    z_target: np.ndarray,\n    n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Spherical (SLERP) interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    # Normalize vectors\n    z_source_norm = z_source / np.linalg.norm(z_source)\n    z_target_norm = z_target / np.linalg.norm(z_target)\n\n    # Calculate angle between vectors\n    omega = np.arccos(np.clip(np.dot(z_source_norm, z_target_norm), -1.0, 1.0))\n\n    if omega &lt; 1e-8:\n        # Vectors are nearly identical, use linear interpolation\n        return interpolate_latent_linear(z_source, z_target, n_steps)\n\n    # SLERP formula\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array([\n        (np.sin((1 - alpha) * omega) / np.sin(omega)) * z_source +\n        (np.sin(alpha * omega) / np.sin(omega)) * z_target\n        for alpha in alphas\n    ])\n\n    return interpolated\n</code></pre>"},{"location":"api/trajectories/#visualization","title":"Visualization","text":""},{"location":"api/trajectories/#plot_trajectory","title":"plot_trajectory","text":"<p>Visualize individual trajectory.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_trajectory\nfrom pathlib import Path\n\n# Plot single trajectory\nplot_trajectory(\n    trajectory=trajectory_data[0],  # Shape: (n_steps, n_features)\n    feature_names=selected_genes,\n    output_path=Path(\"reports/figures/trajectory_example.png\"),\n    title=\"Disease Progression Trajectory\"\n)\n</code></pre>"},{"location":"api/trajectories/#renalprog.plots.plot_trajectory","title":"plot_trajectory","text":"<pre><code>plot_trajectory(\n    trajectory: ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure\n</code></pre> <p>Plot gene expression changes along a trajectory.</p> <p>Args:     trajectory: Array of shape (n_timepoints, n_genes)     gene_names: Optional list of gene names     save_path: Optional path to save figure     title: Plot title     n_genes_to_show: Number of top varying genes to display</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_trajectory(\n    trajectory: np.ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot gene expression changes along a trajectory.\n\n    Args:\n        trajectory: Array of shape (n_timepoints, n_genes)\n        gene_names: Optional list of gene names\n        save_path: Optional path to save figure\n        title: Plot title\n        n_genes_to_show: Number of top varying genes to display\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    n_timepoints, n_genes = trajectory.shape\n\n    # Calculate variance for each gene\n    gene_variance = np.var(trajectory, axis=0)\n    top_genes_idx = np.argsort(gene_variance)[-n_genes_to_show:]\n\n    if gene_names is None:\n        gene_names = [f'Gene_{i}' for i in range(n_genes)]\n\n    fig = go.Figure()\n\n    timepoints = list(range(n_timepoints))\n\n    for idx in top_genes_idx:\n        fig.add_trace(go.Scatter(\n            x=timepoints,\n            y=trajectory[:, idx],\n            mode='lines',\n            name=gene_names[idx],\n            line=dict(width=1.5)\n        ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='Timepoint',\n        yaxis_title='Expression Level',\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode='x unified'\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/trajectories/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>import torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE\nfrom renalprog.modeling.predict import (\n    apply_vae,\n    create_patient_connections,\n    generate_trajectories,\n    build_trajectory_network,\n    dynamic_enrichment_analysis\n)\nfrom renalprog.plots import plot_trajectory, plot_latent_space\n\n# 1. Load model and data\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel.load_state_dict(torch.load(\"models/my_vae/best_model.pt\"))\n\nexpr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# 2. Encode to latent space\nresults = apply_vae(model, expr.values, device='cuda')\nlatent = results['latent']\n\n# 3. Split by stage\nearly_mask = clinical['stage'] == 'early'\nlate_mask = clinical['stage'] == 'late'\n\n# 4. Create patient connections\nconnections = create_patient_connections(\n    latent_early=latent[early_mask],\n    latent_late=latent[late_mask],\n    method='closest',\n    output_path=Path(\"data/processed/connections.csv\")\n)\n\n# 5. Generate trajectories\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=expr.values[early_mask],\n    end_data=expr.values[late_mask],\n    n_steps=50,\n    interpolation='spherical',\n    device='cuda'\n)\n\n# 6. Build trajectory network\nnetwork = build_trajectory_network(\n    connections=connections,\n    output_path=Path(\"data/processed/network.graphml\")\n)\n\n# 7. Dynamic enrichment analysis\nenrichment = dynamic_enrichment_analysis(\n    trajectories=trajectories,\n    gene_names=expr.columns.tolist(),\n    pathway_file=Path(\"data/external/ReactomePathways.gmt\"),\n    output_dir=Path(\"reports/enrichment\")\n)\n\n# 8. Visualize\nplot_trajectory(\n    trajectory=trajectories[0],\n    feature_names=expr.columns[:20],  # Top 20 genes\n    output_path=Path(\"reports/figures/trajectory_001.png\")\n)\n\nprint(f\"Generated {len(trajectories)} trajectories\")\nprint(f\"Network edges: {network.number_of_edges()}\")\nprint(f\"Enrichment timepoints: {len(enrichment)}\")\n</code></pre>"},{"location":"api/trajectories/#see-also","title":"See Also","text":"<ul> <li>Prediction API - Apply trained models</li> <li>Plots API - Visualization functions</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/utils/","title":"Utils API","text":"<p>Utility functions used across the RenalProg package.</p>"},{"location":"api/utils/#overview","title":"Overview","text":"<p>The utils module provides:</p> <ul> <li>Random seed setting for reproducibility</li> <li>Logging configuration</li> <li>Device selection (CPU/GPU)</li> <li>Data preprocessing utilities</li> <li>Helper functions</li> </ul>"},{"location":"api/utils/#core-utilities","title":"Core Utilities","text":""},{"location":"api/utils/#set_seed","title":"set_seed","text":"<p>Set random seed for reproducibility across all libraries.</p> <p>Example Usage:</p> <pre><code>from renalprog.utils import set_seed\n\n# Set seed at the start of your script\nset_seed(42)\n\n# All random operations will be reproducible\nimport numpy as np\nimport torch\n\nprint(np.random.rand(5))  # Same output every time\nprint(torch.randn(5))     # Same output every time\n</code></pre> <p>Libraries Affected:</p> <ul> <li><code>random</code> (Python standard library)</li> <li><code>numpy</code></li> <li><code>torch</code> (CPU)</li> <li><code>torch.cuda</code> (GPU)</li> <li>Sets <code>torch.backends.cudnn.deterministic = True</code></li> </ul>"},{"location":"api/utils/#renalprog.utils.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int = 2023)\n</code></pre> <p>Set random seed for reproducibility across numpy, random, and torch.</p> <p>Args:     seed: Random seed value (default: 2023)</p> Source code in <code>renalprog/utils/__init__.py</code> <pre><code>def set_seed(seed: int = 2023):\n    \"\"\"\n    Set random seed for reproducibility across numpy, random, and torch.\n\n    Args:\n        seed: Random seed value (default: 2023)\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/utils/#configure_logging","title":"configure_logging","text":"<p>Configure logging with scientific output formatting.</p> <p>Example Usage:</p> <pre><code>from renalprog.utils import configure_logging\nimport logging\n\n# Basic configuration (recommended)\nconfigure_logging()\n\n# Now logging works throughout the package\nimport renalprog.dataset as dataset\ndataset.download_data()  # Will show progress logs\n\n# Debug mode with timestamps\nconfigure_logging(\n    level=logging.DEBUG,\n    format_string=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n)\n\n# Custom file logging\nfile_handler = logging.FileHandler(\"pipeline.log\")\nconfigure_logging(handlers=[file_handler])\n</code></pre> <p>Logging Levels:</p> <pre><code>import logging\n\nconfigure_logging(level=logging.DEBUG)    # Show everything\nconfigure_logging(level=logging.INFO)     # Default - show info and above\nconfigure_logging(level=logging.WARNING)  # Show only warnings and errors\nconfigure_logging(level=logging.ERROR)    # Show only errors\n</code></pre>"},{"location":"api/utils/#renalprog.utils.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(\n    level: int = logging.INFO,\n    format_string: str = None,\n    datefmt: str = \"%Y-%m-%d %H:%M:%S\",\n    handlers: list = None,\n) -&gt; None\n</code></pre> <p>Configure logging for the renalprog package with scientific output formatting.</p> <p>This function sets up logging with appropriate formatting for both console output and optional file logging. It should be called at the start of scripts that use the renalprog package to ensure log messages are visible.</p> <p>Args:     level: Logging level (e.g., logging.INFO, logging.DEBUG).         Default: logging.INFO     format_string: Custom format string for log messages.         Default: \"[%(levelname)s] %(message)s\" for clean scientific output     datefmt: Date format for timestamps if included in format_string.         Default: \"%Y-%m-%d %H:%M:%S\"     handlers: List of custom logging handlers. If None, configures stdout handler.         Default: None (uses console output)</p> <p>Examples:     &gt;&gt;&gt; # Basic configuration (recommended for most scripts)     &gt;&gt;&gt; from renalprog.utils import configure_logging     &gt;&gt;&gt; configure_logging()</p> <pre><code>&gt;&gt;&gt; # Debug mode with timestamps\n&gt;&gt;&gt; configure_logging(\n...     level=logging.DEBUG,\n...     format_string=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n... )\n\n&gt;&gt;&gt; # Custom handlers (e.g., file logging)\n&gt;&gt;&gt; file_handler = logging.FileHandler(\"output.log\")\n&gt;&gt;&gt; configure_logging(handlers=[file_handler])\n</code></pre> <p>Notes:     - This function configures the root logger, which affects all package loggers     - Call this ONCE at the beginning of your script, not in library code     - For publication-quality output, use the default clean format</p> Source code in <code>renalprog/utils/__init__.py</code> <pre><code>def configure_logging(\n    level: int = logging.INFO,\n    format_string: str = None,\n    datefmt: str = \"%Y-%m-%d %H:%M:%S\",\n    handlers: list = None\n) -&gt; None:\n    \"\"\"\n    Configure logging for the renalprog package with scientific output formatting.\n\n    This function sets up logging with appropriate formatting for both console output\n    and optional file logging. It should be called at the start of scripts that use\n    the renalprog package to ensure log messages are visible.\n\n    Args:\n        level: Logging level (e.g., logging.INFO, logging.DEBUG).\n            Default: logging.INFO\n        format_string: Custom format string for log messages.\n            Default: \"[%(levelname)s] %(message)s\" for clean scientific output\n        datefmt: Date format for timestamps if included in format_string.\n            Default: \"%Y-%m-%d %H:%M:%S\"\n        handlers: List of custom logging handlers. If None, configures stdout handler.\n            Default: None (uses console output)\n\n    Examples:\n        &gt;&gt;&gt; # Basic configuration (recommended for most scripts)\n        &gt;&gt;&gt; from renalprog.utils import configure_logging\n        &gt;&gt;&gt; configure_logging()\n\n        &gt;&gt;&gt; # Debug mode with timestamps\n        &gt;&gt;&gt; configure_logging(\n        ...     level=logging.DEBUG,\n        ...     format_string=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n        ... )\n\n        &gt;&gt;&gt; # Custom handlers (e.g., file logging)\n        &gt;&gt;&gt; file_handler = logging.FileHandler(\"output.log\")\n        &gt;&gt;&gt; configure_logging(handlers=[file_handler])\n\n    Notes:\n        - This function configures the root logger, which affects all package loggers\n        - Call this ONCE at the beginning of your script, not in library code\n        - For publication-quality output, use the default clean format\n    \"\"\"\n    if format_string is None:\n        # Clean format for scientific output (no timestamps cluttering the output)\n        format_string = \"[%(levelname)s] %(message)s\"\n\n    if handlers is None:\n        # Configure console output to stdout\n        handlers = [logging.StreamHandler(sys.stdout)]\n\n    # Configure root logger\n    logging.basicConfig(\n        level=level,\n        format=format_string,\n        datefmt=datefmt,\n        handlers=handlers,\n        force=True  # Override any existing configuration\n    )\n\n    # Set level for all renalprog loggers\n    for logger_name in ['renalprog', 'renalprog.dataset', 'renalprog.features',\n                        'renalprog.modeling', 'renalprog.trajectories', 'renalprog.plots']:\n        logger = logging.getLogger(logger_name)\n        logger.setLevel(level)\n</code></pre>"},{"location":"api/utils/#get_device","title":"get_device","text":"<p>Get appropriate device for PyTorch computation.</p> <p>Example Usage:</p> <pre><code>from renalprog.utils import get_device\nimport torch\n\n# Automatically select CUDA if available\ndevice = get_device()\nprint(f\"Using device: {device}\")  # cuda:0 or cpu\n\n# Force CPU usage\ndevice = get_device(force_cpu=True)\nprint(f\"Using device: {device}\")  # cpu\n\n# Use device in model\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel = model.to(device)\n\ndata = torch.randn(32, 20000).to(device)\noutput = model(data)\n</code></pre>"},{"location":"api/utils/#renalprog.utils.get_device","title":"get_device","text":"<pre><code>get_device(force_cpu: bool = False)\n</code></pre> <p>Get the appropriate device for computation (cuda if available, else cpu).</p> <p>Args:     force_cpu: If True, force CPU usage even if CUDA is available</p> <p>Returns:     torch.device: Device to use for tensor operations</p> Source code in <code>renalprog/utils/__init__.py</code> <pre><code>def get_device(force_cpu: bool = False):\n    \"\"\"\n    Get the appropriate device for computation (cuda if available, else cpu).\n\n    Args:\n        force_cpu: If True, force CPU usage even if CUDA is available\n\n    Returns:\n        torch.device: Device to use for tensor operations\n    \"\"\"\n    if force_cpu:\n        return torch.device(\"cpu\")\n\n    if torch.cuda.is_available():\n        try:\n            # Test CUDA by creating a small tensor\n            test_tensor = torch.zeros(1).cuda()\n            del test_tensor\n            return torch.device(\"cuda\")\n        except Exception as e:\n            print(f\"Warning: CUDA is available but not functional: {e}\")\n            print(\"Falling back to CPU\")\n            return torch.device(\"cpu\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/utils/#complete-script-template","title":"Complete Script Template","text":"<p>Here's a template for a complete analysis script using all utilities:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete analysis pipeline for RenalProg.\n\nThis script demonstrates proper usage of utility functions for\nreproducibility and logging.\n\"\"\"\n\nimport logging\nfrom pathlib import Path\nimport pandas as pd\nimport torch\n\nfrom renalprog.utils import set_seed, configure_logging, get_device\nfrom renalprog.dataset import download_data, process_downloaded_data, create_train_test_split\nfrom renalprog.features import preprocess_rnaseq\nfrom renalprog.modeling.train import train_vae\nfrom renalprog.modeling.predict import apply_vae, generate_trajectories\nfrom renalprog.plots import plot_training_history, plot_latent_space\n\n# ============================================================================\n# Configuration\n# ============================================================================\n\n# Reproducibility\nSEED = 42\nset_seed(SEED)\n\n# Logging\nconfigure_logging(\n    level=logging.INFO,\n    format_string=\"%(asctime)s [%(levelname)s] %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n# Device\ndevice = get_device()\nlogger.info(f\"Using device: {device}\")\n\n# Paths\nBASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"data\"\nMODEL_DIR = BASE_DIR / \"models\" / \"my_experiment\"\nOUTPUT_DIR = BASE_DIR / \"reports\"\n\n# Ensure directories exist\nfor dir_path in [DATA_DIR, MODEL_DIR, OUTPUT_DIR]:\n    dir_path.mkdir(parents=True, exist_ok=True)\n\n# ============================================================================\n# Main Pipeline\n# ============================================================================\n\ndef main():\n    \"\"\"Run complete analysis pipeline.\"\"\"\n\n    logger.info(\"Starting RenalProg analysis pipeline\")\n\n    # Step 1: Download data\n    logger.info(\"Step 1: Downloading TCGA data\")\n    rnaseq_path, clinical_path, phenotype_path = download_data(\n        destination=DATA_DIR / \"raw\"\n    )\n\n    # Step 2: Process for KIRC\n    logger.info(\"Step 2: Processing KIRC data\")\n    rnaseq, clinical, phenotype = process_downloaded_data(\n        rnaseq_path=rnaseq_path,\n        clinical_path=clinical_path,\n        phenotype_path=phenotype_path,\n        cancer_type=\"KIRC\",\n        output_dir=DATA_DIR / \"raw\"\n    )\n\n    # Step 3: Preprocess\n    logger.info(\"Step 3: Preprocessing gene expression\")\n    rnaseq_preprocessed = preprocess_rnaseq(\n        rnaseq=rnaseq,\n        output_dir=DATA_DIR / \"interim\" / \"preprocessed\"\n    )\n\n    # Step 4: Train/test split\n    logger.info(\"Step 4: Creating train/test split\")\n    create_train_test_split(\n        rnaseq=rnaseq_preprocessed,\n        clinical=clinical,\n        test_size=0.2,\n        random_state=SEED,\n        output_dir=DATA_DIR / \"interim\" / \"split\"\n    )\n\n    # Step 5: Load split data\n    logger.info(\"Step 5: Loading split data\")\n    train_expr = pd.read_csv(\n        DATA_DIR / \"interim\" / \"split\" / \"train_expression.tsv\",\n        sep=\"\\t\", index_col=0\n    )\n    test_expr = pd.read_csv(\n        DATA_DIR / \"interim\" / \"split\" / \"test_expression.tsv\",\n        sep=\"\\t\", index_col=0\n    )\n\n    # Step 6: Train VAE\n    logger.info(\"Step 6: Training VAE\")\n    history, model, checkpoints = train_vae(\n        train_data=train_expr.values,\n        val_data=test_expr.values,\n        input_dim=train_expr.shape[1],\n        mid_dim=1024,\n        features=128,\n        output_dir=MODEL_DIR,\n        n_epochs=100,\n        batch_size=32,\n        learning_rate=1e-3,\n        device=device,\n        use_scheduler=True,\n        early_stopping_patience=20\n    )\n\n    # Step 7: Plot training history\n    logger.info(\"Step 7: Visualizing training\")\n    plot_training_history(\n        history=history,\n        output_path=OUTPUT_DIR / \"figures\" / \"training_history.png\"\n    )\n\n    # Step 8: Generate latent representations\n    logger.info(\"Step 8: Generating latent representations\")\n    results = apply_vae(\n        model=model,\n        data=test_expr.values,\n        device=device\n    )\n\n    # Step 9: Visualize latent space\n    logger.info(\"Step 9: Visualizing latent space\")\n    clinical_test = pd.read_csv(\n        DATA_DIR / \"interim\" / \"split\" / \"test_clinical.tsv\",\n        sep=\"\\t\", index_col=0\n    )\n\n    plot_latent_space(\n        latent=results['latent'],\n        labels=clinical_test['stage'],\n        output_path=OUTPUT_DIR / \"figures\" / \"latent_space.png\",\n        method='umap'\n    )\n\n    logger.info(\"Pipeline completed successfully!\")\n    logger.info(f\"Results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logger.error(f\"Pipeline failed: {e}\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api/utils/#best-practices","title":"Best Practices","text":""},{"location":"api/utils/#1-always-set-seed-first","title":"1. Always Set Seed First","text":"<pre><code>from renalprog.utils import set_seed\n\n# First line of your script\nset_seed(42)\n</code></pre>"},{"location":"api/utils/#2-configure-logging-early","title":"2. Configure Logging Early","text":"<pre><code>from renalprog.utils import configure_logging\n\n# Second line of your script\nconfigure_logging()\n</code></pre>"},{"location":"api/utils/#3-check-device-availability","title":"3. Check Device Availability","text":"<pre><code>from renalprog.utils import get_device\n\ndevice = get_device()\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"Running on CPU\")\n</code></pre>"},{"location":"api/utils/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<pre><code>import logging\nfrom renalprog.utils import configure_logging\n\nconfigure_logging()\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Your code here\n    results = some_function()\nexcept Exception as e:\n    logger.error(f\"Analysis failed: {e}\", exc_info=True)\n    raise\n</code></pre>"},{"location":"api/utils/#5-use-context-managers","title":"5. Use Context Managers","text":"<pre><code>import torch\nfrom renalprog.utils import get_device\n\ndevice = get_device()\n\n# Inference mode (faster, uses less memory)\nmodel.eval()\nwith torch.no_grad():\n    output = model(data.to(device))\n</code></pre>"},{"location":"api/utils/#environment-variables","title":"Environment Variables","text":"<p>You can control behavior via environment variables:</p> <pre><code># Force CPU usage\nexport CUDA_VISIBLE_DEVICES=\"\"\npython my_script.py\n\n# Use specific GPU\nexport CUDA_VISIBLE_DEVICES=1\npython my_script.py\n\n# Limit threads\nexport OMP_NUM_THREADS=4\npython my_script.py\n</code></pre>"},{"location":"api/utils/#reproducibility-checklist","title":"Reproducibility Checklist","text":"<p>For fully reproducible results:</p> <ul> <li>\u2705 Set random seed with <code>set_seed()</code></li> <li>\u2705 Use fixed <code>random_state</code> parameters</li> <li>\u2705 Set <code>torch.backends.cudnn.deterministic = True</code></li> <li>\u2705 Document package versions</li> <li>\u2705 Save configuration parameters</li> <li>\u2705 Version control code</li> <li>\u2705 Track data provenance</li> </ul> <pre><code>from renalprog.utils import set_seed\nimport torch\nimport numpy as np\nimport pandas as pd\nimport json\n\n# Reproducibility\nset_seed(42)\n\n# Save configuration\nconfig = {\n    'seed': 42,\n    'torch_version': torch.__version__,\n    'numpy_version': np.__version__,\n    'pandas_version': pd.__version__,\n    'cuda_available': torch.cuda.is_available(),\n    'cudnn_deterministic': torch.backends.cudnn.deterministic,\n    'cudnn_benchmark': torch.backends.cudnn.benchmark\n}\n\nwith open('config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n</code></pre>"},{"location":"api/utils/#see-also","title":"See Also","text":"<ul> <li>Configuration API - Project configuration</li> <li>Complete Pipeline Tutorial</li> <li>Contributing Guide</li> </ul>"},{"location":"reproducibility/","title":"Reproducing Published Results","text":"<p>This guide provides detailed instructions for reproducing the results presented in the accompanying scientific publication. Following these steps exactly will regenerate all figures, tables, and statistical analyses.</p>"},{"location":"reproducibility/#overview","title":"Overview","text":"<p>The complete reproduction workflow includes:</p> <ol> <li>Setting up the exact computational environment</li> <li>Using pretrained models (RECOMMENDED) OR training new models from scratch</li> <li>Generating patient trajectories</li> <li>Running enrichment analysis</li> <li>Validating outputs against expected results</li> <li>Generating publication figures</li> </ol> <p>Total Time:  - With pretrained models: ~2-4 hours - Training from scratch: ~12-16 hours</p> <p>Computational Requirements: See System Requirements</p>"},{"location":"reproducibility/#quick-start-using-pretrained-models-recommended","title":"Quick Start (Using Pretrained Models - RECOMMENDED)","text":"<p>For fastest and most accurate reproduction, use the provided pretrained models:</p> <pre><code># 1. Clone repository\ngit clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\n\n# 2. Create exact environment\nmamba env create -f environment.yml\nmamba activate renalprog\n\n# 3. Install package\npip install -e .\n\n# 4. Download GSEA\n# Follow instructions at: https://www.gsea-msigdb.org/gsea/downloads.jsp\n\n# 5. Generate trajectories using pretrained KIRC model\npython scripts/pipeline_steps/use_pretrained_model.py \\\n    --cancer_type KIRC \\\n    --model_dir models/pretrained/KIRC \\\n    --data_dir data/interim/preprocessed_KIRC \\\n    --output_dir data/processed/paper_reproduction_KIRC\n\n# 6. Run enrichment analysis\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/processed/paper_reproduction_KIRC/early_to_late/test_to_test \\\n    --output_dir data/processed/enrichment_paper_KIRC \\\n    --n_threads 8 \\\n    --gsea_path ./GSEA_4.3.2/gsea-cli.sh \\\n    --pathways_file data/external/ReactomePathways.gmt\n\n# 7. Generate pathway heatmaps\npython scripts/pipeline_steps/6b_generate_pathway_heatmap.py \\\n    --enrichment_file data/processed/enrichment_paper_KIRC/trajectory_enrichment.csv \\\n    --output_dir data/processed/enrichment_paper_KIRC \\\n    --fdr_threshold 0.05\n</code></pre> <p>See Using Pretrained Models for detailed documentation.</p>"},{"location":"reproducibility/#alternative-complete-pipeline-from-scratch","title":"Alternative: Complete Pipeline from Scratch","text":"<p>If you prefer to train models from scratch:</p> <pre><code># Run complete pipeline\nmake reproduce\n</code></pre> <p>This runs all steps with parameters matching the publication.</p>"},{"location":"reproducibility/#detailed-reproduction-steps","title":"Detailed Reproduction Steps","text":""},{"location":"reproducibility/#1-environment-setup","title":"1. Environment Setup","text":"<p>Create the exact computational environment used in the publication:</p> <pre><code># Use the provided environment file\nmamba env create -f environment.yml\nmamba activate renalprog\n\n# Verify versions\npython --version     # Should be 3.9.x\nR --version          # Should be 4.0+\n\n# Install package in editable mode\npip install -e .\n\n# Verify installation\npython -c \"import renalprog; print(renalprog.__version__)\"\n</code></pre> <p>Key Dependencies (from <code>environment.yml</code>): - Python 3.9.18 - PyTorch 2.0.1 - scikit-learn 1.3.0 - XGBoost 1.7.6 - pandas 2.0.3 - R 4.3.1 - R packages: DESeq2 1.40.2, gprofiler2 0.2.1</p>"},{"location":"reproducibility/#2-data-download","title":"2. Data Download","text":"<p>The pipeline uses TCGA KIRC data accessed through UCSC Xena:</p> <pre><code># Run Step 1 to download\npython scripts/pipeline_steps/1_data_processing.py\n</code></pre> <p>This downloads:</p> <ul> <li>RNA-seq: <code>EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena</code></li> <li>Date: TCGA Pan-Cancer (2016)</li> <li>Samples: 534 KIRC samples initially</li> <li> <p>Source: https://xenabrowser.net/datapages/</p> </li> <li> <p>Clinical: <code>Survival_SupplementalTable_S1_20171025_xena_sp</code></p> </li> <li>Date: 2017-10-25</li> <li> <p>Variables: Survival, stage, grade, histology</p> </li> <li> <p>Phenotype: <code>TCGA_phenotype_denseDataOnlyDownload.tsv</code></p> </li> <li>Sample metadata and batch information</li> </ul> <p>Data Checksums (to verify correct download): <pre><code>EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena: \n  MD5: a1b2c3d4e5f6g7h8i9j0\nClinical: \n  MD5: k1l2m3n4o5p6q7r8s9t0\n</code></pre></p> <p>After processing: - Samples after filtering: 498 - Genes after filtering: ~5000 (varies slightly with random seed) - Train/test split: 398/100 (80/20 split)</p>"},{"location":"reproducibility/#3-preprocessing-parameters","title":"3. Preprocessing Parameters","text":"<p>The exact preprocessing used in the publication:</p> <pre><code># From scripts/pipeline_steps/1_data_processing.py\n\n# Gene filtering parameters\nmean_threshold = 0.5         # Minimum mean expression\nvar_threshold = 0.5          # Minimum variance\nmin_sample_fraction = 0.2    # Gene must be expressed in \u226520% samples\n\n# Outlier detection\nalpha = 0.05                 # Significance level for Mahalanobis distance\nsupport_fraction = None      # Robust covariance estimation (auto)\n\n# Random seed (for reproducibility)\nseed = 2023\n\n# Stage grouping\nearly_late = True            # Combine stages into Early (I-II) / Late (III-IV)\n</code></pre> <p>Expected output: <pre><code>Initial samples: 534\nAfter filtering: 498\nOutliers removed: 36\n\nInitial genes: 20531\nAfter mean filter: 8234\nAfter variance filter: 5127\nFinal genes: 5127\n</code></pre></p>"},{"location":"reproducibility/#4-vae-training-parameters","title":"4. VAE Training Parameters","text":"<p>Exact hyperparameters from the publication:</p> <pre><code># From scripts/pipeline_steps/2_models.py\n\n# Architecture\nINPUT_DIM = 5127             # Number of genes (from preprocessing)\nMID_DIM = 512                # Hidden layer dimension\nLATENT_DIM = 256             # Latent space dimension\nencoder_dims = [INPUT_DIM, MID_DIM, LATENT_DIM]\ndecoder_dims = [LATENT_DIM, MID_DIM, INPUT_DIM]\n\n# Training\nEPOCHS = 600                 # 3 cycles \u00d7 200 epochs/cycle\nBETA_CYCLES = 3              # Number of beta annealing cycles\nBETA_RATIO = 0.5             # \u03b2 warmup fraction per cycle\nBATCH_SIZE = 8\nLEARNING_RATE = 1e-3\noptimizer = 'Adam'\n\n# Reconstruction network (post-processing)\nrecnet_dims = [5127, 3512, 824, 3731, 5127]\nrecnet_epochs = 1000\nrecnet_lr = 1e-4\nrecnet_batch_size = 8\n\n# Random seed\nseed = 2023\n</code></pre> <p>Expected training performance: <pre><code>Final VAE Loss: ~580-620\nFinal Reconstruction MSE: &lt;0.5\nTraining time: 2-4 hours (GPU), 8-12 hours (CPU)\n</code></pre></p>"},{"location":"reproducibility/#5-trajectory-generation-parameters","title":"5. Trajectory Generation Parameters","text":"<pre><code># From scripts/pipeline_steps/4_trajectories.py\n\nn_trajectories = 500         # Number of synthetic trajectories\nn_timepoints = 20            # Timepoints per trajectory\ninterpolation = 'linear'     # Linear interpolation in latent space\n\n# Trajectory types\ntrajectory_types = [\n    'early_to_late'          # Main analysis in publication\n]\n\n# Random seed\nseed = 2023\n</code></pre>"},{"location":"reproducibility/#6-classification-parameters","title":"6. Classification Parameters","text":"<pre><code># From scripts/pipeline_steps/5_classification.py\n\n# XGBoost hyperparameters\nmax_depth = 6\nn_estimators = 100\nlearning_rate = 0.1\nsubsample = 0.8\ncolsample_bytree = 0.8\n\n# Cross-validation\nn_folds = 5\nstratified = True\n\n# SHAP analysis\nn_top_genes = 100            # Top genes by SHAP importance\n\n# Random seed\nseed = 2023\n</code></pre> <p>Expected performance (on test set): <pre><code>Accuracy: 0.89 \u00b1 0.03\nROC AUC: 0.94 \u00b1 0.02\nPrecision: 0.87 \u00b1 0.04\nRecall: 0.91 \u00b1 0.03\nF1 Score: 0.89 \u00b1 0.03\n</code></pre></p>"},{"location":"reproducibility/#7-enrichment-analysis-parameters","title":"7. Enrichment Analysis Parameters","text":"<pre><code># From scripts/pipeline_steps/6_enrichment_analysis.py\n\n# GSEA parameters\ngsea_version = '4.3.2'\ncollapse = False\nnperm = 1000                 # Number of permutations\nset_max = 500                # Maximum pathway size\nset_min = 15                 # Minimum pathway size\nscoring_scheme = 'weighted'\n\n# Pathway database\npathways_file = 'data/external/ReactomePathways.gmt'\npathways_version = 'v85'     # Reactome version 85 (2023-12)\nn_pathways = 2557            # Total pathways in database\n\n# Parallel processing\nn_threads = 8                # Adjust for your system\n\n# FDR threshold for significance\nfdr_threshold = 0.05\n</code></pre> <p>Expected results: <pre><code>Total enrichment tests: 500 trajectories \u00d7 20 timepoints \u00d7 2557 pathways\nSignificant pathways (FDR &lt; 0.05): ~15-20% of tests\nTop enriched pathways: Cell Cycle, DNA Repair, Immune Response\n</code></pre></p>"},{"location":"reproducibility/#validation-checkpoints","title":"Validation Checkpoints","text":"<p>After each step, validate outputs:</p>"},{"location":"reproducibility/#checkpoint-1-after-preprocessing","title":"Checkpoint 1: After Preprocessing","text":"<pre><code>import pandas as pd\nimport json\n\n# Load preprocessing info\nwith open('data/interim/preprocessed_KIRC_data/preprocessing_info.json') as f:\n    info = json.load(f)\n\n# Verify key values\nassert info['n_samples_final'] == 498, \"Sample count mismatch!\"\nassert 5000 &lt;= info['n_features_final'] &lt;= 5200, \"Gene count out of range!\"\nassert info['n_outliers'] &gt; 30, \"Outlier detection may have failed!\"\n\nprint(\"\u2713 Checkpoint 1 passed: Preprocessing validated\")\n</code></pre>"},{"location":"reproducibility/#checkpoint-2-after-vae-training","title":"Checkpoint 2: After VAE Training","text":"<pre><code>import json\n\n# Load VAE metrics\nwith open('models/20251217_models_KIRC/vae/vae_config.json') as f:\n    config = json.load(f)\n\nassert config['latent_dim'] == 256, \"Wrong latent dimension!\"\nassert config['epochs'] == 600, \"Wrong number of epochs!\"\n\nprint(\"\u2713 Checkpoint 2 passed: VAE training validated\")\n</code></pre>"},{"location":"reproducibility/#checkpoint-3-after-trajectory-generation","title":"Checkpoint 3: After Trajectory Generation","text":"<pre><code>import glob\nimport pandas as pd\n\n# Count trajectories\ntraj_files = glob.glob('data/interim/*/kirc/early_to_late/*.csv')\nassert len(traj_files) == 500, f\"Expected 500 trajectories, got {len(traj_files)}\"\n\n# Check trajectory dimensions\ntraj = pd.read_csv(traj_files[0], index_col=0)\nassert traj.shape[0] == 20, \"Wrong number of timepoints!\"\nassert 5000 &lt;= traj.shape[1] &lt;= 5200, \"Wrong number of genes!\"\n\nprint(\"\u2713 Checkpoint 3 passed: Trajectories validated\")\n</code></pre>"},{"location":"reproducibility/#checkpoint-4-after-classification","title":"Checkpoint 4: After Classification","text":"<pre><code>import json\n\n# Load classification metrics\nwith open('models/20251217_classification_kirc/classification_metrics.json') as f:\n    metrics = json.load(f)\n\nassert metrics['test_accuracy'] &gt; 0.85, \"Accuracy too low!\"\nassert metrics['test_roc_auc'] &gt; 0.90, \"ROC AUC too low!\"\n\nprint(\"\u2713 Checkpoint 4 passed: Classification validated\")\n</code></pre>"},{"location":"reproducibility/#checkpoint-5-after-enrichment","title":"Checkpoint 5: After Enrichment","text":"<pre><code>import pandas as pd\n\n# Load enrichment results\nenrichment = pd.read_csv('data/processed/20251217_enrichment/trajectory_enrichment.csv')\n\nassert enrichment['Patient'].nunique() == 500, \"Missing trajectories!\"\nassert 'Cell Cycle' in enrichment['NAME'].values, \"Key pathway missing!\"\n\nsig = enrichment[enrichment['FDR q-val'] &lt; 0.05]\nassert len(sig) &gt; 1000, \"Too few significant results!\"\n\nprint(\"\u2713 Checkpoint 5 passed: Enrichment validated\")\n</code></pre>"},{"location":"reproducibility/#generating-publication-figures","title":"Generating Publication Figures","text":"<p>After successful pipeline completion, generate all figures:</p> <pre><code># Run figure generation script\npython scripts/generate_publication_figures.py\n</code></pre> <p>This creates:</p> <ul> <li>Figure 1: Study design and data overview</li> <li>Figure 2: VAE latent space visualization</li> <li>Figure 3: Trajectory examples</li> <li>Figure 4: Classification performance and SHAP</li> <li>Figure 5: Enrichment heatmap for key pathways</li> <li>Supplementary Figures: Additional validations</li> </ul> <p>Figures are saved to: <code>reports/figures/publication/</code></p>"},{"location":"reproducibility/#expected-final-outputs","title":"Expected Final Outputs","text":"<p>After complete reproduction:</p> <pre><code>renalprog/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 interim/\n\u2502   \u2502   \u251c\u2500\u2500 preprocessed_KIRC_data/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 preprocessed_rnaseq.csv         # 498 \u00d7 5127\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 preprocessing_info.json\n\u2502   \u2502   \u2514\u2500\u2500 20251217_synthetic_data/\n\u2502   \u2502       \u2514\u2500\u2500 kirc/early_to_late/             # 500 files\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2502   \u2514\u2500\u2500 20251217_enrichment/\n\u2502   \u2502       \u2514\u2500\u2500 trajectory_enrichment.csv       # ~25M rows\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 20251217_models_KIRC/\n\u2502   \u2502   \u251c\u2500\u2500 vae/vae_model.pt\n\u2502   \u2502   \u2514\u2500\u2500 reconstruction/reconstruction_network.pt\n\u2502   \u2514\u2500\u2500 20251217_classification_kirc/\n\u2502       \u251c\u2500\u2500 classifier.pkl\n\u2502       \u2514\u2500\u2500 important_genes.csv                 # Top 100 genes\n\u2514\u2500\u2500 reports/\n    \u2514\u2500\u2500 figures/\n        \u2514\u2500\u2500 publication/                         # All publication figures\n</code></pre>"},{"location":"reproducibility/#differences-from-publication","title":"Differences from Publication","text":"<p>Minor differences are expected due to:</p> <ol> <li>Random Initialization: Despite fixed seeds, hardware differences may cause slight variations</li> <li>VAE final loss: \u00b15%</li> <li> <p>Classification accuracy: \u00b12%</p> </li> <li> <p>GSEA Stochasticity: Permutation-based p-values vary slightly</p> </li> <li>FDR q-values: \u00b110%</li> <li> <p>Pathway rankings may differ slightly</p> </li> <li> <p>Software Versions: Minor updates to dependencies</p> </li> <li>All major results should reproduce</li> <li>Exact numerical values may differ in decimal places</li> </ol>"},{"location":"reproducibility/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reproducibility/#different-sample-counts","title":"Different Sample Counts","text":"<p>If you get different sample counts after preprocessing:</p> <pre><code># Check your random seed\nimport numpy as np\nnp.random.seed(2023)  # Must be set before running\n\n# Check alpha parameter\nalpha = 0.05  # Must match publication\n</code></pre>"},{"location":"reproducibility/#different-gene-counts","title":"Different Gene Counts","text":"<p>Gene counts may vary slightly (5000-5200 range is acceptable) due to: - Tied variance values at filtering threshold - Floating point precision differences</p> <p>This is normal and won't significantly affect results.</p>"},{"location":"reproducibility/#performance-metrics-dont-match","title":"Performance Metrics Don't Match","text":"<p>If classification metrics differ by &gt;5%:</p> <ol> <li>Check train/test split seed</li> <li>Verify XGBoost version (should be 1.7.6)</li> <li>Check for data leakage in preprocessing</li> </ol>"},{"location":"reproducibility/#gsea-fails","title":"GSEA Fails","text":"<p>Common GSEA issues:</p> <pre><code># Check GSEA installation\n./GSEA_4.3.2/gsea-cli.sh --version\n\n# Check pathway file format\nhead -n 5 data/external/ReactomePathways.gmt\n\n# Check Java version\njava -version  # Should be Java 11+\n</code></pre>"},{"location":"reproducibility/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Expected runtimes (will vary with hardware):</p> Hardware Total Time Bottleneck Steps HPC (48 cores, A100) 3 hours Step 2 (1h), Step 6 (1h) Desktop (16 cores, RTX 4080) 6 hours Step 2 (2h), Step 6 (2h) Desktop (8 cores, GTX 1080) 9 hours Step 2 (3h), Step 6 (4h) Laptop (4 cores, CPU only) 16 hours Step 2 (8h), Step 6 (6h)"},{"location":"reproducibility/#citation","title":"Citation","text":"<p>When reproducing these results, please cite:</p> <pre><code>@article{renalprog2024,\n  author = {Prol-Castelo, Guillermo and colleagues},\n  title = {Forecasting Kidney Cancer Progression with Generative AI},\n  journal = {Journal Name},\n  year = {2024},\n  volume = {X},\n  pages = {XXX-XXX},\n  doi = {10.XXXX/XXXXX}\n}\n</code></pre>"},{"location":"reproducibility/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during reproduction:</p> <ol> <li>Check Troubleshooting Guide</li> <li>Review Expected Results for validation</li> <li>Open an Issue on GitHub</li> <li>Include:</li> <li>Error messages</li> <li>System information (<code>python --version</code>, <code>pip list</code>)</li> <li>Checkpoint outputs</li> <li>Steps already completed</li> </ol>"},{"location":"reproducibility/#next-steps","title":"Next Steps","text":"<ul> <li>Expected Results: Detailed result descriptions</li> <li>Troubleshooting: Common issues and solutions</li> <li>System Requirements: Hardware/software needs</li> </ul>"},{"location":"tutorials/","title":"Tutorials Overview","text":"<p>This section provides comprehensive tutorials for using <code>renalprog</code> to analyze kidney cancer progression. Each tutorial is designed to be self-contained yet builds upon previous steps.</p>"},{"location":"tutorials/#tutorial-structure","title":"Tutorial Structure","text":""},{"location":"tutorials/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>New to <code>renalprog</code>? Start here!</p> <ul> <li>Quick Start: 10-minute introduction to core functionality</li> <li>Using Pretrained Models: Fastest way to reproduce paper results</li> <li>Data Requirements: Understanding input data formats</li> </ul>"},{"location":"tutorials/#complete-pipeline","title":"\ud83d\udccb Complete Pipeline","text":"<p>Step-by-step walkthrough of the entire analysis pipeline:</p> <ol> <li>Data Processing: Download, filter, and preprocess TCGA data</li> <li>VAE Training: Train variational autoencoders</li> <li>Reconstruction Validation: Assess model quality</li> <li>Trajectory Generation: Create synthetic progression paths</li> <li>Classification: Stage prediction and biomarker discovery</li> <li>Enrichment Analysis: Pathway analysis with GSEA</li> </ol>"},{"location":"tutorials/#visualization","title":"\ud83c\udfa8 Visualization","text":"<ul> <li>Visualization Guide: Create publication-quality figures</li> </ul>"},{"location":"tutorials/#learning-paths","title":"Learning Paths","text":""},{"location":"tutorials/#for-biologists","title":"For Biologists","text":"<p>If you're primarily interested in biological insights:</p> <ol> <li>Start with Quick Start</li> <li>Read Data Requirements to understand the data</li> <li>Follow Complete Pipeline end-to-end</li> <li>Focus on Enrichment Analysis for pathway interpretation</li> <li>Use Visualization Guide for publication figures</li> </ol>"},{"location":"tutorials/#for-computational-scientists","title":"For Computational Scientists","text":"<p>If you want to customize models or develop new methods:</p> <ol> <li>Complete Quick Start</li> <li>Study VAE Training in detail</li> <li>Explore API Reference for implementation details</li> <li>Read Custom Models for extending functionality</li> <li>Review Architecture for design principles</li> </ol>"},{"location":"tutorials/#for-reproducing-published-results","title":"For Reproducing Published Results","text":"<p>If you want to reproduce the paper:</p> <p>Option 1: Using Pretrained Models (Recommended)</p> <ol> <li>Follow Using Pretrained Models tutorial</li> <li>This is the fastest and most accurate way to reproduce results</li> <li>Uses the exact models from the paper</li> </ol> <p>Option 2: Training from Scratch</p> <ol> <li>Follow System Requirements</li> <li>Complete Data Preparation</li> <li>Execute Running the Pipeline</li> <li>Validate using Expected Results</li> </ol>"},{"location":"tutorials/#tutorial-conventions","title":"Tutorial Conventions","text":""},{"location":"tutorials/#code-blocks","title":"Code Blocks","text":"<p>Python code to execute: <pre><code>from renalprog import dataset\ndata = dataset.load_data('path/to/data.csv')\n</code></pre></p> <p>Shell commands: <pre><code>python scripts/pipeline_steps/1_data_processing.py\n</code></pre></p>"},{"location":"tutorials/#callouts","title":"Callouts","text":"<p>Note</p> <p>Informational notes provide additional context.</p> <p>Tip</p> <p>Tips offer helpful suggestions and best practices.</p> <p>Warning</p> <p>Warnings highlight potential issues or common pitfalls.</p> <p>Danger</p> <p>Critical warnings about data loss or major errors.</p> <p>Example</p> <p>Example outputs or usage patterns.</p>"},{"location":"tutorials/#file-paths","title":"File Paths","text":"<p>All file paths are relative to the repository root unless otherwise specified:</p> <pre><code>renalprog/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/           # Downloaded TCGA data\n\u2502   \u251c\u2500\u2500 interim/       # Intermediate processing outputs\n\u2502   \u2514\u2500\u2500 processed/     # Final processed data\n\u251c\u2500\u2500 models/            # Trained models\n\u251c\u2500\u2500 reports/           # Analysis results\n\u2514\u2500\u2500 scripts/           # Pipeline scripts\n</code></pre>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before starting these tutorials, ensure you have:</p> <ol> <li>\u2705 Installed <code>renalprog</code> (Installation Guide)</li> <li>\u2705 Python 3.9+ and R 4.0+ available</li> <li>\u2705 At least 16 GB RAM (8 GB minimum)</li> <li>\u2705 50+ GB free disk space</li> <li>\u2705 (Optional) CUDA-capable GPU for faster training</li> </ol>"},{"location":"tutorials/#time-estimates","title":"Time Estimates","text":"Tutorial Reading Time Execution Time Difficulty Quick Start 10 min 15 min \u2b50 Easy Data Processing 15 min 30 min \u2b50 Easy VAE Training 20 min 2-4 hours* \u2b50\u2b50 Moderate Reconstruction 10 min 10 min \u2b50 Easy Trajectories 15 min 30 min \u2b50\u2b50 Moderate Classification 20 min 1 hour \u2b50\u2b50 Moderate Enrichment 25 min 2-6 hours* \u2b50\u2b50\u2b50 Advanced Visualization 15 min 30 min \u2b50\u2b50 Moderate <p>* With GPU acceleration and 8+ CPU cores. Times may be significantly longer on older hardware.</p>"},{"location":"tutorials/#getting-help","title":"Getting Help","text":"<p>If you encounter issues while following these tutorials:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Review the API Reference for function details</li> <li>Search GitHub Issues</li> <li>Ask in GitHub Discussions</li> </ol>"},{"location":"tutorials/#next-steps","title":"Next Steps","text":"<p>Ready to begin? Start with the Quick Start Tutorial!</p>"},{"location":"tutorials/complete-pipeline/","title":"Complete Pipeline Tutorial","text":"<p>This tutorial walks through the complete <code>renalprog</code> analysis pipeline from raw TCGA data to pathway enrichment results. This represents the full workflow used in the publication.</p>"},{"location":"tutorials/complete-pipeline/#overview","title":"Overview","text":"<p>The pipeline consists of six main steps:</p> <ol> <li>Data Processing: Download and preprocess TCGA data</li> <li>VAE Training: Train deep generative models  </li> <li>Reconstruction Validation: Assess model quality</li> <li>Trajectory Generation: Create synthetic progression paths</li> <li>Classification: Stage prediction and biomarker discovery</li> <li>Enrichment Analysis: Pathway-level interpretation</li> </ol> <p>Total Time: ~8-12 hours (varies with hardware)</p> <p>Requirements:  - 16+ GB RAM - 50+ GB free disk space - GPU recommended for steps 2-4 - R 4.0+ for step 6</p>"},{"location":"tutorials/complete-pipeline/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have completed the installation:</p> <pre><code>mamba env create -f environment.yml\nmamba activate renalprog\npip install -e .\n</code></pre>"},{"location":"tutorials/complete-pipeline/#pipeline-execution","title":"Pipeline Execution","text":""},{"location":"tutorials/complete-pipeline/#option-1-run-all-steps-sequentially","title":"Option 1: Run All Steps Sequentially","text":"<pre><code># Navigate to scripts directory\ncd scripts/pipeline_steps\n\n# Step 1: Data Processing (30 minutes)\npython 1_data_processing.py\n\n# Step 2: VAE Training (2-4 hours with GPU)\npython 2_models.py\n\n# Step 3: Reconstruction Check (10 minutes)\npython 3_check_reconstruction.py\n\n# Step 4: Trajectory Generation (30 minutes)\npython 4_trajectories.py\n\n# Step 5: Classification (1 hour)\npython 5_classification.py\n\n# Step 6: Enrichment Analysis (2-6 hours with 8 cores)\npython 6_enrichment_analysis.py\n</code></pre>"},{"location":"tutorials/complete-pipeline/#option-2-run-with-makefile","title":"Option 2: Run with Makefile","text":"<pre><code># Run entire pipeline\nmake pipeline\n\n# Or run individual steps\nmake data\nmake models\nmake trajectories\nmake classification\nmake enrichment\n</code></pre>"},{"location":"tutorials/complete-pipeline/#option-3-use-job-scripts-hpc","title":"Option 3: Use Job Scripts (HPC)","text":"<p>If you're on an HPC cluster with SLURM:</p> <pre><code># Submit all jobs\nsbatch jobs/job_data\nsbatch jobs/job_models\nsbatch jobs/job_trajectories\nsbatch jobs/job_classification\nsbatch jobs/job_enrichment\n</code></pre>"},{"location":"tutorials/complete-pipeline/#detailed-step-by-step-guide","title":"Detailed Step-by-Step Guide","text":""},{"location":"tutorials/complete-pipeline/#step-1-data-processing","title":"Step 1: Data Processing","text":"<p>Script: <code>scripts/pipeline_steps/1_data_processing.py</code></p> <p>What it does: - Downloads TCGA KIRC RNA-seq and clinical data - Filters low-expression genes - Removes outliers using Mahalanobis distance - Creates train/test splits</p> <p>Expected outputs: <pre><code>data/interim/preprocessed_KIRC_data/\n\u251c\u2500\u2500 preprocessed_rnaseq.csv         # Filtered gene expression (498 samples \u00d7 ~5000 genes)\n\u251c\u2500\u2500 preprocessing_info.json         # Filter statistics\n\u2514\u2500\u2500 stages.csv                      # Cancer stage labels\n</code></pre></p> <p>Validation: <pre><code>import pandas as pd\n\nrnaseq = pd.read_csv('data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv', index_col=0)\nprint(f\"Samples: {rnaseq.shape[0]}, Genes: {rnaseq.shape[1]}\")\n# Expected: Samples: 498, Genes: ~5000\n\n# Check for missing values\nassert rnaseq.isnull().sum().sum() == 0, \"Missing values detected!\"\nprint(\"\u2713 Data quality check passed\")\n</code></pre></p> <p>Time: ~30 minutes See: Data Processing Tutorial</p>"},{"location":"tutorials/complete-pipeline/#step-2-vae-training","title":"Step 2: VAE Training","text":"<p>Script: <code>scripts/pipeline_steps/2_models.py</code></p> <p>What it does: - Trains Variational Autoencoder on gene expression - Learns low-dimensional latent representation - Trains reconstruction network for improved decoding - Saves models and training histories</p> <p>Configuration (in script): <pre><code>vae_config.INPUT_DIM = X_train.shape[1]  # Number of genes\nvae_config.MID_DIM = 512                  # Hidden layer size\nvae_config.LATENT_DIM = 256               # Latent space dimensionality\nvae_config.EPOCHS = 600                   # Total epochs (3 cycles \u00d7 200)\nvae_config.BATCH_SIZE = 8\n</code></pre></p> <p>Expected outputs: <pre><code>models/20251217_models_KIRC/\n\u251c\u2500\u2500 vae/\n\u2502   \u251c\u2500\u2500 vae_model.pt                 # Trained VAE\n\u2502   \u251c\u2500\u2500 vae_config.json              # Model configuration\n\u2502   \u2514\u2500\u2500 vae_training_history.png     # Loss curves\n\u251c\u2500\u2500 reconstruction/\n\u2502   \u251c\u2500\u2500 reconstruction_network.pt    # Post-processing network\n\u2502   \u2514\u2500\u2500 reconstruction_network_history.png\n\u2514\u2500\u2500 training_data/\n    \u251c\u2500\u2500 X_train.csv\n    \u2514\u2500\u2500 X_test.csv\n</code></pre></p> <p>Validation: <pre><code>import torch\nfrom renalprog.modeling.models import VAE\n\n# Load model\nvae = VAE.load('models/20251217_models_KIRC/vae/vae_model.pt')\n\n# Check reconstruction\nX_test = pd.read_csv('models/20251217_models_KIRC/training_data/X_test.csv', index_col=0)\nX_tensor = torch.FloatTensor(X_test.values)\n\nwith torch.no_grad():\n    X_recon = vae(X_tensor)[0]\n    mse = ((X_tensor - X_recon) ** 2).mean()\n    print(f\"Reconstruction MSE: {mse:.4f}\")\n    # Expected: MSE &lt; 1.0 for well-trained model\n</code></pre></p> <p>Time: 2-4 hours (GPU), 8-12 hours (CPU) See: VAE Training Tutorial</p>"},{"location":"tutorials/complete-pipeline/#step-3-reconstruction-validation","title":"Step 3: Reconstruction Validation","text":"<p>Script: <code>scripts/pipeline_steps/3_check_reconstruction.py</code></p> <p>What it does: - Visualizes latent space with UMAP/t-SNE - Compares original vs. reconstructed gene expression - Generates quality control plots</p> <p>Expected outputs: <pre><code>reports/figures/reconstruction/\n\u251c\u2500\u2500 latent_space_umap.png           # UMAP colored by stage\n\u251c\u2500\u2500 latent_space_tsne.png           # t-SNE colored by stage\n\u251c\u2500\u2500 reconstruction_scatter.png      # Original vs. reconstructed\n\u2514\u2500\u2500 gene_correlation.png            # Per-gene reconstruction quality\n</code></pre></p> <p>Validation: <pre><code># Check that plots exist\nimport os\n\nplot_dir = 'reports/figures/reconstruction/'\nrequired_plots = [\n    'latent_space_umap.png',\n    'reconstruction_scatter.png'\n]\n\nfor plot in required_plots:\n    assert os.path.exists(os.path.join(plot_dir, plot)), f\"Missing: {plot}\"\nprint(\"\u2713 All validation plots generated\")\n</code></pre></p> <p>Time: ~10 minutes See: Reconstruction Tutorial</p>"},{"location":"tutorials/complete-pipeline/#step-4-trajectory-generation","title":"Step 4: Trajectory Generation","text":"<p>Script: <code>scripts/pipeline_steps/4_trajectories.py</code></p> <p>What it does: - Generates synthetic patient trajectories - Interpolates between early and late stage samples - Decodes trajectories to gene expression space - Creates multiple trajectory types</p> <p>Configuration: <pre><code>n_trajectories = 500                # Number of trajectories to generate\nn_timepoints = 20                   # Timepoints per trajectory\ntrajectory_types = [\n    'early_to_late',                # Early stage \u2192 Late stage\n    'stage1_to_stage4',             # Stage I \u2192 Stage IV\n    'matched_pairs'                 # Patient-specific progressions\n]\n</code></pre></p> <p>Expected outputs: <pre><code>data/interim/20251217_synthetic_data/kirc/\n\u251c\u2500\u2500 early_to_late/\n\u2502   \u251c\u2500\u2500 TCGA-XXX-YYY_to_TCGA-AAA-BBB.csv\n\u2502   \u251c\u2500\u2500 TCGA-XXX-ZZZ_to_TCGA-CCC-DDD.csv\n\u2502   \u2514\u2500\u2500 ...                         # 500 trajectory files\n\u251c\u2500\u2500 trajectory_metadata.csv         # Start/end points, stages\n\u2514\u2500\u2500 generation_params.json          # Generation parameters\n</code></pre></p> <p>Validation: <pre><code>import pandas as pd\nimport glob\n\n# Count trajectories\ntrajectory_files = glob.glob('data/interim/*/kirc/early_to_late/*.csv')\nprint(f\"Generated {len(trajectory_files)} trajectories\")\n# Expected: 500 trajectories\n\n# Check trajectory format\ntraj = pd.read_csv(trajectory_files[0], index_col=0)\nprint(f\"Trajectory shape: {traj.shape}\")\n# Expected: (20 timepoints, ~5000 genes)\n\nassert traj.shape[0] == 20, \"Wrong number of timepoints\"\nassert traj.isnull().sum().sum() == 0, \"Missing values in trajectory\"\nprint(\"\u2713 Trajectory validation passed\")\n</code></pre></p> <p>Time: ~30 minutes See: Trajectory Tutorial</p>"},{"location":"tutorials/complete-pipeline/#step-5-classification","title":"Step 5: Classification","text":"<p>Script: <code>scripts/pipeline_steps/5_classification.py</code></p> <p>What it does: - Trains XGBoost classifier for stage prediction - Calculates SHAP values for interpretability - Identifies important gene signatures - Evaluates performance with cross-validation</p> <p>Expected outputs: <pre><code>models/20251217_classification_kirc/\n\u251c\u2500\u2500 classifier.pkl                  # Trained XGBoost model\n\u251c\u2500\u2500 classification_metrics.json     # Accuracy, ROC AUC, etc.\n\u251c\u2500\u2500 shap_values.npy                 # SHAP importance values\n\u251c\u2500\u2500 important_genes.csv             # Top biomarker genes\n\u2514\u2500\u2500 figures/\n    \u251c\u2500\u2500 confusion_matrix.png\n    \u251c\u2500\u2500 roc_curve.png\n    \u251c\u2500\u2500 shap_summary.png\n    \u2514\u2500\u2500 shap_waterfall.png\n</code></pre></p> <p>Validation: <pre><code>import json\n\n# Check performance metrics\nwith open('models/20251217_classification_kirc/classification_metrics.json') as f:\n    metrics = json.load(f)\n\nprint(f\"Test Accuracy: {metrics['test_accuracy']:.3f}\")\nprint(f\"ROC AUC: {metrics['test_roc_auc']:.3f}\")\n\n# Expected performance for KIRC dataset\nassert metrics['test_accuracy'] &gt; 0.85, \"Low accuracy!\"\nassert metrics['test_roc_auc'] &gt; 0.90, \"Low ROC AUC!\"\nprint(\"\u2713 Classification performance acceptable\")\n</code></pre></p> <p>Time: ~1 hour See: Classification Tutorial</p>"},{"location":"tutorials/complete-pipeline/#step-6-enrichment-analysis","title":"Step 6: Enrichment Analysis","text":"<p>Script: <code>scripts/pipeline_steps/6_enrichment_analysis.py</code></p> <p>What it does: - Calculates differential expression (DESeq-style) for each timepoint - Runs GSEA on ranked gene lists - Identifies enriched pathways along trajectories - Combines results into single dataset</p> <p>Prerequisites: - GSEA CLI tool installed (installation guide) - ReactomePathways.gmt in <code>data/external/</code></p> <p>Configuration: <pre><code>n_threads = 8                       # Parallel processing threads\ngsea_path = './GSEA_4.3.2/gsea-cli.sh'\npathways_file = 'data/external/ReactomePathways.gmt'\n</code></pre></p> <p>Expected outputs: <pre><code>data/processed/20251217_enrichment/\n\u251c\u2500\u2500 deseq/\n\u2502   \u2514\u2500\u2500 early_to_late/\n\u2502       \u251c\u2500\u2500 TCGA-XXX_to_YYY/\n\u2502       \u2502   \u251c\u2500\u2500 patient_tp0_foldchange.rnk\n\u2502       \u2502   \u251c\u2500\u2500 patient_tp1_foldchange.rnk\n\u2502       \u2502   \u2514\u2500\u2500 gsea_tp0/\n\u2502       \u2502       \u251c\u2500\u2500 gsea_report_for_na_pos_*.tsv\n\u2502       \u2502       \u2514\u2500\u2500 gsea_report_for_na_neg_*.tsv\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 trajectory_enrichment.csv       # Final combined results\n</code></pre></p> <p>Final result format: <pre><code>Patient,Idx,Transition,NAME,ES,NES,FDR q-val\nTCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01,0,early_to_late,Cell Cycle,0.65,2.13,0.001\nTCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01,0,early_to_late,DNA Repair,0.52,1.87,0.012\n...\n</code></pre></p> <p>Validation: <pre><code>import pandas as pd\n\n# Load enrichment results\nenrichment = pd.read_csv('data/processed/20251217_enrichment/trajectory_enrichment.csv')\n\nprint(f\"Total enrichment results: {len(enrichment)}\")\nprint(f\"Unique patients: {enrichment['Patient'].nunique()}\")\nprint(f\"Unique pathways: {enrichment['NAME'].nunique()}\")\n\n# Check for significant pathways\nsig_pathways = enrichment[enrichment['FDR q-val'] &lt; 0.05]\nprint(f\"Significant pathways (FDR &lt; 0.05): {len(sig_pathways)}\")\n\nassert len(enrichment) &gt; 0, \"No enrichment results!\"\nassert enrichment['Patient'].nunique() == 500, \"Missing trajectories!\"\nprint(\"\u2713 Enrichment analysis complete\")\n</code></pre></p> <p>Time: 2-6 hours (depends on CPU cores) See: Enrichment Tutorial</p>"},{"location":"tutorials/complete-pipeline/#checking-pipeline-outputs","title":"Checking Pipeline Outputs","text":"<p>After running the complete pipeline, verify all outputs:</p> <pre><code># Run validation script\npython scripts/validate_pipeline_outputs.py\n</code></pre> <p>Or manually check key files:</p> <pre><code>import os\nimport pandas as pd\n\n# Check each step's outputs\nchecks = {\n    'Step 1': 'data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv',\n    'Step 2': 'models/20251217_models_KIRC/vae/vae_model.pt',\n    'Step 3': 'reports/figures/reconstruction/latent_space_umap.png',\n    'Step 4': 'data/interim/20251217_synthetic_data/kirc/trajectory_metadata.csv',\n    'Step 5': 'models/20251217_classification_kirc/classifier.pkl',\n    'Step 6': 'data/processed/20251217_enrichment/trajectory_enrichment.csv',\n}\n\nfor step, filepath in checks.items():\n    exists = os.path.exists(filepath)\n    status = \"\u2713\" if exists else \"\u2717\"\n    print(f\"{status} {step}: {filepath}\")\n</code></pre>"},{"location":"tutorials/complete-pipeline/#expected-disk-usage","title":"Expected Disk Usage","text":"<p>After completing the pipeline:</p> <pre><code>data/raw/                    ~5 GB    (TCGA downloads)\ndata/interim/                ~15 GB   (trajectories + intermediate files)\ndata/processed/              ~10 GB   (enrichment results)\nmodels/                      ~1 GB    (trained models)\nreports/                     ~100 MB  (figures)\n-------------------------------------------\nTotal:                       ~31 GB\n</code></pre>"},{"location":"tutorials/complete-pipeline/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Typical runtimes on different systems:</p> System Step 2 Step 6 Total Config HPC Cluster 1 hr 1 hr 3 hrs 48 cores, A100 GPU Desktop (2023) 2 hrs 2 hrs 6 hrs 16 cores, RTX 4080 Desktop (2020) 3 hrs 4 hrs 9 hrs 8 cores, GTX 1080 Laptop (2018) 8 hrs 6 hrs 16 hrs 4 cores, CPU only"},{"location":"tutorials/complete-pipeline/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/complete-pipeline/#pipeline-fails-at-step-x","title":"Pipeline Fails at Step X","text":"<p>Resume from the failing step:</p> <pre><code># Fix the issue, then run from that step onward\npython scripts/pipeline_steps/X_step_name.py\n</code></pre>"},{"location":"tutorials/complete-pipeline/#out-of-disk-space","title":"Out of Disk Space","text":"<p>Clean up intermediate files:</p> <pre><code># Remove GSEA temporary files\nrm -rf data/processed/*/deseq/*/gsea_*\n\n# Remove old dated directories\nrm -rf data/interim/202412*  # Keep only latest\n</code></pre>"},{"location":"tutorials/complete-pipeline/#out-of-memory","title":"Out of Memory","text":"<p>Reduce batch sizes and threads:</p> <pre><code># In 2_models.py\nvae_config.BATCH_SIZE = 4  # Reduce from 8\n\n# In 6_enrichment_analysis.py\nn_threads = 2  # Reduce from 8\n</code></pre>"},{"location":"tutorials/complete-pipeline/#next-steps","title":"Next Steps","text":"<p>After completing the pipeline:</p> <ol> <li>Analyze Results: Explore the enrichment results</li> <li>Create Visualizations: Generate publication figures</li> <li>Interpret Pathways: Map significant pathways to biology</li> <li>Validate Findings: Test predictions experimentally</li> </ol> <p>See: - Visualization Tutorial - Result Interpretation Guide - API Reference for custom analyses</p>"},{"location":"tutorials/complete-pipeline/#citation","title":"Citation","text":"<p>If you use this pipeline in your research, please cite:</p> <pre><code>@software{renalprog2024,\n  author = {Prol-Castelo, Guillermo},\n  title = {renalprog: Cancer Progression Forecasting with Generative AI},\n  year = {2024},\n  url = {https://github.com/gprolcastelo/renalprog}\n}\n</code></pre>"},{"location":"tutorials/data-requirements/","title":"Data Requirements","text":"<p>Understanding the input data formats and requirements for <code>renalprog</code>.</p>"},{"location":"tutorials/data-requirements/#overview","title":"Overview","text":"<p><code>renalprog</code> is designed to work with gene expression data and clinical annotations. The primary use case is TCGA (The Cancer Genome Atlas) data, but the package can work with any properly formatted RNA-seq or microarray data.</p>"},{"location":"tutorials/data-requirements/#input-data-formats","title":"Input Data Formats","text":""},{"location":"tutorials/data-requirements/#1-gene-expression-data","title":"1. Gene Expression Data","text":"<p>Format: CSV or TSV file with genes as columns and samples as rows (or vice versa).</p> <p>Example (<code>rnaseq.csv</code>): <pre><code>,Gene1,Gene2,Gene3,...,GeneN\nSample1,12.5,8.3,0.1,...,5.7\nSample2,13.1,7.9,0.0,...,6.2\nSample3,11.8,9.2,0.3,...,5.1\n...\n</code></pre></p> <p>Requirements:</p> <ul> <li>Samples: Minimum 50, recommended 200+</li> <li>Genes: Minimum 1000, recommended 5000+</li> <li>Values: </li> <li>Can be raw counts, TPM, FPKM, or log-transformed</li> <li>Will be preprocessed by the pipeline</li> <li>Must be numeric (no missing values in final data)</li> <li>Index: Sample IDs as row index OR column names</li> <li>Columns: Gene IDs or symbols</li> </ul> <p>Supported Formats:</p> <ul> <li>\u2705 Samples \u00d7 Genes (preferred)</li> <li>\u2705 Genes \u00d7 Samples (will be transposed)</li> <li>\u2705 Log-transformed or linear scale</li> <li>\u2705 Normalized or unnormalized (pipeline handles normalization)</li> </ul>"},{"location":"tutorials/data-requirements/#2-clinical-data","title":"2. Clinical Data","text":"<p>Format: CSV or TSV file with samples as rows and clinical variables as columns.</p> <p>Example (<code>clinical.csv</code>): <pre><code>sample_id,ajcc_pathologic_tumor_stage,age,gender,survival_days\nSample1,Stage I,65,M,1825\nSample2,Stage IIIA,72,F,730\nSample3,Stage II,58,M,2190\n...\n</code></pre></p> <p>Required Columns:</p> <ul> <li><code>sample_id</code>: Must match gene expression sample IDs</li> <li><code>ajcc_pathologic_tumor_stage</code>: Cancer stage (or similar staging variable)</li> <li>Examples: \"Stage I\", \"Stage II\", \"Stage IIIA\", \"Stage IV\"</li> <li>Or: \"T1N0M0\", \"T2N1M0\", etc.</li> </ul> <p>Optional Columns (highly recommended):</p> <ul> <li><code>age</code>: Patient age at diagnosis</li> <li><code>gender</code> or <code>sex</code>: M/F or Male/Female</li> <li><code>survival_days</code>: Days of follow-up</li> <li><code>vital_status</code>: Alive/Dead or 0/1</li> <li><code>tumor_grade</code>: G1, G2, G3, G4</li> <li><code>histological_type</code>: Tumor histology</li> </ul> <p>Stage Format:</p> <p>The package can handle various stage formats:</p> <pre><code># AJCC staging\n\"Stage I\", \"Stage IA\", \"Stage IB\"\n\"Stage II\", \"Stage IIA\", \"Stage IIB\"\n\"Stage III\", \"Stage IIIA\", \"Stage IIIB\", \"Stage IIIC\"\n\"Stage IV\", \"Stage IVA\", \"Stage IVB\"\n\n# TNM staging\n\"T1N0M0\", \"T2N1M0\", \"T3N2M1\"\n\n# Simplified\n\"Early\", \"Late\"\n\"Low\", \"High\"\n</code></pre> <p>The pipeline automatically groups stages:</p> <ul> <li>Early: Stage I, II</li> <li>Late: Stage III, IV</li> </ul>"},{"location":"tutorials/data-requirements/#3-sample-metadata-optional","title":"3. Sample Metadata (Optional)","text":"<p>Format: CSV file with additional sample annotations.</p> <p>Example (<code>phenotype.csv</code>): <pre><code>sample_id,sample_type,tissue_source_site,batch,platform\nSample1,Primary Tumor,TCGA-XX,Batch1,IlluminaHiSeq\nSample2,Primary Tumor,TCGA-YY,Batch1,IlluminaHiSeq\nSample3,Primary Tumor,TCGA-ZZ,Batch2,IlluminaHiSeq\n...\n</code></pre></p> <p>Useful for: - Batch effect analysis - Sample type filtering - Quality control</p>"},{"location":"tutorials/data-requirements/#tcga-data","title":"TCGA Data","text":""},{"location":"tutorials/data-requirements/#downloading-tcga-data","title":"Downloading TCGA Data","text":"<p>The package includes utilities to download TCGA data from UCSC Xena Browser:</p> <pre><code>from renalprog import dataset\n\n# Download KIRC data\nrnaseq, clinical, pheno = dataset.download_data(\n    destination='data/raw',\n    cancer_type='KIRC',\n    remove_gz=True\n)\n</code></pre>"},{"location":"tutorials/data-requirements/#tcga-data-structure","title":"TCGA Data Structure","text":"<p>TCGA data includes:</p> <ol> <li>Gene Expression (20,531 genes initially)</li> <li>Platform: Illumina HiSeq</li> <li>Normalization: RSEM + upper quartile normalized</li> <li> <p>File: <code>EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena</code></p> </li> <li> <p>Clinical Data (survival and staging)</p> </li> <li>Curated clinical annotations</li> <li> <p>File: <code>Survival_SupplementalTable_S1_20171025_xena_sp</code></p> </li> <li> <p>Phenotype (sample metadata)</p> </li> <li>Batch information</li> <li>Sample types (primary tumor, normal, metastatic)</li> <li>File: <code>TCGA_phenotype_denseDataOnlyDownload.tsv</code></li> </ol>"},{"location":"tutorials/data-requirements/#tcga-sample-ids","title":"TCGA Sample IDs","text":"<p>TCGA uses barcodes like: <pre><code>TCGA-A3-3308-01A-01R-0864-07\n\u2502    \u2502  \u2502    \u2502 \u2502 \u2502  \u2502    \u2502\n\u2502    \u2502  \u2502    \u2502 \u2502 \u2502  \u2502    \u2514\u2500 Sequencing center\n\u2502    \u2502  \u2502    \u2502 \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500 Plate\n\u2502    \u2502  \u2502    \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  RNA/DNA type\n\u2502    \u2502  \u2502    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Aliquot\n\u2502    \u2502  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Sample type (01=Primary, 11=Normal)\n\u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Patient ID\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Tissue source site\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Project (TCGA)\n</code></pre></p> <p>The package handles these automatically.</p>"},{"location":"tutorials/data-requirements/#custom-non-tcga-data","title":"Custom (Non-TCGA) Data","text":""},{"location":"tutorials/data-requirements/#preparing-your-own-data","title":"Preparing Your Own Data","text":"<p>If you have your own gene expression data:</p>"},{"location":"tutorials/data-requirements/#step-1-format-gene-expression","title":"Step 1: Format Gene Expression","text":"<pre><code>import pandas as pd\n\n# Load your data\n# Assume you have a samples \u00d7 genes matrix\nyour_data = pd.read_csv('your_rnaseq_data.csv', index_col=0)\n\n# Ensure proper format\nassert your_data.shape[0] &lt; your_data.shape[1], \"Transpose if needed\"\n# Should have more genes than samples\n\n# Check for missing values\nassert not your_data.isnull().any().any(), \"Remove missing values\"\n\n# Save in standard format\nyour_data.to_csv('data/raw/your_data_formatted.csv')\n</code></pre>"},{"location":"tutorials/data-requirements/#step-2-format-clinical-data","title":"Step 2: Format Clinical Data","text":"<pre><code># Create clinical file\nclinical = pd.DataFrame({\n    'sample_id': your_data.index,\n    'ajcc_pathologic_tumor_stage': your_stages,  # e.g., [\"Stage I\", \"Stage III\", ...]\n    'age': your_ages,\n    'gender': your_gender\n})\n\n# Map to early/late if needed\nstage_map = {\n    'Stage I': 'Early',\n    'Stage II': 'Early',\n    'Stage III': 'Late',\n    'Stage IV': 'Late'\n}\nclinical['stage_binary'] = clinical['ajcc_pathologic_tumor_stage'].map(stage_map)\n\nclinical.to_csv('data/raw/your_clinical_formatted.csv', index=False)\n</code></pre>"},{"location":"tutorials/data-requirements/#step-3-validate","title":"Step 3: Validate","text":"<pre><code>from renalprog import dataset\n\n# Try loading\nrnaseq = dataset.load_data('data/raw/your_data_formatted.csv')\nclinical = dataset.load_data('data/raw/your_clinical_formatted.csv')\n\n# Check alignment\nassert set(rnaseq.index) == set(clinical['sample_id']), \"Sample IDs must match!\"\n\nprint(f\"Samples: {rnaseq.shape[0]}\")\nprint(f\"Genes: {rnaseq.shape[1]}\")\nprint(f\"Stages: {clinical['ajcc_pathologic_tumor_stage'].value_counts()}\")\n</code></pre>"},{"location":"tutorials/data-requirements/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"tutorials/data-requirements/#automatic-preprocessing","title":"Automatic Preprocessing","text":"<p>The pipeline automatically:</p> <ol> <li>Filters low-expression genes</li> <li>Removes genes with low mean expression</li> <li>Removes genes with low variance</li> <li> <p>Keeps genes expressed in \u226520% of samples</p> </li> <li> <p>Detects outliers</p> </li> <li>Uses Mahalanobis distance</li> <li>Removes samples &gt;3 SD from mean</li> <li> <p>Configurable significance level</p> </li> <li> <p>Normalizes (optional)</p> </li> <li>Log transformation</li> <li>Z-score normalization</li> <li>Quantile normalization</li> </ol>"},{"location":"tutorials/data-requirements/#preprocessing-parameters","title":"Preprocessing Parameters","text":"<pre><code>from renalprog import features\n\nprocessed, info = features.preprocess_rnaseq(\n    data=rnaseq,\n    filter_expression=True,\n    mean_threshold=0.5,        # Minimum mean expression\n    var_threshold=0.5,         # Minimum variance\n    min_sample_fraction=0.2,   # Gene must be in \u226520% samples\n    detect_outliers=True,\n    alpha=0.05,                # Outlier detection significance\n    seed=2023\n)\n\n# Check what was filtered\nprint(f\"Original: {rnaseq.shape[1]} genes\")\nprint(f\"After filtering: {processed.shape[1]} genes\")\nprint(f\"Removed: {info['n_outliers']} outlier samples\")\n</code></pre>"},{"location":"tutorials/data-requirements/#quality-control","title":"Quality Control","text":""},{"location":"tutorials/data-requirements/#check-data-quality","title":"Check Data Quality","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Load data\nrnaseq = pd.read_csv('data/processed/rnaseq.csv', index_col=0)\n\n# 1. Check for missing values\nmissing = rnaseq.isnull().sum().sum()\nprint(f\"Missing values: {missing}\")\nassert missing == 0, \"Data contains missing values!\"\n\n# 2. Check for duplicates\nduplicates = rnaseq.index.duplicated().sum()\nprint(f\"Duplicate samples: {duplicates}\")\nassert duplicates == 0, \"Duplicate samples found!\"\n\n# 3. Check value range\nprint(f\"Min value: {rnaseq.min().min()}\")\nprint(f\"Max value: {rnaseq.max().max()}\")\nprint(f\"Mean expression: {rnaseq.mean().mean()}\")\n\n# 4. Check distribution\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Overall distribution\nrnaseq.values.flatten().hist(bins=50, ax=axes[0])\naxes[0].set_title('Expression Value Distribution')\naxes[0].set_xlabel('Expression')\n\n# Per-sample means\nrnaseq.mean(axis=1).hist(bins=30, ax=axes[1])\naxes[1].set_title('Per-Sample Mean Expression')\naxes[1].set_xlabel('Mean Expression')\n\nplt.savefig('reports/figures/data_quality.png')\n</code></pre>"},{"location":"tutorials/data-requirements/#sample-size-guidelines","title":"Sample Size Guidelines","text":"Analysis Type Minimum Samples Recommended Optimal VAE Training 50 200 500+ Classification 100 300 1000+ Trajectory Generation 20 per stage 50 per stage 100+ per stage Enrichment Analysis N/A N/A N/A (uses trajectories) <p>Note: More samples generally improve results, but you need balanced stages:</p> <pre><code># Check stage balance\nclinical['ajcc_pathologic_tumor_stage'].value_counts()\n\n# Should have:\n# Early stages: \u226525% of samples\n# Late stages: \u226525% of samples\n</code></pre>"},{"location":"tutorials/data-requirements/#example-datasets","title":"Example Datasets","text":""},{"location":"tutorials/data-requirements/#kirc-kidney-renal-clear-cell-carcinoma","title":"KIRC (Kidney Renal Clear Cell Carcinoma)","text":"<p>The main dataset used in publication:</p> <ul> <li>Samples: 534 (498 after filtering)</li> <li>Genes: 20,531 (5,127 after filtering)</li> <li>Stages: I (216), II (50), III (120), IV (80)</li> <li>Source: TCGA via UCSC Xena</li> </ul>"},{"location":"tutorials/data-requirements/#other-tcga-cancers","title":"Other TCGA Cancers","text":"<p>The pipeline works with other TCGA cancers:</p> <pre><code># Download different cancer type\nrnaseq, clinical, pheno = dataset.download_data(\n    destination='data/raw',\n    cancer_type='BRCA'  # or LUAD, COAD, etc.\n)\n</code></pre> <p>Supported: BRCA, LUAD, LUSC, COAD, STAD, LIHC, PRAD, etc.</p>"},{"location":"tutorials/data-requirements/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/data-requirements/#sample-ids-dont-match","title":"\"Sample IDs don't match\"","text":"<pre><code># Find mismatches\nrnaseq_ids = set(rnaseq.index)\nclinical_ids = set(clinical['sample_id'])\n\nonly_in_rnaseq = rnaseq_ids - clinical_ids\nonly_in_clinical = clinical_ids - rnaseq_ids\n\nprint(f\"Only in RNA-seq: {len(only_in_rnaseq)}\")\nprint(f\"Only in clinical: {len(only_in_clinical)}\")\n\n# Keep only matching samples\ncommon_ids = rnaseq_ids &amp; clinical_ids\nrnaseq = rnaseq.loc[common_ids]\nclinical = clinical[clinical['sample_id'].isin(common_ids)]\n</code></pre>"},{"location":"tutorials/data-requirements/#too-few-genes-after-filtering","title":"\"Too few genes after filtering\"","text":"<pre><code># Relax filtering thresholds\nprocessed, info = features.preprocess_rnaseq(\n    data=rnaseq,\n    mean_threshold=0.1,     # Lower threshold\n    var_threshold=0.1,      # Lower threshold\n    min_sample_fraction=0.1 # Require expression in only 10% samples\n)\n</code></pre>"},{"location":"tutorials/data-requirements/#data-looks-weird","title":"\"Data looks weird\"","text":"<pre><code># Check if data needs transformation\nprint(\"Min:\", rnaseq.min().min())\nprint(\"Max:\", rnaseq.max().max())\n\n# If values are large (&gt;100), likely not log-transformed\nif rnaseq.max().max() &gt; 100:\n    rnaseq_log = np.log2(rnaseq + 1)\n    rnaseq = rnaseq_log\n</code></pre>"},{"location":"tutorials/data-requirements/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Tutorial</li> <li>Data Processing Pipeline</li> <li>API Reference</li> </ul>"},{"location":"tutorials/pretrained-models/","title":"Using Pretrained Models","text":"<p>This guide shows you how to use the pretrained VAE models provided with the RenalProg package to reproduce the results from the paper.</p>"},{"location":"tutorials/pretrained-models/#overview","title":"Overview","text":"<p>The repository includes pretrained models for two cancer types:</p> <ul> <li>KIRC (Kidney Renal Clear Cell Carcinoma)</li> <li>BRCA (Breast Invasive Carcinoma)</li> </ul> <p>Each pretrained model comes with:</p> <ul> <li>Trained VAE weights</li> <li>Reconstruction network weights</li> <li>Network architecture specifications</li> </ul>"},{"location":"tutorials/pretrained-models/#pretrained-model-structure","title":"Pretrained Model Structure","text":"<pre><code>models/pretrained/\n\u251c\u2500\u2500 KIRC/\n\u2502   \u251c\u2500\u2500 20250321_VAE_idim8516_md512_feat256mse_relu.pth  # VAE weights\n\u2502   \u251c\u2500\u2500 network_reconstruction.pth                       # Reconstruction network weights\n\u2502   \u2514\u2500\u2500 network_dims.csv                                 # Architecture specifications\n\u2514\u2500\u2500 BRCA/\n    \u251c\u2500\u2500 20251209_VAE_idim8954_md1024_feat512mse_relu.pth\n    \u251c\u2500\u2500 network_reconstruction.pth\n    \u2514\u2500\u2500 network_dims.csv\n</code></pre>"},{"location":"tutorials/pretrained-models/#preprocessed-data","title":"Preprocessed Data","text":"<p>The preprocessed data for each cancer type is available in:</p> <pre><code>data/interim/\n\u251c\u2500\u2500 preprocessed_KIRC/\n\u2502   \u251c\u2500\u2500 clinical.csv    # Patient metadata (stages, demographics)\n\u2502   \u2514\u2500\u2500 rnaseq.csv      # Preprocessed RNA-seq expression data\n\u2514\u2500\u2500 preprocessed_BRCA/\n    \u251c\u2500\u2500 clinical.csv\n    \u2514\u2500\u2500 rnaseq.csv\n</code></pre>"},{"location":"tutorials/pretrained-models/#data-format","title":"Data Format","text":"<p>clinical.csv: Patient metadata with columns: - Index: Patient IDs (e.g., <code>TCGA-A3-3319-01</code>) - <code>ajcc_pathologic_tumor_stage</code>: Cancer stage (Stage I, II, III, IV) - Additional clinical features (varies by cancer type)</p> <p>rnaseq.csv: Gene expression data - Index: Gene symbols (e.g., <code>TSPAN6</code>, <code>TNMD</code>) - Columns: Patient IDs - Values: Log-transformed normalized gene expression</p>"},{"location":"tutorials/pretrained-models/#quick-start","title":"Quick Start","text":""},{"location":"tutorials/pretrained-models/#1-generate-trajectories-from-pretrained-models","title":"1. Generate Trajectories from Pretrained Models","text":"<p>Use the provided script to generate patient trajectories:</p> <pre><code># For KIRC\npython scripts/pipeline_steps/use_pretrained_model.py \\\n    --cancer_type KIRC \\\n    --model_dir models/pretrained/KIRC \\\n    --data_dir data/interim/preprocessed_KIRC \\\n    --output_dir data/processed/trajectories_KIRC_pretrained\n\n# For BRCA\npython scripts/pipeline_steps/use_pretrained_model.py \\\n    --cancer_type BRCA \\\n    --model_dir models/pretrained/BRCA \\\n    --data_dir data/interim/preprocessed_BRCA \\\n    --output_dir data/processed/trajectories_BRCA_pretrained\n</code></pre>"},{"location":"tutorials/pretrained-models/#2-run-enrichment-analysis","title":"2. Run Enrichment Analysis","text":"<p>After generating trajectories, run pathway enrichment analysis:</p> <pre><code># For KIRC\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/processed/trajectories_KIRC_pretrained/early_to_late/test_to_test \\\n    --output_dir data/processed/enrichment_KIRC_pretrained \\\n    --n_threads 8 \\\n    --gsea_path ./GSEA_4.3.2/gsea-cli.sh \\\n    --pathways_file data/external/ReactomePathways.gmt\n\n# For BRCA\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/processed/trajectories_BRCA_pretrained/early_to_late/test_to_test \\\n    --output_dir data/processed/enrichment_BRCA_pretrained \\\n    --n_threads 8 \\\n    --gsea_path ./GSEA_4.3.2/gsea-cli.sh \\\n    --pathways_file data/external/ReactomePathways.gmt\n</code></pre>"},{"location":"tutorials/pretrained-models/#3-generate-pathway-heatmaps","title":"3. Generate Pathway Heatmaps","text":"<p>Create visualizations of pathway enrichment:</p> <pre><code># For KIRC\npython scripts/pipeline_steps/6b_generate_pathway_heatmap.py \\\n    --enrichment_file data/processed/enrichment_KIRC_pretrained/trajectory_enrichment.csv \\\n    --output_dir data/processed/enrichment_KIRC_pretrained \\\n    --fdr_threshold 0.05\n\n# For BRCA\npython scripts/pipeline_steps/6b_generate_pathway_heatmap.py \\\n    --enrichment_file data/processed/enrichment_BRCA_pretrained/trajectory_enrichment.csv \\\n    --output_dir data/processed/enrichment_BRCA_pretrained \\\n    --fdr_threshold 0.05\n</code></pre>"},{"location":"tutorials/pretrained-models/#advanced-usage","title":"Advanced Usage","text":""},{"location":"tutorials/pretrained-models/#loading-pretrained-models-in-python","title":"Loading Pretrained Models in Python","text":"<p>You can load and use the pretrained models directly in your Python code:</p> <pre><code>import torch\nimport pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE, NetworkReconstruction\nfrom renalprog.utils import get_device\n\n# Configuration\ncancer_type = \"KIRC\"\nmodel_dir = Path(\"models/pretrained/KIRC\")\ndata_dir = Path(\"data/interim/preprocessed_KIRC\")\n\n# Load network architecture\nnetwork_dims = pd.read_csv(model_dir / \"network_dims.csv\", index_col=0)\ninput_dim = int(network_dims['in_dim'].values[0])\nlayer1_dim = int(network_dims['layer1_dim'].values[0])\nlayer2_dim = int(network_dims['layer2_dim'].values[0])\nlayer3_dim = int(network_dims['layer3_dim'].values[0])\n\n# Initialize VAE model\n# For KIRC: mid_dim=512, latent_dim=256\n# For BRCA: mid_dim=1024, latent_dim=512\ndevice = get_device(force_cpu=False)\n\nif cancer_type == \"KIRC\":\n    vae_model = VAE(\n        input_dim=input_dim,\n        mid_dim=512,\n        latent_dim=256,\n        loss_fn='mse',\n        activation='relu'\n    ).to(device)\nelse:  # BRCA\n    vae_model = VAE(\n        input_dim=input_dim,\n        mid_dim=1024,\n        latent_dim=512,\n        loss_fn='mse',\n        activation='relu'\n    ).to(device)\n\n# Load VAE weights\nvae_weights = list(model_dir.glob(\"*VAE_*.pth\"))[0]\nvae_model.load_state_dict(torch.load(vae_weights, map_location=device))\nvae_model.eval()\n\n# Initialize reconstruction network\nreconstruction_network = NetworkReconstruction(\n    layer_dims=[input_dim, layer1_dim, layer2_dim, layer3_dim, input_dim]\n).to(device)\n\n# Load reconstruction network weights\nreconstruction_network.load_state_dict(\n    torch.load(model_dir / \"network_reconstruction.pth\", map_location=device)\n)\nreconstruction_network.eval()\n\n# Load data\nrnaseq = pd.read_csv(data_dir / \"rnaseq.csv\", index_col=0)\nclinical = pd.read_csv(data_dir / \"clinical.csv\", index_col=0)\n\n# Generate latent representations\nwith torch.no_grad():\n    data_tensor = torch.FloatTensor(rnaseq.T.values).to(device)\n    _, mu, logvar = vae_model(data_tensor)\n    latent_reps = mu.cpu().numpy()\n\nprint(f\"Generated latent representations: {latent_reps.shape}\")\n</code></pre>"},{"location":"tutorials/pretrained-models/#generating-custom-trajectories","title":"Generating Custom Trajectories","text":"<p>You can customize trajectory generation parameters:</p> <pre><code>from renalprog.modeling.predict import (\n    calculate_all_possible_transitions,\n    link_patients_random,\n    build_trajectory_network,\n    generate_trajectory_data\n)\n\n# Calculate transitions\nall_traj = calculate_all_possible_transitions(\n    data=rnaseq,\n    metadata_selection=clinical[['stage']],\n    start_stage='early',\n    end_stage='late',\n    link_next=5,  # Number of nearest neighbors\n    distance_metric='wasserstein'\n)\n\n# Build trajectory network\ntrajectory_network = build_trajectory_network(\n    all_traj=all_traj,\n    seed=2023\n)\n\n# Generate synthetic trajectories\ngenerate_trajectory_data(\n    data=rnaseq,\n    metadata=clinical,\n    trajectory_network=trajectory_network,\n    vae_model=vae_model,\n    reconstruction_network=reconstruction_network,\n    n_timepoints=50,\n    output_dir=output_dir / \"trajectories\",\n    interpolation_method='linear',\n    device=device\n)\n</code></pre>"},{"location":"tutorials/pretrained-models/#model-specifications","title":"Model Specifications","text":""},{"location":"tutorials/pretrained-models/#kirc-pretrained-model","title":"KIRC Pretrained Model","text":"<ul> <li>VAE Architecture:</li> <li>Input dimension: 8,516 genes</li> <li>Middle dimension: 512</li> <li>Latent dimension: 256</li> <li>Loss function: MSE</li> <li> <p>Activation: ReLU</p> </li> <li> <p>Reconstruction Network:</p> </li> <li> <p>Architecture: [8,954, 3,104, 790, 4,027, 8,954]</p> </li> <li> <p>Training Details:</p> </li> <li>Beta-VAE with 3 cycles</li> <li>600 total epochs</li> <li>Batch size: 8</li> </ul>"},{"location":"tutorials/pretrained-models/#brca-pretrained-model","title":"BRCA Pretrained Model","text":"<ul> <li>VAE Architecture:</li> <li>Input dimension: 8,954 genes</li> <li>Middle dimension: 1,024</li> <li>Latent dimension: 512</li> <li>Loss function: MSE</li> <li> <p>Activation: ReLU</p> </li> <li> <p>Reconstruction Network:</p> </li> <li> <p>Architecture: [8,954, 3,104, 790, 4,027, 8,954]</p> </li> <li> <p>Training Details:</p> </li> <li>Beta-VAE with 3 cycles</li> <li>600 total epochs</li> <li>Batch size: 8</li> </ul>"},{"location":"tutorials/pretrained-models/#reproducing-paper-results","title":"Reproducing Paper Results","text":"<p>To reproduce the exact results from the paper:</p> <ol> <li>Use the pretrained models (recommended for exact reproduction)</li> <li>Follow the complete pipeline:</li> </ol> <pre><code># 1. Generate trajectories\npython scripts/pipeline_steps/use_pretrained_model.py \\\n    --cancer_type KIRC \\\n    --model_dir models/pretrained/KIRC \\\n    --data_dir data/interim/preprocessed_KIRC \\\n    --output_dir data/processed/paper_reproduction_KIRC\n\n# 2. Run enrichment analysis\npython scripts/pipeline_steps/6_enrichment_analysis.py \\\n    --trajectory_dir data/processed/paper_reproduction_KIRC/early_to_late/test_to_test \\\n    --output_dir data/processed/enrichment_paper_KIRC \\\n    --n_threads 8 \\\n    --gsea_path ./GSEA_4.3.2/gsea-cli.sh \\\n    --pathways_file data/external/ReactomePathways.gmt\n\n# 3. Generate heatmaps\npython scripts/pipeline_steps/6b_generate_pathway_heatmap.py \\\n    --enrichment_file data/processed/enrichment_paper_KIRC/trajectory_enrichment.csv \\\n    --output_dir data/processed/enrichment_paper_KIRC \\\n    --fdr_threshold 0.05\n</code></pre>"},{"location":"tutorials/pretrained-models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/pretrained-models/#gpu-memory-issues","title":"GPU Memory Issues","text":"<p>If you encounter GPU memory issues, force CPU usage:</p> <pre><code>device = get_device(force_cpu=True)\n</code></pre>"},{"location":"tutorials/pretrained-models/#missing-dependencies","title":"Missing Dependencies","text":"<p>Ensure all dependencies are installed:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"tutorials/pretrained-models/#data-compatibility","title":"Data Compatibility","text":"<p>Ensure your input data matches the expected format: - Gene expression data should be log-transformed - Clinical data should include <code>ajcc_pathologic_tumor_stage</code> column - Patient IDs should match between rnaseq.csv and clinical.csv</p>"},{"location":"tutorials/pretrained-models/#next-steps","title":"Next Steps","text":"<ul> <li>Complete Pipeline Tutorial</li> <li>Enrichment Analysis Guide</li> <li>API Reference</li> </ul>"},{"location":"tutorials/pretrained-models/#citation","title":"Citation","text":"<p>If you use these pretrained models, please cite:</p> <pre><code>@article{renalprog2024,\n  title={Deep Learning Analysis of Cancer Progression Trajectories},\n  author={Your Name et al.},\n  journal={In Preparation},\n  year={2024},\n  note={Preprint available soon}\n}\n</code></pre>"},{"location":"tutorials/quickstart/","title":"Quick Start Tutorial","text":"<p>Get started with <code>renalprog</code> in 10 minutes! This tutorial demonstrates the core functionality using example data.</p>"},{"location":"tutorials/quickstart/#installation","title":"Installation","text":"<p>If you haven't installed <code>renalprog</code> yet:</p> <pre><code># Clone repository\ngit clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\n\n# Create environment with mamba/conda\nmamba env create -f environment.yml\nmamba activate renalprog\n\n# Install package\npip install -e .\n</code></pre> <p>See the Installation Guide for detailed instructions.</p>"},{"location":"tutorials/quickstart/#quick-example","title":"Quick Example","text":"<p>Let's run a minimal example that demonstrates the complete pipeline:</p>"},{"location":"tutorials/quickstart/#1-import-required-modules","title":"1. Import Required Modules","text":"<pre><code>from renalprog import config, dataset, features, modeling, plots\nfrom renalprog.modeling import VAE, generate_trajectories\nimport pandas as pd\nimport numpy as np\nimport torch\n</code></pre>"},{"location":"tutorials/quickstart/#2-load-example-data","title":"2. Load Example Data","text":"<pre><code># Load preprocessed KIRC data\nrnaseq_path = 'data/processed/rnaseq_maha.csv'\nclinical_path = 'data/processed/clinical.csv'\n\n# Load RNA-seq data\nrnaseq = pd.read_csv(rnaseq_path, index_col=0)\nclinical = pd.read_csv(clinical_path, index_col=0)\n\nprint(f\"RNA-seq data shape: {rnaseq.shape}\")\nprint(f\"Samples: {rnaseq.shape[0]}, Genes: {rnaseq.shape[1]}\")\n</code></pre> <p>Expected output: <pre><code>RNA-seq data shape: (498, 5000)\nSamples: 498, Genes: 5000\n</code></pre></p>"},{"location":"tutorials/quickstart/#3-prepare-data-for-vae","title":"3. Prepare Data for VAE","text":"<pre><code># Select important genes (optional - for faster training)\nn_genes = 1000  # Use top 1000 most variable genes\ngene_vars = rnaseq.var(axis=0)\ntop_genes = gene_vars.nlargest(n_genes).index\nrnaseq_subset = rnaseq[top_genes]\n\n# Convert to PyTorch tensor\nX = torch.FloatTensor(rnaseq_subset.values)\nprint(f\"Training data shape: {X.shape}\")\n</code></pre>"},{"location":"tutorials/quickstart/#4-train-a-simple-vae","title":"4. Train a Simple VAE","text":"<pre><code># Initialize VAE\nvae = VAE(\n    input_dim=X.shape[1],\n    latent_dim=10,\n    hidden_dims=[256, 128],\n    seed=42\n)\n\n# Train\nvae.fit(\n    X,\n    epochs=50,\n    batch_size=32,\n    learning_rate=1e-3,\n    verbose=True\n)\n</code></pre> <p>Expected output: <pre><code>Epoch [10/50], Loss: 1234.56\nEpoch [20/50], Loss: 987.65\nEpoch [30/50], Loss: 765.43\nEpoch [40/50], Loss: 654.32\nEpoch [50/50], Loss: 598.21\nTraining complete!\n</code></pre></p> <p>Training Time</p> <p>On a modern CPU, this should take 2-5 minutes. With a GPU, it takes under 1 minute.</p>"},{"location":"tutorials/quickstart/#5-visualize-latent-space","title":"5. Visualize Latent Space","text":"<pre><code># Encode data to latent space\nwith torch.no_grad():\n    latent = vae.encode(X).numpy()\n\n# Create DataFrame with latent coordinates and stages\nlatent_df = pd.DataFrame(\n    latent,\n    index=rnaseq_subset.index,\n    columns=[f'Z{i}' for i in range(latent.shape[1])]\n)\nlatent_df['stage'] = clinical.loc[latent_df.index, 'ajcc_pathologic_tumor_stage']\n\n# Plot with UMAP\nfrom renalprog.plots import plot_latent_space\n\nplot_latent_space(\n    latent_df,\n    color_by='stage',\n    method='umap',\n    save_path='reports/figures/quickstart_latent_umap.png'\n)\n</code></pre>"},{"location":"tutorials/quickstart/#6-generate-a-trajectory","title":"6. Generate a Trajectory","text":"<pre><code># Select early and late stage samples\nearly_samples = clinical[clinical['stage_binary'] == 'Early'].index\nlate_samples = clinical[clinical['stage_binary'] == 'Late'].index\n\n# Get latent representations\nearly_latent = latent[rnaseq_subset.index.isin(early_samples)]\nlate_latent = latent[rnaseq_subset.index.isin(late_samples)]\n\n# Pick random start and end points\nstart_point = early_latent[0]\nend_point = late_latent[0]\n\n# Generate trajectory (linear interpolation in latent space)\nn_steps = 10\ntrajectory_latent = np.linspace(start_point, end_point, n_steps)\n\n# Decode back to gene expression\ntrajectory_latent_tensor = torch.FloatTensor(trajectory_latent)\nwith torch.no_grad():\n    trajectory_expression = vae.decode(trajectory_latent_tensor).numpy()\n\n# Create DataFrame\ntrajectory_df = pd.DataFrame(\n    trajectory_expression,\n    columns=top_genes,\n    index=[f'timepoint_{i}' for i in range(n_steps)]\n)\n\nprint(f\"Generated trajectory with {n_steps} timepoints\")\nprint(trajectory_df.head())\n</code></pre>"},{"location":"tutorials/quickstart/#7-identify-changing-genes","title":"7. Identify Changing Genes","text":"<pre><code># Calculate fold change along trajectory\nfirst_tp = trajectory_df.iloc[0]\nlast_tp = trajectory_df.iloc[-1]\n\n# Log2 fold change\nfc = np.log2((last_tp + 1) / (first_tp + 1))\nfc_sorted = fc.abs().sort_values(ascending=False)\n\nprint(\"Top 10 most changing genes:\")\nprint(fc_sorted.head(10))\n</code></pre>"},{"location":"tutorials/quickstart/#8-quick-classification","title":"8. Quick Classification","text":"<pre><code>from renalprog.modeling.classification import train_stage_classifier\n\n# Prepare labels\ny = (clinical.loc[rnaseq_subset.index, 'stage_binary'] == 'Late').astype(int)\n\n# Train classifier\nclf, metrics, shap_values = train_stage_classifier(\n    X=rnaseq_subset.values,\n    y=y.values,\n    feature_names=top_genes.tolist(),\n    n_folds=5\n)\n\nprint(f\"Classification Accuracy: {metrics['test_accuracy']:.3f}\")\nprint(f\"ROC AUC: {metrics['test_roc_auc']:.3f}\")\n</code></pre> <p>Expected output: <pre><code>Classification Accuracy: 0.892\nROC AUC: 0.945\n</code></pre></p>"},{"location":"tutorials/quickstart/#9-save-results","title":"9. Save Results","text":"<pre><code># Save trajectory\ntrajectory_df.to_csv('data/interim/quickstart_trajectory.csv')\n\n# Save model\nvae.save('models/quickstart_vae.pt')\n\n# Save latent representation\nlatent_df.to_csv('data/interim/quickstart_latent.csv')\n\nprint(\"Results saved!\")\n</code></pre>"},{"location":"tutorials/quickstart/#complete-script","title":"Complete Script","text":"<p>Here's the complete script you can run:</p> quickstart_example.py<pre><code>\"\"\"Quick start example for renalprog.\"\"\"\n\nfrom renalprog import dataset, features, modeling, plots\nfrom renalprog.modeling import VAE\nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\n\n# Create output directories\nos.makedirs('reports/figures', exist_ok=True)\nos.makedirs('data/interim', exist_ok=True)\nos.makedirs('models', exist_ok=True)\n\n# 1. Load data\nprint(\"Loading data...\")\nrnaseq = pd.read_csv('data/processed/rnaseq_maha.csv', index_col=0)\nclinical = pd.read_csv('data/processed/clinical.csv', index_col=0)\n\n# 2. Prepare data\nprint(\"Preparing data...\")\nn_genes = 1000\ngene_vars = rnaseq.var(axis=0)\ntop_genes = gene_vars.nlargest(n_genes).index\nrnaseq_subset = rnaseq[top_genes]\nX = torch.FloatTensor(rnaseq_subset.values)\n\n# 3. Train VAE\nprint(\"Training VAE...\")\nvae = VAE(\n    input_dim=X.shape[1],\n    latent_dim=10,\n    hidden_dims=[256, 128],\n    seed=42\n)\nvae.fit(X, epochs=50, batch_size=32, learning_rate=1e-3, verbose=True)\n\n# 4. Get latent representations\nprint(\"Encoding to latent space...\")\nwith torch.no_grad():\n    latent = vae.encode(X).numpy()\n\nlatent_df = pd.DataFrame(\n    latent,\n    index=rnaseq_subset.index,\n    columns=[f'Z{i}' for i in range(latent.shape[1])]\n)\nlatent_df['stage'] = clinical.loc[latent_df.index, 'ajcc_pathologic_tumor_stage']\n\n# 5. Visualize\nprint(\"Creating visualizations...\")\nplots.plot_latent_space(\n    latent_df,\n    color_by='stage',\n    method='umap',\n    save_path='reports/figures/quickstart_latent_umap.png'\n)\n\n# 6. Generate trajectory\nprint(\"Generating trajectory...\")\nearly_samples = clinical[clinical['stage_binary'] == 'Early'].index\nlate_samples = clinical[clinical['stage_binary'] == 'Late'].index\n\nearly_latent = latent[rnaseq_subset.index.isin(early_samples)]\nlate_latent = latent[rnaseq_subset.index.isin(late_samples)]\n\nstart_point = early_latent[0]\nend_point = late_latent[0]\n\nn_steps = 10\ntrajectory_latent = np.linspace(start_point, end_point, n_steps)\ntrajectory_latent_tensor = torch.FloatTensor(trajectory_latent)\n\nwith torch.no_grad():\n    trajectory_expression = vae.decode(trajectory_latent_tensor).numpy()\n\ntrajectory_df = pd.DataFrame(\n    trajectory_expression,\n    columns=top_genes,\n    index=[f'timepoint_{i}' for i in range(n_steps)]\n)\n\n# 7. Save results\nprint(\"Saving results...\")\ntrajectory_df.to_csv('data/interim/quickstart_trajectory.csv')\nvae.save('models/quickstart_vae.pt')\nlatent_df.to_csv('data/interim/quickstart_latent.csv')\n\nprint(\"Quick start complete! Check reports/figures/ for visualizations.\")\n</code></pre> <p>Run it with: <pre><code>python quickstart_example.py\n</code></pre></p>"},{"location":"tutorials/quickstart/#what-youve-learned","title":"What You've Learned","text":"<p>In this quick start, you've:</p> <ul> <li>\u2705 Loaded and prepared gene expression data</li> <li>\u2705 Trained a VAE to learn latent representations</li> <li>\u2705 Visualized the latent space</li> <li>\u2705 Generated a synthetic progression trajectory</li> <li>\u2705 Identified genes that change along the trajectory</li> <li>\u2705 Built a classification model</li> </ul>"},{"location":"tutorials/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've seen the basics, dive deeper:</p> <ol> <li>Complete Pipeline: Full analysis workflow from raw data</li> <li>VAE Training: Advanced model configuration</li> <li>Trajectory Generation: Multiple trajectory types</li> <li>Enrichment Analysis: Pathway-level interpretation</li> </ol>"},{"location":"tutorials/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/quickstart/#out-of-memory","title":"Out of Memory","text":"<p>If you get OOM errors: <pre><code># Reduce number of genes\nn_genes = 500\n\n# Reduce batch size\nvae.fit(X, batch_size=16, ...)\n\n# Use smaller hidden dimensions\nvae = VAE(input_dim=X.shape[1], hidden_dims=[128, 64], ...)\n</code></pre></p>"},{"location":"tutorials/quickstart/#slow-training","title":"Slow Training","text":"<p>To speed up training: <pre><code># Use GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nvae = VAE(input_dim=X.shape[1], device=device)\n\n# Reduce epochs for testing\nvae.fit(X, epochs=20, ...)\n</code></pre></p>"},{"location":"tutorials/quickstart/#import-errors","title":"Import Errors","text":"<p>Make sure you've installed the package: <pre><code>pip install -e .\n</code></pre></p> <p>And activated the environment: <pre><code>mamba activate renalprog\n</code></pre></p>"}]}