{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#renalprog","title":"renalprog","text":"<p>A Python package for simulating kidney cancer progression with synthetic data generation and machine learning</p> <p> </p> <p> <sub>Logo: Kidneys icons created by Smashicons - Flaticon</sub> </p>"},{"location":"#overview","title":"Overview","text":"<p><code>renalprog</code> is a comprehensive bioinformatics pipeline for analyzing kidney cancer (KIRC) progression using deep learning and pathway enrichment analysis. The package integrates Variational Autoencoders (VAEs) with differential expression analysis and gene set enrichment to model cancer progression trajectories.</p>"},{"location":"#scientific-context","title":"Scientific Context","text":"<p>Cancer progression is a complex, dynamic process involving multiple molecular alterations across time. Traditional static analyses fail to capture the temporal dynamics of tumor evolution. <code>renalprog</code> addresses this challenge by:</p> <ol> <li>Learning latent representations of gene expression data using deep generative models (VAEs)</li> <li>Generating synthetic trajectories between cancer stages in latent space</li> <li>Identifying enriched biological pathways along progression trajectories</li> <li>Classifying cancer stages using interpretable machine learning</li> </ol> <p>This approach enables researchers to:</p> <ul> <li>Identify key biological pathways driving cancer progression</li> <li>Predict patient outcomes based on molecular profiles</li> <li>Understand the temporal dynamics of tumor evolution</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#data-processing","title":"\ud83d\udd2c Data Processing","text":"<ul> <li>Automated filtering of low-expression genes</li> <li>Outlier detection using Mahalanobis distance</li> <li>Normalization and batch effect correction</li> <li>Integration with TCGA bulk RNA-seq datasets</li> </ul>"},{"location":"#deep-learning-models","title":"\ud83e\udde0 Deep Learning Models","text":"<ul> <li>Variational Autoencoder (VAE) and Autoencoder (AE) for unsupervised representation learning</li> <li>Conditional VAE (CVAE) for fully supervised tasks</li> <li>Support for custom architectures and hyperparameters</li> <li>GPU acceleration with PyTorch</li> </ul>"},{"location":"#trajectory-generation","title":"\ud83d\udd04 Trajectory Generation","text":"<ul> <li>Generate synthetic patient trajectories between cancer stages</li> <li>Interpolation in latent space and decoding to gene expression space</li> <li>Control for clinical covariates (age, gender)</li> <li>Export trajectory gene expression for downstream analysis</li> <li>Visualization of trajectories</li> <li>Integration with enrichment analysis pipeline</li> </ul>"},{"location":"#stage-classification","title":"\ud83d\udcca Stage Classification","text":"<ul> <li>XGBoost-based classification of early vs. late stage cancer</li> <li>Cross-validation and performance evaluation</li> <li>Gene signature discovery</li> </ul>"},{"location":"#enrichment-analysis","title":"\ud83e\uddec Enrichment Analysis","text":"<ul> <li>Integration with pyDESeq2 for differential expression</li> <li>GSEA (Gene Set Enrichment Analysis) for pathway analysis</li> <li>Support for Reactome, KEGG, and other pathway databases</li> <li>Parallel (multi CPU and multi node) processing for large-scale analyses</li> </ul>"},{"location":"#visualization","title":"\ud83d\udcc8 Visualization","text":"<ul> <li>Comprehensive plotting functions for all analysis steps</li> <li>UMAP/t-SNE visualizations of latent space</li> <li>Pathway enrichment heatmaps</li> <li>Classification performance metrics</li> <li>Interactive plots with Plotly</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See Quick Start Tutorial for a complete example.</p>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<p>The <code>renalprog</code> pipeline consists of six main steps:</p> <pre><code>graph LR\n    A[Raw Data] --&gt; B[1. Data Processing]\n    B --&gt; C[2. VAE Training]\n    C --&gt; D[3. Reconstruction Check]\n    D --&gt; E[4. Trajectory Generation]\n    E --&gt; F[5. Classification]\n    F --&gt; G[6. Enrichment Analysis]\n    G --&gt; H[Results &amp; Visualization]</code></pre>"},{"location":"#step-1-data-processing","title":"Step 1: Data Processing","text":"<ul> <li>Filter low-expression genes</li> <li>Remove outliers using Mahalanobis distance</li> <li>Normalize expression values</li> <li>Prepare clinical metadata</li> </ul>"},{"location":"#step-2-vae-training","title":"Step 2: VAE Training","text":"<ul> <li>Train deep generative models on gene expression data</li> <li>Learn low-dimensional latent representations</li> <li>Validate reconstruction quality</li> </ul>"},{"location":"#step-3-reconstruction-validation","title":"Step 3: Reconstruction Validation","text":"<ul> <li>Assess VAE reconstruction accuracy</li> <li>Visualize latent space structure</li> <li>Identify potential issues</li> </ul>"},{"location":"#step-4-trajectory-generation","title":"Step 4: Trajectory Generation","text":"<ul> <li>Generate synthetic patient trajectories</li> <li>Interpolate between cancer stages</li> <li>Export trajectory gene expression</li> </ul>"},{"location":"#step-5-classification","title":"Step 5: Classification","text":"<ul> <li>Train XGBoost classifier for stage prediction</li> <li>Calculate SHAP values for interpretability</li> <li>Identify important gene signatures</li> </ul>"},{"location":"#step-6-enrichment-analysis","title":"Step 6: Enrichment Analysis","text":"<ul> <li>Differential expression analysis with DESeq2</li> <li>Pathway enrichment with GSEA</li> <li>Identify biological processes along trajectories</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#for-new-users","title":"\ud83d\udcda For New Users","text":"<p>Start with:</p> <ol> <li>Installation Guide - Set up your environment</li> <li>Quick Start Tutorial - Run your first analysis</li> <li>Complete Pipeline Tutorial - End-to-end workflow</li> </ol>"},{"location":"#for-developers","title":"\ud83d\udee0\ufe0f For Developers","text":"<p>Extend and customize:</p> <ol> <li>API Reference - Complete function documentation</li> <li>Contributing Guidelines - Join development</li> </ol>"},{"location":"#citation","title":"Citation","text":"<p>Cite renalprog</p> <p>This citation is temporal and will be updated upon formal publication.</p> <p>If you use <code>renalprog</code> in your research, please cite:</p> <pre><code>@software{renalprog2025,\n  author = {Prol-Castelo, Guillermo and EVENFLOW Project},\n  title = {renalprog: Simulating Kidney Cancer Progression with Generative AI},\n  year = {2025},\n  publisher = {GitHub},\n  url = {https://github.com/gprolcastelo/renalprog}\n}\n</code></pre> <p>See How to Cite for additional references.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0 - see the LICENSE file for details.</p>"},{"location":"#support","title":"Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: Contact the EVENFLOW Project team</li> </ul>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This work is supported by the EVENFLOW Project and builds upon numerous open-source tools and databases. See Acknowledgments for complete credits.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> <li>Contributing Guidelines</li> </ul>"},{"location":"acknowledgments/","title":"Acknowledgments","text":""},{"location":"acknowledgments/#funding","title":"Funding","text":"<p>This work was supported by the EU project EVENFLOW under Horizon Europe agreement No. 101070430.</p>"},{"location":"acknowledgments/#research-institutions","title":"Research Institutions","text":"<p>We gratefully acknowledge the Barcelona Supercomputing Center - Centro Nacional de Supercomputaci\u00f3n (BSC-CNS) for providing access to high-performance computing resources that were essential for the development and validation of this framework, for their continued support in computational biology research and for fostering an environment of scientific excellence.</p> <p>We thank NCSR Demokritos for their support and collaboration in this project.</p>"},{"location":"acknowledgments/#data-sources","title":"Data Sources","text":"<p>We acknowledge the following data sources:</p> <ul> <li>The Cancer Genome Atlas (TCGA) for providing the cancer genomics data</li> <li>Reactome for pathway annotations</li> <li>MSigDB for gene set collections</li> </ul>"},{"location":"acknowledgments/#open-source-community","title":"Open Source Community","text":"<p>This project builds upon numerous open-source Python libraries and tools. We thank the developers and maintainers of:</p> <ul> <li>PyTorch and the deep learning community</li> <li>scikit-learn and the machine learning community</li> <li>pandas, numpy, and the scientific Python ecosystem</li> <li>GSEA (Gene Set Enrichment Analysis) tools</li> <li>All other dependencies listed in our requirements</li> </ul>"},{"location":"acknowledgments/#design-attribution","title":"Design Attribution","text":"<p>The repository logo is: Kidneys icons created by Smashicons - Flaticon</p> <p>The views and opinions expressed in this documentation are those of the authors and do not necessarily reflect the official policy or position of the EVENFLOW project or the Barcelona Supercomputing Center.</p>"},{"location":"citation/","title":"Citation","text":"<p>Temporary Citation</p> <p>This citation is temporary and will be updated once the preprint/publication is available.</p> <p>Please check back regularly for updates, or watch the GitHub repository for announcements.</p>"},{"location":"citation/#current-citation-temporary","title":"Current Citation (Temporary)","text":"<p>Warning</p> <p>This citation is temporary and will be updated once the preprint publication is available.</p> <p>If you use RenalProg in your research, please cite it as follows until the official publication is available:</p> <pre><code>@software{renalprog2024,\n  title = {RenalProg: A Deep Learning Framework for Kidney Cancer Progression Modeling},\n  author = {[Guillermo Prol-Castelo, Elina Syrri, Nikolaos Manginas, Vasileos Manginas, Nikos Katzouris, Davide Cirillo, George Paliouras, Alfonso Valencia]},\n  year = {2025},\n  url = {https://github.com/gprolcas/renalprog},\n  note = {Preprint in preparation}\n}\n</code></pre>"},{"location":"citation/#updates","title":"Updates","text":"<p>Last updated: December 2025</p> <p>Status: Preprint in preparation</p> <p>Expected publication: 2026</p> <p>Stay Updated</p> <p>To receive notifications when the official publication is available:</p> <ul> <li>\u2b50 Star the GitHub repository</li> <li>\ud83d\udc40 Watch for releases</li> <li>\ud83d\udce7 Follow project updates</li> </ul> <p>This page will be updated immediately upon preprint/publication release.</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#apache-license-20","title":"Apache License 2.0","text":"<p>RenalProg is licensed under the Apache License, Version 2.0.</p> <p>The full license text is available in the LICENSE file in the repository.</p>"},{"location":"license/#citation-requirement","title":"Citation Requirement","text":"<p>While not legally required by the Apache 2.0 License, we kindly request that you cite this work in any publications or presentations that use RenalProg. See the Citation page for details.</p>"},{"location":"license/#third-party-components","title":"Third-Party Components","text":"<p>RenalProg includes or depends on several third-party libraries and tools, each with their own licenses.</p>"},{"location":"license/#contributing","title":"Contributing","text":"<p>By contributing to RenalProg, you agree that your contributions will be licensed under the Apache License 2.0. See the Contributing Guidelines for more information.</p> <p>Last updated: December 2025</p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/","title":"Dynamic Enrichment Analysis","text":"<p>This document describes the dynamic enrichment analysis pipeline for renalprog.</p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#overview","title":"Overview","text":"<p>The enrichment analysis pipeline performs Gene Set Enrichment Analysis (GSEA) on synthetic cancer progression trajectories. This allows us to identify biological pathways that are enriched at different stages of progression.</p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#pipeline-steps","title":"Pipeline Steps","text":""},{"location":"advanced/ENRICHMENT_ANALYSIS/#1-pydeseq2-differential-expression-analysis","title":"1. pyDESeq2 Differential Expression Analysis","text":"<p>For each synthetic trajectory timepoint:</p> <ol> <li>Load trajectory gene expression data (reverse log-transform from RSEM)</li> <li>Load healthy control samples (reverse log-transform from RSEM)</li> <li>Run pyDESeq2 differential expression analysis comparing trajectory vs controls</li> <li>Extract log2 fold-change and adjusted p-values for each gene</li> <li>Rank genes by log2 fold-change</li> <li>Save ranked gene list (<code>.rnk</code> file) for GSEA</li> </ol> <p>Note: The pipeline uses PyDESeq2 for proper differential expression analysis, not simple fold-change calculations. This ensures statistical rigor and proper handling of count data variance.</p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#2-gsea-analysis","title":"2. GSEA Analysis","text":"<p>For each ranked gene list:</p> <ol> <li>Run GSEA using preranked mode</li> <li>Test against pathway database (ReactomePathways.gmt)</li> <li>Calculate enrichment scores and FDR q-values</li> <li>Generate positive and negative enrichment reports</li> </ol>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#3-results-combination","title":"3. Results Combination","text":"<p>Combine all GSEA results into a single dataset:</p> <ul> <li>One row per (patient, timepoint, pathway)</li> <li>Includes enrichment score (ES), normalized ES (NES), and FDR q-value</li> <li>Missing pathways filled with NaN values</li> </ul>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#installation-requirements","title":"Installation Requirements","text":""},{"location":"advanced/ENRICHMENT_ANALYSIS/#python-dependencies","title":"Python Dependencies","text":"<p>The enrichment pipeline requires PyDESeq2 for differential expression analysis:</p> <pre><code>pip install pydeseq2\n</code></pre> <p>PyDESeq2 is a Python implementation of the DESeq2 method for differential expression analysis of count data.</p> <p>Citation: <pre><code>Muzellec, B., Telenczuk, M., &amp; Cabeli, V. (2022).\nPyDESeq2: a python package for bulk RNA-seq differential expression analysis.\nbioRxiv, 2022-12.\n</code></pre></p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#gsea-cli-tool","title":"GSEA CLI Tool","text":"<ol> <li>Download GSEA from: https://www.gsea-msigdb.org/gsea/downloads.jsp</li> <li>Extract to project root (creates <code>GSEA_4.3.2/</code> directory)</li> <li>Ensure <code>gsea-cli.sh</code> (Unix) or <code>gsea-cli.bat</code> (Windows) is executable</li> </ol> <p>Citation: <pre><code>Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., ... &amp; Mesirov, J. P. (2005).\nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles.\nProceedings of the National Academy of Sciences, 102(43), 15545-15550.\n</code></pre></p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#pathway-database","title":"Pathway Database","text":"<p>The ReactomePathways.gmt file is included in <code>data/external/ReactomePathways.gmt</code>.</p> <p>Citation: <pre><code>Jassal, B., Matthews, L., Viteri, G., Gong, C., Lorente, P., Fabregat, A., ... &amp; D'Eustachio, P. (2020).\nThe reactome pathway knowledgebase.\nNucleic acids research, 48(D1), D498-D503.\n</code></pre></p> <p>Technical Details:</p> <p>The DESeq2 processing involves:</p> <ol> <li>Reverse log-transformation: Input data is log-transformed RSEM values, which are converted back to RSEM.</li> <li>PyDESeq2 analysis: Proper variance modeling and statistical testing.</li> <li>Rank file generation: Genes ranked by log fold-change for GSEA preranked mode.</li> </ol>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#gsea-parameters","title":"GSEA Parameters","text":"<p>Default GSEA parameters in <code>generate_gsea_command()</code>: - <code>collapse</code>: false - <code>nperm</code>: 1000 permutations - <code>set_max</code>: 500 (maximum pathway size) - <code>set_min</code>: 15 (minimum pathway size)</p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#output-format","title":"Output Format","text":""},{"location":"advanced/ENRICHMENT_ANALYSIS/#directory-structure","title":"Directory Structure","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 test_to_test/                                          # Synthetic trajectories\n\u2502   \u251c\u2500\u2500 early_to_late/                                     # Transition type\n\u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient2/                          # Patient trajectory\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient_0.rnk                  # Ranked gene list for timepoint 0\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient_1.rnk\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 reports/                                   # GSEA output for all patients in directory\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient_0.GseaPreranked.*  # GSEA output files\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 gsea_report_for_na_pos_*.tsv           # Positive enrichment report\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 gsea_report_for_na_neg_*.tsv           # Negative enrichment report\n\u2502   \u2502   \u251c\u2500\u2500patient3_to_patient4/\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 gsea_commands_*.cmd                                # GSEA command files\n\u251c\u2500\u2500 full_gsea_reports_kirc.csv                             # Final combined results\n\u2514\u2500\u2500 heatmap_kirc_significantNES.csv                        # Significant NES heatmap data\n</code></pre>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#final-results-format","title":"Final Results Format","text":"<p><code>full_gsea_reports_kirc.csv</code> columns:</p> <ul> <li><code>Patient</code>: Patient identifier (e.g., \"TCGA-CZ-5989-01_to_TCGA-B0-5108-01)</li> <li><code>Idx</code>: Timepoint index (0 to n_samples-1)</li> <li><code>Transition</code>: Transition type (e.g., \"early_to_late\")</li> <li><code>NAME</code>: Pathway name (from ReactomePathways.gmt)</li> <li><code>ES</code>: Enrichment score</li> <li><code>NES</code>: Normalized enrichment score</li> <li><code>FDR q-val</code>: False discovery rate q-value</li> </ul> <p>Example: <pre><code>Patient,Idx,Transition,NAME,ES,NES,FDR q-val\nTCGA-CZ-5989-01_to_TCGA-B0-5108-01,0,early_to_late,Cell Cycle,0.65,2.13,0.001\nTCGA-CZ-5989-01_to_TCGA-B0-5108-01,0,early_to_late,DNA Repair,0.52,1.87,0.012\nTCGA-CZ-5989-01_to_TCGA-B0-5108-01,1,early_to_late,Cell Cycle,0.71,2.31,0.000\n</code></pre></p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#gsea-not-found","title":"GSEA Not Found","text":"<p>Error: <code>GSEA CLI not found at ./GSEA_4.3.2/gsea-cli.sh</code></p> <p>Solution:</p> <ol> <li>Download GSEA from https://www.gsea-msigdb.org/gsea/downloads.jsp</li> <li>Extract to project root</li> <li>Or specify custom path with <code>--gsea_path</code></li> </ol>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#gsea-command-failures","title":"GSEA Command Failures","text":"<p>Error: GSEA commands fail with non-zero exit code</p> <p>Solution:</p> <ol> <li>Check GSEA installation</li> <li>Verify pathway file format (GMT)</li> <li>Check file permissions</li> <li>Review GSEA log files in output directories</li> </ol>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#missing-pathways","title":"Missing Pathways","text":"<p>Warning: Some pathways have all NaN values</p> <p>Explanation: Normal - not all pathways are significant in every sample</p>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>Error: Cannot run <code>gsea-cli.sh</code> on Windows</p> <p>Solution:</p> <ol> <li>Install Git Bash or WSL (Windows Subsystem for Linux)</li> <li>Use <code>gsea-cli.bat</code> instead if available</li> <li>Or run in WSL environment</li> </ol>"},{"location":"advanced/ENRICHMENT_ANALYSIS/#see-also","title":"See Also","text":"<ul> <li>Trajectory Generation</li> <li>Classification Pipeline</li> <li>R Analysis Scripts</li> </ul>"},{"location":"advanced/GSEA_INSTALLATION/","title":"GSEA Installation Guide","text":"<p>This guide provides instructions for installing and configuring GSEA (Gene Set Enrichment Analysis) for use with renalprog.</p>"},{"location":"advanced/GSEA_INSTALLATION/#what-is-gsea","title":"What is GSEA?","text":"<p>GSEA is a computational method that determines whether a priori defined sets of genes show statistically significant, concordant differences between two biological states.</p> <p>Citation: <pre><code>Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., ... &amp; Mesirov, J. P. (2005).\nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles.\nProceedings of the National Academy of Sciences, 102(43), 15545-15550.\nDOI: 10.1073/pnas.0506580102\n</code></pre></p>"},{"location":"advanced/GSEA_INSTALLATION/#installation","title":"Installation","text":""},{"location":"advanced/GSEA_INSTALLATION/#step-1-download-gsea","title":"Step 1: Download GSEA","text":"<ol> <li> <p>Visit the GSEA downloads page: https://www.gsea-msigdb.org/gsea/downloads.jsp</p> </li> <li> <p>Register for an account (free for academic use)</p> </li> <li> <p>Download the latest version of GSEA:</p> </li> <li>For Windows/Mac/Linux: Download \"GSEA_X.X.X.zip\" (X.X.X = version number)</li> <li>Recommended version: GSEA 4.3.2 or later</li> </ol>"},{"location":"advanced/GSEA_INSTALLATION/#step-2-extract-gsea","title":"Step 2: Extract GSEA","text":"<ol> <li> <p>Extract the downloaded ZIP file to your renalprog project root:    <pre><code>renalprog/\n\u251c\u2500\u2500 GSEA_4.3.2/          # \u2190 Extract here\n\u2502   \u251c\u2500\u2500 gsea-cli.sh      # Unix/Linux/Mac\n\u2502   \u251c\u2500\u2500 gsea-cli.bat     # Windows\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 data/\n\u251c\u2500\u2500 renalprog/\n\u2514\u2500\u2500 ...\n</code></pre></p> </li> <li> <p>The extraction should create a folder named <code>GSEA_X.X.X/</code> (e.g., <code>GSEA_4.3.2/</code>)</p> </li> </ol>"},{"location":"advanced/GSEA_INSTALLATION/#step-3-make-executable-unixlinuxmac","title":"Step 3: Make Executable (Unix/Linux/Mac)","text":"<pre><code>cd GSEA_4.3.2\nchmod +x gsea-cli.sh\n</code></pre>"},{"location":"advanced/GSEA_INSTALLATION/#step-4-test-installation","title":"Step 4: Test Installation","text":""},{"location":"advanced/GSEA_INSTALLATION/#on-unixlinuxmac","title":"On Unix/Linux/Mac:","text":"<pre><code>./GSEA_4.3.2/gsea-cli.sh --help\n</code></pre>"},{"location":"advanced/GSEA_INSTALLATION/#on-windows-git-bash-or-wsl","title":"On Windows (Git Bash or WSL):","text":"<pre><code>bash GSEA_4.3.2/gsea-cli.sh --help\n</code></pre>"},{"location":"advanced/GSEA_INSTALLATION/#on-windows-command-prompt","title":"On Windows (Command Prompt):","text":"<pre><code>GSEA_4.3.2\\gsea-cli.bat --help\n</code></pre> <p>You should see the GSEA help message if installation was successful.</p>"},{"location":"advanced/GSEA_INSTALLATION/#system-requirements","title":"System Requirements","text":""},{"location":"advanced/GSEA_INSTALLATION/#java","title":"Java","text":"<p>GSEA requires Java 11 or later.</p> <p>Check Java version: <pre><code>java -version\n</code></pre></p> <p>If Java is not installed:</p> <ul> <li>Windows: check Windows build of Java here.</li> <li>Ubuntu/Debian: <code>sudo apt-get install openjdk-11-jdk</code></li> </ul>"},{"location":"advanced/GSEA_INSTALLATION/#pathway-databases","title":"Pathway Databases","text":""},{"location":"advanced/GSEA_INSTALLATION/#reactomepathwaysgmt","title":"ReactomePathways.gmt","text":"<p>The renalprog package includes ReactomePathways.gmt in <code>data/external/</code>.</p>"},{"location":"advanced/GSEA_INSTALLATION/#additional-gmt-files","title":"Additional GMT Files","text":"<p>You can download additional gene set databases from MSigDB: https://www.gsea-msigdb.org/gsea/msigdb/collections.jsp</p> <p>Place downloaded GMT files in <code>data/external/</code> and reference them with the <code>--pathways_file</code> argument.</p>"},{"location":"advanced/GSEA_INSTALLATION/#configuration","title":"Configuration","text":""},{"location":"advanced/GSEA_INSTALLATION/#default-configuration","title":"Default Configuration","text":"<p><code>renalprog</code> uses these GSEA parameters by default:</p> <ul> <li>--collapse false Don't collapse probe sets</li> <li>--nperm 1000            # Number of permutations</li> <li>--set_max 500           # Maximum gene set size</li> <li>--set_min 15            # Minimum gene set size</li> </ul>"},{"location":"advanced/GSEA_INSTALLATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/GSEA_INSTALLATION/#gsea-cli-not-found","title":"\"GSEA CLI not found\"","text":"<p>Problem: Script cannot find GSEA installation</p> <p>Solution: 1. Verify GSEA is extracted to project root 2. Check the folder name matches <code>GSEA_X.X.X/</code> 3. Specify custom path: <code>--gsea_path /path/to/gsea-cli.sh</code></p>"},{"location":"advanced/GSEA_INSTALLATION/#java-not-found-or-unsupportedclassversionerror","title":"\"Java not found\" or \"UnsupportedClassVersionError\"","text":"<p>Problem: Java is not installed or version is too old</p> <p>Solution: 1. Install Java 11 or later 2. Update PATH environment variable 3. Set JAVA_HOME environment variable</p>"},{"location":"advanced/GSEA_INSTALLATION/#permission-denied-unixlinuxmac","title":"\"Permission denied\" (Unix/Linux/Mac)","text":"<p>Problem: GSEA script is not executable</p> <p>Solution: <pre><code>chmod +x GSEA_4.3.2/gsea-cli.sh\n</code></pre></p>"},{"location":"advanced/GSEA_INSTALLATION/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>Problem: Cannot run <code>gsea-cli.sh</code> on Windows</p> <p>Solution 1: Use Git Bash <pre><code># Install Git for Windows (includes Git Bash)\n# Then run:\nbash GSEA_4.3.2/gsea-cli.sh --help\n</code></pre></p> <p>Solution 2: Use WSL (Windows Subsystem for Linux) <pre><code># Install WSL\nwsl --install\n\n# Then in WSL terminal:\n./GSEA_4.3.2/gsea-cli.sh --help\n</code></pre></p> <p>Solution 3: create a batch file to run GSEA <pre><code>GSEA_4.3.2\\gsea-cli.bat --help\n</code></pre></p>"},{"location":"advanced/GSEA_INSTALLATION/#license-and-citation","title":"License and Citation","text":""},{"location":"advanced/GSEA_INSTALLATION/#gsea-license","title":"GSEA License","text":"<p>GSEA software is distributed under a custom license: - Free for academic and non-profit use - Commercial use requires a license - See: https://www.gsea-msigdb.org/gsea/login.jsp</p>"},{"location":"advanced/GSEA_INSTALLATION/#citations","title":"Citations","text":"<p>For the current implementation of GSEA in <code>renalprog</code>, or the package more generally, please see the citation page. </p> <p>Besides, if you use GSEA in your research, please cite:</p> <p>GSEA: <pre><code>Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., ... &amp; Mesirov, J. P. (2005). \nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. \nProceedings of the National Academy of Sciences, 102(43), 15545-15550.\n</code></pre></p> <p>If using Reactome pathways:</p> <pre><code>Milacic, M., Beavers, D., Conley, P., Gong, C., Gillespie, M., Griss, J., ... &amp; D\u2019Eustachio, P. (2024). \nThe reactome pathway knowledgebase 2024. \nNucleic acids research, 52(D1), D672-D678.\n</code></pre> <p>If using other gene sets, please cite accordingly.</p>"},{"location":"advanced/GSEA_INSTALLATION/#additional-resources","title":"Additional Resources","text":"<ul> <li>GSEA User Guide: https://www.gsea-msigdb.org/gsea/doc/GSEAUserGuideFrame.html</li> <li>Reactome: https://reactome.org/</li> </ul>"},{"location":"advanced/GSEA_INSTALLATION/#contact","title":"Contact","text":"<p>For renalprog-specific issues, please open an issue on GitHub.</p> <p>For GSEA software issues, contact the GSEA team through their forum or email.</p>"},{"location":"advanced/PATHWAY_HEATMAP/","title":"Pathway Enrichment Heatmap","text":""},{"location":"advanced/PATHWAY_HEATMAP/#overview","title":"Overview","text":"<p>The pathway enrichment heatmap visualization summarizes GSEA results across all trajectories and timepoints. The pipeline generates five different heatmaps to provide comprehensive views of pathway dynamics during cancer progression.</p>"},{"location":"advanced/PATHWAY_HEATMAP/#heatmap-types","title":"Heatmap Types","text":""},{"location":"advanced/PATHWAY_HEATMAP/#1-top-varying-pathways","title":"1. Top Varying Pathways","text":"<p>Shows the 50 pathways with the highest variance between first and last timepoints.</p> <ul> <li>Purpose: Identify pathways that change most dramatically during progression</li> <li>Colormap: RdBu_r (red-white-blue diverging)</li> <li>Interpretation: Pathways with largest temporal dynamics</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#2-top-upregulated-pathways","title":"2. Top Upregulated Pathways","text":"<p>Shows the 50 pathways with highest average NES (most consistently upregulated).</p> <ul> <li>Purpose: Identify pathways activated during progression</li> <li>Colormap: YlGn (yellow-green sequential)</li> <li>Interpretation: Consistently activated pathways across progression</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#3-top-downregulated-pathways","title":"3. Top Downregulated Pathways","text":"<p>Shows the 50 pathways with lowest average NES (most consistently downregulated).</p> <ul> <li>Purpose: Identify pathways suppressed during progression</li> <li>Colormap: YlOrBr (yellow-orange-brown sequential)</li> <li>Interpretation: Consistently suppressed pathways across progression</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#4-high-level-pathways","title":"4. High-Level Pathways","text":"<p>Shows 29 Reactome high-level pathways (top-level categories).</p> <ul> <li>Purpose: Get broad overview of biological processes</li> <li>Pathways: Cell Cycle, DNA Repair, Immune System, Metabolism, Signal Transduction, etc.</li> <li>Colormap: RdBu_r</li> <li>Interpretation: System-level view of cancer progression</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#5-literature-pathways","title":"5. Literature Pathways","text":"<p>Shows 33 pathways from kidney cancer literature.</p> <ul> <li>Purpose: Focus on pathways known to be important in kidney cancer</li> <li>Pathways: VHL/HIF pathway, PI3K/AKT/MTOR, Warburg effect, TCA cycle, etc.</li> <li>Colormap: RdBu_r</li> <li>Interpretation: Validation of known biology and discovery of new patterns</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#what-the-heatmaps-show","title":"What the Heatmaps Show","text":"<ul> <li>Rows: Pathway names</li> <li>Columns: Pseudotime progression (50 timepoints from early to late)</li> <li>Values: Sum of NES (Normalized Enrichment Score) across all patients at each timepoint</li> <li>Colors: </li> <li>Red/Green/Brown (high values): Pathway enriched/upregulated</li> <li>Blue/Yellow (low values): Pathway depleted/downregulated</li> <li>White (middle): NES near zero or not significant</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#filtering-criteria","title":"Filtering criteria","text":"<p>Only pathways meeting these criteria are included:</p> <ol> <li>FDR q-value &lt; 0.05 (default, configurable)</li> <li>At least one significant enrichment across all trajectories</li> </ol>"},{"location":"advanced/PATHWAY_HEATMAP/#files-generated","title":"Files Generated","text":"<p>When you run the pathway heatmap generation, mainly, the following files are created:</p>"},{"location":"advanced/PATHWAY_HEATMAP/#data-file","title":"Data File","text":"<ul> <li>pathway_heatmap_data.csv: Matrix of NES values (pathways \u00d7 timepoints)</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#visualization-files-for-each-heatmap-type","title":"Visualization Files (for each heatmap type)","text":"<ol> <li>top_varying_pathways.[pdf/png/svg]: Top 50 most varying pathways</li> <li>top_upregulated_pathways.[pdf/png/svg]: Top 50 upregulated pathways</li> <li>top_downregulated_pathways.[pdf/png/svg]: Top 50 downregulated pathways</li> <li>high_level_pathways.[pdf/png/svg]: 29 Reactome high-level pathways</li> <li>literature_pathways.[pdf/png/svg]: 33 kidney cancer literature pathways</li> </ol> <p>All heatmaps are saved in PDF (publication quality), PNG (high-res), and SVG (vector) formats.</p>"},{"location":"advanced/PATHWAY_HEATMAP/#implementation-details","title":"Implementation Details","text":"<p>The heatmaps are generated by:</p> <ol> <li>Filtering: Keep only results with FDR q-val &lt; threshold</li> <li>Grouping: Group by Pathway name and Timepoint index</li> <li>Aggregation: Sum NES values across all patients at each timepoint</li> <li> <p>Pathway Selection:</p> </li> <li> <p>Top varying: Calculate variance between first and last timepoint, select top 50</p> </li> <li>Top upregulated: Calculate mean NES, select top 50 positive</li> <li>Top downregulated: Calculate mean NES, select top 50 negative</li> <li>High-level: Filter for 29 predefined Reactome pathways</li> <li> <p>Literature: Filter for 33 kidney cancer-related pathways</p> </li> <li> <p>Pivoting: Create matrix with pathways as rows, timepoints as columns</p> </li> <li>Visualization: Create heatmap with appropriate colormap and styling</li> </ol>"},{"location":"advanced/PATHWAY_HEATMAP/#technical-notes","title":"Technical Notes","text":"<ul> <li>Uses PyDESeq2 for differential expression (not simple fold-change)</li> <li>RSEM data is reverse log-transformed before DESeq2 analysis</li> <li>Each trajectory timepoint compared against healthy controls</li> <li>NES values summed across all patients for each (pathway, timepoint) pair</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/PATHWAY_HEATMAP/#no-significant-pathways-found","title":"No significant pathways found","text":"<p>If heatmaps are empty or have few pathways:</p> <ul> <li>Check FDR threshold (try 0.1 or 0.25)</li> <li>Verify GSEA ran successfully for all trajectories</li> <li>Check that PyDESeq2 analysis completed without errors</li> <li>Ensure trajectory data has sufficient dynamic range</li> </ul>"},{"location":"advanced/PATHWAY_HEATMAP/#missing-literaturehigh-level-pathways","title":"Missing literature/high-level pathways","text":"<p>If specific pathway categories are missing:</p> <ul> <li>Pathway names must match exactly (case-sensitive)</li> <li>Check pathway database (ReactomePathways.gmt) contains these pathways</li> <li>Pathways may not be significant at current FDR threshold</li> </ul>"},{"location":"advanced/R_ANALYSIS/","title":"R Analysis Pipeline","text":"<p>This guide provides comprehensive documentation for the R analysis scripts in the renalprog package, covering differential expression analysis and gene enrichment analysis.</p>"},{"location":"advanced/R_ANALYSIS/#overview","title":"Overview","text":"<p>The R analysis pipeline provides two main capabilities:</p> <ol> <li>Differential Expression Analysis - Identify genes that are differentially expressed between disease stages</li> <li>Gene Enrichment Analysis - Identify biological pathways and processes enriched in gene lists</li> </ol> <p>Both scripts are include a command-line interface with argument parsing, comprehensive logging, and error handling.</p>"},{"location":"advanced/R_ANALYSIS/#prerequisites","title":"Prerequisites","text":""},{"location":"advanced/R_ANALYSIS/#r-installation","title":"R Installation","text":"<p>Required: R version 4.0 or higher</p> <p>Check your R version: <pre><code>R --version\n</code></pre></p>"},{"location":"advanced/R_ANALYSIS/#installation-methods","title":"Installation Methods","text":"<p>This section summarizes different methods to install the required R packages. Check the detailed installation instructions.</p>"},{"location":"advanced/R_ANALYSIS/#method-1-condamamba-recommended","title":"Method 1: Conda/Mamba (Recommended)","text":"<p>If using the renalprog conda environment:</p> <pre><code># Activate environment\nmamba activate renalprog\n\n# Install R packages via conda\nmamba install -c conda-forge -c bioconda \\\n    r-base \\\n    r-gprofiler2 \\\n    r-ggplot2 \\\n    r-optparse \\\n    r-dplyr \\\n    r-effsize \\\n    bioconductor-limma \\\n    bioconductor-edger\n</code></pre> <p>Advantages:</p> <ul> <li>All dependencies in one environment</li> <li>No admin privileges needed</li> <li>Reproducible across systems</li> <li>Automatic dependency resolution</li> </ul>"},{"location":"advanced/R_ANALYSIS/#method-2-automated-script","title":"Method 2: Automated Script","text":"<p>Use the provided installation script:</p> <pre><code># From repository root\nRscript scripts/r_analysis/install_r_packages.R\n</code></pre> <p>This installs:</p> <ul> <li>CRAN packages: gprofiler2, ggplot2, optparse, dplyr, effsize</li> <li>Bioconductor packages: limma, edgeR</li> </ul>"},{"location":"advanced/R_ANALYSIS/#method-3-manual-installation","title":"Method 3: Manual Installation","text":"<p>In an R console:</p> <pre><code># CRAN packages\ninstall.packages(c(\"gprofiler2\", \"ggplot2\", \"optparse\", \"dplyr\", \"effsize\"))\n\n# Bioconductor packages\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"limma\", \"edgeR\"))\n</code></pre>"},{"location":"advanced/R_ANALYSIS/#script-1-differential-expression-analysis","title":"Script 1: Differential Expression Analysis","text":""},{"location":"advanced/R_ANALYSIS/#purpose","title":"Purpose","text":"<p>Identifies genes that are differentially expressed between two groups (e.g., early vs. late stage cancer) using two complementary statistical methods:</p> <ol> <li>Limma - Linear Models for Microarray Data (parametric, empirical Bayes)</li> <li>Wilcoxon - Non-parametric rank-sum test with Cohen's d effect sizes</li> </ol>"},{"location":"advanced/R_ANALYSIS/#quick-start","title":"Quick Start","text":"<pre><code># Basic usage with default parameters\nRscript scripts/r_analysis/differential_expression.R\n\n# Custom analysis\nRscript scripts/r_analysis/differential_expression.R \\\n    --data-path data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv \\\n    --clinical-path data/interim/preprocessed_KIRC_data/clinical_data.csv \\\n    --output-dir results/differential_expression \\\n    --alpha 0.01 \\\n    --description KIRC_analysis\n\n# View help\nRscript scripts/r_analysis/differential_expression.R --help\n</code></pre>"},{"location":"advanced/R_ANALYSIS/#command-line-arguments","title":"Command-Line Arguments","text":"Argument Short Type Default Description <code>--data-path</code> <code>-d</code> string <code>preprocessed_rnaseq.csv</code> RNA-seq expression matrix file <code>--clinical-path</code> <code>-c</code> string <code>clinical_data.csv</code> Clinical metadata file <code>--output-dir</code> <code>-o</code> string <code>results/differential_expression</code> Base output directory <code>--alpha</code> <code>-a</code> numeric 0.01 Significance threshold (FDR/adjusted p-value) <code>--group1</code> <code>-g1</code> string \"early\" First comparison group name <code>--group2</code> <code>-g2</code> string \"late\" Second comparison group name <code>--description</code> <code>-x</code> string (timestamp) Output subdirectory suffix <code>--help</code> <code>-h</code> flag - Display help message"},{"location":"advanced/R_ANALYSIS/#input-data-format","title":"Input Data Format","text":""},{"location":"advanced/R_ANALYSIS/#rna-seq-expression-matrix","title":"RNA-seq Expression Matrix","text":"<p>Can be in either orientation (auto-detected):</p> <ul> <li>Genes \u00d7 Samples (preferred)</li> <li>Samples \u00d7 Genes (auto-transposed)</li> </ul> <p>CSV format: <pre><code>,Sample1,Sample2,Sample3,...\nGene1,5.23,4.89,6.12,...\nGene2,2.45,2.78,2.63,...\nGene3,8.91,9.23,8.75,...\n</code></pre></p> <p>Requirements:</p> <ul> <li>First column: Gene names/IDs</li> <li>Remaining columns: Sample expression values</li> <li>Values should be normalized (log2-transformed recommended)</li> </ul>"},{"location":"advanced/R_ANALYSIS/#clinical-metadata","title":"Clinical Metadata","text":"<p>CSV format: <pre><code>sample,stage,other_columns,...\nSample1,early,...\nSample2,late,...\nSample3,early,...\n</code></pre></p> <p>Requirements:</p> <ul> <li>Must have a <code>stage</code> column containing group labels</li> <li>Accepts: \"early\", \"late\", \"Early\", \"Late\", \"Stage I\", \"Stage II\", etc.</li> <li>Automatic mapping: Stage I/II \u2192 early, Stage III/IV \u2192 late</li> </ul>"},{"location":"advanced/R_ANALYSIS/#output-files","title":"Output Files","text":"<p>The script creates a timestamped directory structure:</p> <pre><code>results/differential_expression/YYYYMMDD_HHMMSS_description/\n\u251c\u2500\u2500 limma/\n\u2502   \u251c\u2500\u2500 sorted_table.csv              # All genes with limma statistics\n\u2502   \u251c\u2500\u2500 significant_genes.csv         # Significant genes (FDR &lt; alpha)\n\u2502   \u2514\u2500\u2500 log.txt                       # Limma analysis log\n\u251c\u2500\u2500 wilcox/\n\u2502   \u251c\u2500\u2500 wilcox_results.csv            # All genes with Wilcoxon statistics\n\u2502   \u251c\u2500\u2500 significant_genes.csv         # Significant genes (FDR &lt; alpha)\n\u2502   \u2514\u2500\u2500 log.txt                       # Wilcoxon analysis log\n\u251c\u2500\u2500 combined_results_all_genes.csv            # All genes from both methods\n\u251c\u2500\u2500 combined_results_both_significant.csv     # Consensus genes (both methods)\n\u251c\u2500\u2500 summary_statistics.csv                    # Summary of results\n\u2514\u2500\u2500 differential_expression_analysis.log      # Complete analysis log\n</code></pre>"},{"location":"advanced/R_ANALYSIS/#output-file-descriptions","title":"Output File Descriptions","text":""},{"location":"advanced/R_ANALYSIS/#limma-results-limmasorted_tablecsv","title":"Limma Results (<code>limma/sorted_table.csv</code>)","text":"Column Description <code>gene</code> Gene identifier <code>logFC</code> Log2 fold change (group2 vs group1) <code>AveExpr</code> Average expression across all samples <code>t</code> t-statistic <code>P.Value</code> Raw p-value <code>adj.P.Val</code> FDR-adjusted p-value (Benjamini-Hochberg) <code>B</code> Log-odds of differential expression"},{"location":"advanced/R_ANALYSIS/#wilcoxon-results-wilcoxwilcox_resultscsv","title":"Wilcoxon Results (<code>wilcox/wilcox_results.csv</code>)","text":"Column Description <code>gene</code> Gene identifier <code>p_value</code> Raw p-value from Wilcoxon test <code>fdr</code> FDR-adjusted p-value (Benjamini-Hochberg) <code>cohens_d</code> Cohen's d effect size <code>mean_group1</code> Mean expression in group1 <code>mean_group2</code> Mean expression in group2 <code>mean_diff</code> Difference in means (group2 - group1)"},{"location":"advanced/R_ANALYSIS/#combined-results-combined_results_both_significantcsv","title":"Combined Results (<code>combined_results_both_significant.csv</code>)","text":"<p>Consensus genes significant in both Limma and Wilcoxon tests.</p> Column Description <code>gene</code> Gene identifier <code>limma_logFC</code> Log2 fold change from Limma <code>limma_adj.P.Val</code> FDR from Limma <code>wilcox_fdr</code> FDR from Wilcoxon <code>wilcox_cohens_d</code> Effect size from Wilcoxon <code>avg_rank</code> Average rank across both methods"},{"location":"advanced/R_ANALYSIS/#script-2-gene-enrichment-analysis","title":"Script 2: Gene Enrichment Analysis","text":""},{"location":"advanced/R_ANALYSIS/#purpose_1","title":"Purpose","text":"<p>Performs functional enrichment analysis to identify biological pathways, processes, and functions that are over-represented in a gene list using the g:Profiler web service.</p>"},{"location":"advanced/R_ANALYSIS/#quick-start_1","title":"Quick Start","text":"<pre><code># Basic usage with default gene list\nRscript scripts/r_analysis/gene_enrichment.R\n\n# Custom gene list and parameters\nRscript scripts/r_analysis/gene_enrichment.R \\\n    --input data/external/my_genes.csv \\\n    --output reports/enrichment_analysis \\\n    --sources GO,KEGG,REAC,WP \\\n    --top-n 20 \\\n    --alpha 0.05\n\n# View help\nRscript scripts/r_analysis/gene_enrichment.R --help\n</code></pre>"},{"location":"advanced/R_ANALYSIS/#command-line-arguments_1","title":"Command-Line Arguments","text":"Argument Short Type Default Description <code>--input</code> <code>-i</code> string <code>genes.csv</code> Input gene list (single-column CSV) <code>--output</code> <code>-o</code> string (auto-generated) Output directory <code>--sources</code> <code>-s</code> string \"GO,REAC,KEGG,WP\" Databases to query (comma-separated) <code>--top-n</code> <code>-n</code> integer 20 Number of top terms per database <code>--organism</code> - string \"hsapiens\" Organism identifier <code>--alpha</code> <code>-a</code> numeric 0.05 FDR significance threshold <code>--width</code> <code>-w</code> numeric 25 Plot width (inches) <code>--height</code> - numeric 15 Plot height (inches) <code>--dpi</code> - integer 600 Plot resolution"},{"location":"advanced/R_ANALYSIS/#data-sources","title":"Data Sources","text":"Source Description <code>GO</code> Gene Ontology (BP, MF, CC) <code>KEGG</code> KEGG Pathways <code>REAC</code> Reactome Pathways <code>WP</code> WikiPathways"},{"location":"advanced/R_ANALYSIS/#input-data-format_1","title":"Input Data Format","text":"<p>Gene list CSV: <pre><code>gene\nTP53\nVEGFA\nHIF1A\nVHL\nPTEN\n...\n</code></pre></p> <p>Requirements: - CSV file with a column named \"gene\" (or first column if no header) - Gene symbols (e.g., \"TP53\") or Ensembl IDs - One gene per row - No duplicates</p> <p>Alternative formats accepted: <pre><code># Single column, no header\nTP53\nVEGFA\nHIF1A\n</code></pre></p>"},{"location":"advanced/R_ANALYSIS/#output-files_1","title":"Output Files","text":"<pre><code>reports/enrichment_analysis/YYYYMMDD_HHMMSS/\n\u251c\u2500\u2500 gostplot.pdf                  # Publication-quality plot (vector)\n\u251c\u2500\u2500 gostplot.png                  # High-resolution raster plot\n\u251c\u2500\u2500 gostres.csv                   # Full enrichment results (CSV)\n\u251c\u2500\u2500 gostres.tsv                   # Full enrichment results (TSV)\n\u251c\u2500\u2500 plot_data_gprofiler.csv       # Plot-ready data (CSV)\n\u251c\u2500\u2500 plot_data_gprofiler.tsv       # Plot-ready data (TSV)\n\u2514\u2500\u2500 gene_enrichment.log           # Analysis log\n</code></pre>"},{"location":"advanced/R_ANALYSIS/#output-file-descriptions_1","title":"Output File Descriptions","text":""},{"location":"advanced/R_ANALYSIS/#enrichment-results-gostrescsv","title":"Enrichment Results (<code>gostres.csv</code>)","text":"Column Description <code>term_id</code> Database term identifier (e.g., GO:0007049) <code>term_name</code> Human-readable term name <code>p_value</code> Raw p-value <code>adjusted_p_value</code> FDR-adjusted p-value <code>source</code> Database source (GO, KEGG, etc.) <code>term_size</code> Total genes in term <code>query_size</code> Genes queried <code>intersection_size</code> Genes found in term <code>intersection</code> List of genes in term <code>precision</code> Intersection / query size <code>recall</code> Intersection / term size"},{"location":"advanced/R_ANALYSIS/#plot-data-plot_data_gprofilercsv","title":"Plot Data (<code>plot_data_gprofiler.csv</code>)","text":"<p>Pre-processed data ready for custom plotting: - Filtered to significant terms (FDR &lt; threshold) - Sorted by significance - Top N terms per database</p>"},{"location":"advanced/R_ANALYSIS/#complete-pipeline-workflow","title":"Complete Pipeline Workflow","text":""},{"location":"advanced/R_ANALYSIS/#step-by-step-example","title":"Step-by-Step Example","text":"<pre><code># Step 1: Install R packages (one time)\nRscript scripts/r_analysis/install_r_packages.R\n\n# Step 2: Run differential expression analysis\nRscript scripts/r_analysis/differential_expression.R \\\n    --data-path data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv \\\n    --clinical-path data/interim/preprocessed_KIRC_data/clinical_data.csv \\\n    --alpha 0.01 \\\n    --description KIRC_early_vs_late\n\n# Step 3: Extract significant genes for enrichment\n# Option A: Use limma results\nhead -n 101 results/differential_expression/*/limma/significant_genes.csv &gt; my_genes.csv\n\n# Option B: Use consensus genes (both methods)\nhead -n 101 results/differential_expression/*/combined_results_both_significant.csv &gt; my_genes.csv\n\n# Step 4: Run enrichment analysis\nRscript scripts/r_analysis/gene_enrichment.R \\\n    --input my_genes.csv \\\n    --sources GO,KEGG,REAC \\\n    --top-n 15 \\\n    --alpha 0.05\n</code></pre>"},{"location":"advanced/R_ANALYSIS/#integration-with-python-pipeline","title":"Integration with Python Pipeline","text":"<p>The R scripts integrate seamlessly with the Python renalprog pipeline if R is installed in the same conda/mamba environment.</p>"},{"location":"advanced/R_ANALYSIS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/R_ANALYSIS/#package-installation-issues","title":"Package Installation Issues","text":""},{"location":"advanced/R_ANALYSIS/#problem-biocmanager-not-found","title":"Problem: BiocManager not found","text":"<pre><code># Solution: Install BiocManager first\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"limma\", \"edgeR\"))\n</code></pre>"},{"location":"advanced/R_ANALYSIS/#data-format-issues","title":"Data Format Issues","text":""},{"location":"advanced/R_ANALYSIS/#problem-no-stage-column-found","title":"Problem: \"No stage column found\"","text":"<p>Solution: Ensure clinical data has a <code>stage</code> column: <pre><code># Check column names\nclinical &lt;- read.csv(\"clinical_data.csv\")\ncolnames(clinical)\n</code></pre></p>"},{"location":"advanced/R_ANALYSIS/#output-issues","title":"Output Issues","text":""},{"location":"advanced/R_ANALYSIS/#problem-plots-not-generated","title":"Problem: Plots not generated","text":"<p>Solutions: <pre><code># Check ggplot2 installation\ninstall.packages(\"ggplot2\")\n\n# Reduce plot dimensions\nRscript scripts/r_analysis/gene_enrichment.R --width 15 --height 10\n\n# Check write permissions\ndir.create(\"output_directory\", recursive = TRUE)\n</code></pre></p>"},{"location":"advanced/R_ANALYSIS/#citation","title":"Citation","text":"<p>If you use these R scripts in your research, please cite:</p> <p>For differential expression (Limma):</p> <p>Ritchie, M. E., Phipson, B., Wu, D. I., Hu, Y., Law, C. W., Shi, W., &amp; Smyth, G. K. (2015). \"limma powers differential expression analyses for RNA-sequencing and microarray studies.\" Nucleic Acids Research, 43(7), e47.</p> <p>For enrichment analysis (g:Profiler):</p> <p>Kolberg, L., Raudvere, U., Kuzmin, I., Vilo, J., &amp; Peterson, H. (2020). \"gprofiler2--an R package for gene list functional enrichment analysis and namespace conversion toolset g:Profiler.\" F1000Research, 9, ELIXIR-709.</p>"},{"location":"advanced/R_ANALYSIS/#additional-resources","title":"Additional Resources","text":""},{"location":"advanced/R_ANALYSIS/#documentation","title":"Documentation","text":"<ul> <li>Limma User's Guide</li> <li>g:Profiler</li> <li>edgeR User's Guide</li> </ul>"},{"location":"advanced/R_ANALYSIS/#related-tutorials","title":"Related Tutorials","text":"<ul> <li>Full Pipeline</li> <li>Dynamic Enrichment Analysis Tutorial</li> <li>Classification Tutorial</li> </ul>"},{"location":"advanced/R_ANALYSIS/#external-links","title":"External Links","text":"<ul> <li>Bioconductor</li> <li>CRAN</li> </ul>"},{"location":"advanced/performance/","title":"Performance Optimization","text":"<p>The VAE model architecture can easily be tuned using Ax or Optuna.</p>"},{"location":"advanced/performance/#hyperparameter-optimization-with-ax","title":"Hyperparameter Optimization with Ax","text":"<p>This example shows how to optimize VAE hyperparameters using Adaptive Experimentation Platform (Ax).</p>"},{"location":"advanced/performance/#minimal-working-example","title":"Minimal Working Example","text":"<pre><code>\"\"\"\nMinimal example for VAE hyperparameter optimization using Ax.\n\nThis script optimizes:\n- Latent dimension size\n- Middle layer dimension\n- Learning rate\n- Beta (KL weight)\n\"\"\"\n\nfrom ax.service.ax_client import AxClient\nfrom ax.service.utils.instantiation import ObjectiveProperties\nimport torch\nimport numpy as np\nfrom pathlib import Path\n\nfrom renalprog import dataset\nfrom renalprog.config import VAEConfig\nfrom renalprog.modeling.train import train_vae\n\n# ============================================================================\n# 1. Setup: Load Data\n# ============================================================================\nprint(\"Loading data...\")\n\n# Load preprocessed data\nX_train, X_test, y_train, y_test, _, _ = dataset.create_train_test_split(\n    rnaseq_path=Path('data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv'),\n    clinical_path=Path('data/interim/preprocessed_KIRC_data/clinical_data.csv'),\n    test_size=0.2,\n    seed=2023,\n    output_dir=Path('data/interim/train_test_split')\n)\n\ninput_dim = X_train.shape[1]\nprint(f\"Input dimension: {input_dim}\")\nprint(f\"Training samples: {X_train.shape[0]}\")\n\n# ============================================================================\n# 2. Define Evaluation Function\n# ============================================================================\ndef evaluate_vae(parameterization):\n    \"\"\"\n    Train VAE with given hyperparameters and return validation loss.\n\n    Args:\n        parameterization: Dict with hyperparameters from Ax\n\n    Returns:\n        Dict with 'val_loss' metric\n    \"\"\"\n    # Extract hyperparameters\n    latent_dim = parameterization['latent_dim']\n    mid_dim = parameterization['mid_dim']\n    learning_rate = parameterization['learning_rate']\n    beta_ratio = parameterization['beta_ratio']\n\n    print(f\"\\nTrying: latent_dim={latent_dim}, mid_dim={mid_dim}, \"\n          f\"lr={learning_rate:.4f}, beta={beta_ratio:.2f}\")\n\n    # Configure VAE\n    vae_config = VAEConfig()\n    vae_config.INPUT_DIM = input_dim\n    vae_config.LATENT_DIM = latent_dim\n    vae_config.MID_DIM = mid_dim\n    vae_config.LEARNING_RATE = learning_rate\n    vae_config.BETA_RATIO = 0.5\n    vae_config.BETA_CYCLES = 3  # Single cycle for speed\n    vae_config.EPOCHS = 200 * vae_config.BETA_CYCLES  \n    vae_config.BATCH_SIZE = 32\n\n    # Train VAE\n    try:\n        vae_model, history = train_vae(\n            X_train=X_train,\n            X_test=X_test,\n            config=vae_config,\n            save_dir=None,  # Don't save intermediate models\n            force_cpu=False # Running on GPU is recommended\n        )\n\n        # Get average validation loss for the last 20 epochs\n        val_loss = np.mean(history['val_loss'][-20:])\n\n        print(f\"  \u2192 Validation loss: {val_loss:.4f}\")\n\n        return {'val_loss': (val_loss, 0.0)}  # (mean, sem)\n\n    except Exception as e:\n        print(f\"  \u2192 Training failed: {e}\")\n        return {'val_loss': (float('inf'), 0.0)}\n\n# ============================================================================\n# 3. Setup Ax Client\n# ============================================================================\nax_client = AxClient()\n\nax_client.create_experiment(\n    name=\"vae_optimization\",\n    parameters=[\n        {\n            \"name\": \"latent_dim\",\n            \"type\": \"range\",\n            \"bounds\": [64, 512],\n            \"value_type\": \"int\",\n            \"log_scale\": True,  # Search in log space\n        },\n        {\n            \"name\": \"mid_dim\",\n            \"type\": \"range\",\n            \"bounds\": [256, 2048],\n            \"value_type\": \"int\",\n            \"log_scale\": True,\n        },\n        {\n            \"name\": \"learning_rate\",\n            \"type\": \"range\",\n            \"bounds\": [1e-4, 1e-2],\n            \"value_type\": \"float\",\n            \"log_scale\": True,\n        },\n    ],\n    objectives={\n        \"val_loss\": ObjectiveProperties(minimize=True)\n    },\n)\n\n# ============================================================================\n# 4. Run Optimization\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"Starting Bayesian Optimization\")\nprint(\"=\"*80)\n\nn_trials = 20  # Number of configurations to try\n\nfor trial_idx in range(n_trials):\n    print(f\"\\n{'='*80}\")\n    print(f\"Trial {trial_idx + 1}/{n_trials}\")\n    print(f\"{'='*80}\")\n\n    # Get next parameters to try\n    parameters, trial_index = ax_client.get_next_trial()\n\n    # Evaluate\n    result = evaluate_vae(parameters)\n\n    # Report results back to Ax\n    ax_client.complete_trial(trial_index=trial_index, raw_data=result)\n\n# ============================================================================\n# 5. Get Best Configuration\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"OPTIMIZATION COMPLETE\")\nprint(\"=\"*80)\n\nbest_parameters, metrics = ax_client.get_best_parameters()\n\nprint(\"\\nBest hyperparameters found:\")\nfor param, value in best_parameters.items():\n    print(f\"  {param}: {value}\")\n\nprint(f\"\\nBest validation loss: {metrics[0]['val_loss']:.4f}\")\n\n# ============================================================================\n# 6. Train Final Model with Best Configuration\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training final model with best hyperparameters...\")\nprint(\"=\"*80)\n\nfinal_config = VAEConfig()\nfinal_config.INPUT_DIM = input_dim\nfinal_config.LATENT_DIM = best_parameters['latent_dim']\nfinal_config.MID_DIM = best_parameters['mid_dim']\nfinal_config.LEARNING_RATE = best_parameters['learning_rate']\nfinal_config.BETA_RATIO = best_parameters['beta_ratio']\nfinal_config.EPOCHS = 600  # Full training\nfinal_config.BATCH_SIZE = 8\nfinal_config.BETA_CYCLES = 3\n\nfinal_model, final_history = train_vae(\n    X_train=X_train,\n    X_test=X_test,\n    config=final_config,\n    save_dir=Path('models/optimized_vae'),\n    force_cpu=False\n)\n\nprint(f\"\\n Final model saved to: models/optimized_vae/\")\nprint(f\" Final validation loss: {final_history['val_loss'][-1]:.4f}\")\n</code></pre>"},{"location":"advanced/performance/#see-also","title":"See Also","text":"<ul> <li>Ax Documentation</li> <li>Bayesian Optimization in Pytorch</li> <li>VAE Configuration API</li> <li>VAE Training API</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API reference for the <code>renalprog</code> package. All functions, classes, and modules are documented here.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>renalprog/\n\u251c\u2500\u2500 __init__.py              # Package initialization\n\u251c\u2500\u2500 config.py                # Configuration and paths\n\u251c\u2500\u2500 dataset.py               # Data loading and splitting\n\u251c\u2500\u2500 features.py              # Preprocessing and feature engineering\n\u251c\u2500\u2500 plots.py                 # Visualization functions\n\u251c\u2500\u2500 modeling/                # Machine learning models\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 models.py           # VAE architectures\n\u2502   \u251c\u2500\u2500 train.py            # Training functions\n\u2502   \u251c\u2500\u2500 predict.py          # Prediction and inference\n\u2502   \u251c\u2500\u2500 trajectories.py     # Trajectory generation\n\u2502   \u251c\u2500\u2500 classification.py   # XGBoost classification\n\u2502   \u2514\u2500\u2500 enrichment.py       # GSEA integration\n\u2514\u2500\u2500 utils/                   # Utility functions\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 logging.py          # Logging configuration\n    \u2514\u2500\u2500 helpers.py          # Helper functions\n</code></pre>"},{"location":"api/#quick-links","title":"Quick Links","text":""},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li>Configuration - Paths and settings</li> <li>Dataset - Data loading and processing</li> <li>Features - Preprocessing and filtering</li> </ul>"},{"location":"api/#modeling","title":"Modeling","text":"<ul> <li>VAE Models - Variational autoencoders</li> <li>Training - Model training</li> <li>Prediction - Inference and generation</li> <li>Trajectories - Synthetic progression</li> <li>Classification - Stage prediction</li> <li>Enrichment - Pathway analysis</li> </ul>"},{"location":"api/#visualization","title":"Visualization","text":"<ul> <li>Plots - All plotting functions</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<ul> <li>Utils - Helper functions and logging</li> </ul>"},{"location":"api/#installation","title":"Installation","text":"<pre><code>pip install renalprog\n</code></pre> <p>Or for development:</p> <pre><code>git clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\npip install -e .\n</code></pre>"},{"location":"api/#basic-usage","title":"Basic Usage","text":""},{"location":"api/#import-the-package","title":"Import the Package","text":"<pre><code>import renalprog\nfrom renalprog import config, dataset, features, modeling, plots\n</code></pre>"},{"location":"api/#load-data","title":"Load Data","text":"<pre><code># Load preprocessed data\nrnaseq = dataset.load_data('data/processed/rnaseq_maha.csv')\nclinical = dataset.load_data('data/processed/clinical.csv')\n\n# Create train/test split\nX_train, X_test, y_train, y_test = dataset.create_train_test_split(\n    rnaseq_path='data/processed/rnaseq_maha.csv',\n    clinical_path='data/processed/clinical.csv',\n    test_size=0.2,\n    seed=2023\n)\n</code></pre>"},{"location":"api/#train-a-model","title":"Train a Model","text":"<pre><code>from renalprog.modeling import VAE, train_vae\n\n# Initialize VAE\nvae = VAE(\n    input_dim=X_train.shape[1],\n    latent_dim=256,\n    hidden_dims=[512, 256]\n)\n\n# Train\nvae.fit(X_train, epochs=100, batch_size=32)\n\n# Save\nvae.save('models/my_vae.pt')\n</code></pre>"},{"location":"api/#generate-trajectories","title":"Generate Trajectories","text":"<pre><code>from renalprog.modeling import generate_trajectories\n\ntrajectories = generate_trajectories(\n    vae=vae,\n    start_samples=early_stage_samples,\n    end_samples=late_stage_samples,\n    n_trajectories=100,\n    n_timepoints=20\n)\n</code></pre>"},{"location":"api/#run-classification","title":"Run Classification","text":"<pre><code>from renalprog.modeling.classification import train_stage_classifier\n\nclf, metrics, shap_values = train_stage_classifier(\n    X=X_train,\n    y=y_train,\n    feature_names=gene_names,\n    n_folds=5\n)\n</code></pre>"},{"location":"api/#visualize","title":"Visualize","text":"<pre><code>from renalprog import plots\n\n# Plot training history\nplots.plot_training_history(\n    history,\n    save_path='figures/training.png'\n)\n</code></pre>"},{"location":"api/#module-details","title":"Module Details","text":""},{"location":"api/#renalprogconfig","title":"renalprog.config","text":"<p>Configuration module with paths and constants.</p> <p>Key Components:</p> <ul> <li><code>PATHS</code>: Dictionary of all project paths</li> <li><code>VAEConfig</code>: VAE hyperparameter configuration</li> <li><code>get_dated_dir()</code>: Create dated output directories</li> <li><code>KIRCPaths</code>: KIRC-specific data paths</li> </ul> <p>Example: <pre><code>from renalprog.config import PATHS, VAEConfig\n\n# Access paths\ndata_dir = PATHS['data']\nmodels_dir = PATHS['models']\n\n# Configure VAE\nconfig = VAEConfig()\nconfig.LATENT_DIM = 256\nconfig.EPOCHS = 600\n</code></pre></p> <p>See: Configuration Reference</p>"},{"location":"api/#renalprogdataset","title":"renalprog.dataset","text":"<p>Data loading, processing, and splitting utilities.</p> <p>Key Functions:</p> <ul> <li><code>download_data()</code>: Download TCGA data from Xena</li> <li><code>process_downloaded_data()</code>: Process raw TCGA files</li> <li><code>load_data()</code>: Load CSV files</li> <li><code>create_train_test_split()</code>: Stratified train/test split</li> </ul> <p>Example: <pre><code>from renalprog import dataset\n\n# Download data\nrnaseq, clinical, pheno = dataset.download_data(\n    destination='data/raw',\n    cancer_type='KIRC'\n)\n\n# Create split\nX_train, X_test, y_train, y_test = dataset.create_train_test_split(\n    rnaseq_path='data/processed/rnaseq.csv',\n    clinical_path='data/processed/clinical.csv',\n    test_size=0.2,\n    seed=2023\n)\n</code></pre></p> <p>See: Dataset Reference</p>"},{"location":"api/#renalprogfeatures","title":"renalprog.features","text":"<p>Preprocessing and feature engineering.</p> <p>Key Functions:</p> <ul> <li><code>preprocess_rnaseq()</code>: Filter and normalize gene expression</li> <li><code>filter_low_expression()</code>: Remove lowly expressed genes</li> <li><code>detect_outliers()</code>: Mahalanobis distance outlier detection</li> <li><code>normalize()</code>: Various normalization methods</li> </ul> <p>Example: <pre><code>from renalprog import features\n\n# Preprocess RNA-seq data\nprocessed, info = features.preprocess_rnaseq(\n    data=rnaseq,\n    filter_expression=True,\n    detect_outliers=True,\n    mean_threshold=0.5,\n    var_threshold=0.5,\n    alpha=0.05\n)\n</code></pre></p> <p>See: Features Reference</p>"},{"location":"api/#renalprogmodeling","title":"renalprog.modeling","text":"<p>Machine learning models and training.</p> <p>Submodules:</p> <ul> <li><code>models</code>: VAE architectures</li> <li><code>train</code>: Training functions</li> <li><code>predict</code>: Inference and generation</li> <li><code>trajectories</code>: Synthetic trajectory generation</li> <li><code>classification</code>: XGBoost stage classification</li> <li><code>enrichment</code>: GSEA integration</li> </ul> <p>Example: <pre><code>from renalprog.modeling import VAE, train_vae, generate_trajectories\n\n# Train VAE\nvae, history = train_vae(\n    X_train=X_train,\n    X_test=X_test,\n    config=vae_config\n)\n\n# Generate trajectories\ntrajectories = generate_trajectories(\n    vae=vae,\n    start_samples=early_samples,\n    end_samples=late_samples,\n    n_trajectories=100\n)\n</code></pre></p> <p>See: - Models Reference - Training Reference - Trajectories Reference - Classification Reference - Enrichment Reference</p>"},{"location":"api/#renalprogplots","title":"renalprog.plots","text":"<p>Visualization functions for all analysis steps.</p> <p>Key Functions:</p> <ul> <li><code>plot_training_history()</code>: Loss curves</li> <li><code>plot_reconstruction()</code>: Original vs reconstructed</li> <li><code>plot_trajectories()</code>: Trajectory visualization</li> <li><code>plot_classification_metrics()</code>: ROC, confusion matrix</li> <li><code>plot_enrichment_heatmap()</code>: Pathway enrichment</li> </ul> <p>See: Plots Reference</p>"},{"location":"api/#renalprogutils","title":"renalprog.utils","text":"<p>Utility functions and helpers.</p> <p>Key Functions:</p> <ul> <li><code>configure_logging()</code>: Set up logging</li> <li><code>set_seed()</code>: Set random seeds for reproducibility</li> <li><code>Timer</code>: Context manager for timing code</li> <li><code>check_file_exists()</code>: File validation</li> </ul> <p>Example: <pre><code>from renalprog.utils import configure_logging, set_seed, Timer\n\n# Configure logging\nconfigure_logging(level='INFO')\n\n# Set random seed\nset_seed(2023)\n\n# Time code execution\nwith Timer(\"Training\"):\n    vae.fit(X_train, epochs=100)\n</code></pre></p> <p>See: Utils Reference</p>"},{"location":"api/#advanced-topics","title":"Advanced Topics","text":""},{"location":"api/#custom-vae-architectures","title":"Custom VAE Architectures","text":"<pre><code>from renalprog.modeling.models import BaseVAE\nimport torch.nn as nn\n\nclass CustomVAE(BaseVAE):\n    def __init__(self, input_dim, latent_dim):\n        super().__init__(input_dim, latent_dim)\n\n        # Custom encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n        )\n\n        self.fc_mu = nn.Linear(512, latent_dim)\n        self.fc_logvar = nn.Linear(512, latent_dim)\n\n        # Custom decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, input_dim)\n        )\n</code></pre>"},{"location":"api/#custom-preprocessing","title":"Custom Preprocessing","text":"<pre><code>from renalprog.features import preprocess_rnaseq\n\ndef custom_preprocess(data, **kwargs):\n    \"\"\"Custom preprocessing pipeline.\"\"\"\n\n    # Step 1: Log transform\n    data_log = np.log2(data + 1)\n\n    # Step 2: Quantile normalization\n    data_norm = quantile_normalize(data_log)\n\n    # Step 3: Standard preprocessing\n    data_processed, info = preprocess_rnaseq(\n        data_norm,\n        **kwargs\n    )\n\n    return data_processed, info\n</code></pre>"},{"location":"api/#batch-processing","title":"Batch Processing","text":"<pre><code>from renalprog.modeling import generate_trajectories\nimport glob\n\n# Process multiple experiments\nfor experiment_dir in glob.glob('data/interim/experiment_*'):\n    vae = VAE.load(f'{experiment_dir}/vae_model.pt')\n\n    trajectories = generate_trajectories(\n        vae=vae,\n        start_samples=early,\n        end_samples=late,\n        n_trajectories=100\n    )\n\n    trajectories.to_csv(f'{experiment_dir}/trajectories.csv')\n</code></pre>"},{"location":"api/#type-hints","title":"Type Hints","text":"<p>The package uses type hints throughout:</p> <pre><code>from typing import Tuple, Optional, Dict, List\nimport pandas as pd\nimport numpy as np\nimport torch\n\ndef preprocess_rnaseq(\n    data: pd.DataFrame,\n    filter_expression: bool = True,\n    mean_threshold: float = 0.5,\n    var_threshold: float = 0.5,\n    detect_outliers: bool = True,\n    alpha: float = 0.05,\n    seed: Optional[int] = None\n) -&gt; Tuple[pd.DataFrame, Dict[str, any]]:\n    \"\"\"Preprocess RNA-seq data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>Common exceptions:</p> <pre><code>from renalprog.dataset import load_data\n\ntry:\n    data = load_data('nonexistent_file.csv')\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\nexcept pd.errors.EmptyDataError:\n    print(\"File is empty\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/#performance","title":"Performance","text":""},{"location":"api/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Use GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nvae = VAE(\n    input_dim=5000,\n    latent_dim=256,\n    device=device\n)\n</code></pre>"},{"location":"api/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from renalprog.enrichment import run_gsea_parallel\n\n# Use multiple cores\nrun_gsea_parallel(\n    deseq_dir='data/processed/deseq',\n    n_threads=8\n)\n</code></pre>"},{"location":"api/#testing","title":"Testing","text":"<p>Run the test suite:</p> <pre><code># All tests\npytest\n\n# Specific module\npytest tests/test_dataset.py\n\n# With coverage\npytest --cov=renalprog --cov-report=html\n</code></pre>"},{"location":"api/#contributing","title":"Contributing","text":"<p>See Contributing Guidelines for development information.</p>"},{"location":"api/#version-history","title":"Version History","text":"<ul> <li>v0.1.0 (2024-12): Initial release</li> <li>VAE training pipeline</li> <li>Trajectory generation</li> <li>XGBoost classification</li> <li>GSEA integration</li> </ul>"},{"location":"api/#license","title":"License","text":"<p>Apache License 2.0. See LICENSE.</p>"},{"location":"api/#citation","title":"Citation","text":"<pre><code>@software{renalprog2024,\n  author = {Prol-Castelo, Guillermo},\n  title = {renalprog: Cancer Progression Forecasting with Generative AI},\n  year = {2024},\n  url = {https://github.com/gprolcastelo/renalprog}\n}\n</code></pre>"},{"location":"api/classification/","title":"Classification API","text":"<p>Functions for trajectory classification and survival analysis.</p>"},{"location":"api/classification/#overview","title":"Overview","text":"<p>The classification module provides:</p> <ul> <li>Trajectory-based classification</li> <li>Survival prediction</li> <li>Classifier training and evaluation</li> <li>Feature importance analysis</li> </ul>"},{"location":"api/classification/#main-classification-function","title":"Main Classification Function","text":""},{"location":"api/classification/#classify_trajectories","title":"classify_trajectories","text":"<p>Train classifier to predict disease progression from trajectories.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import classify_trajectories\nimport pandas as pd\nfrom pathlib import Path\n\n# Load trajectory data\ntrajectories = pd.read_csv(\"data/processed/trajectories.csv\")\nlabels = pd.read_csv(\"data/processed/progression_labels.csv\")\n\n# Train classifier\nclassifier, metrics = classify_trajectories(\n    trajectories=trajectories.values,\n    labels=labels['progressed'].values,\n    output_dir=Path(\"models/trajectory_classifier\"),\n    model_type='random_forest',  # or 'logistic', 'svm', 'gradient_boosting'\n    test_size=0.2,\n    random_state=42\n)\n\n# Print performance\nprint(f\"Accuracy: {metrics['accuracy']:.3f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.3f}\")\nprint(f\"Precision: {metrics['precision']:.3f}\")\nprint(f\"Recall: {metrics['recall']:.3f}\")\nprint(f\"F1 Score: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"api/classification/#renalprog.modeling.predict.classify_trajectories","title":"classify_trajectories","text":"<pre><code>classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, DataFrame],\n    gene_subset: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Apply stage classifier to synthetic trajectories.</p> <p>Args:     classifier: Trained classifier model     trajectory_data: Dictionary of patient pair to trajectory DataFrames     gene_subset: Optional subset of genes to use for classification</p> <p>Returns:     DataFrame with classification results for each trajectory point</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, pd.DataFrame],\n    gene_subset: Optional[List[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply stage classifier to synthetic trajectories.\n\n    Args:\n        classifier: Trained classifier model\n        trajectory_data: Dictionary of patient pair to trajectory DataFrames\n        gene_subset: Optional subset of genes to use for classification\n\n    Returns:\n        DataFrame with classification results for each trajectory point\n    \"\"\"\n    logger.info(\"Classifying trajectory points\")\n\n    # TODO: Implement trajectory classification\n    # Migrate from notebooks/kirc_classification_trajectory.ipynb\n\n    raise NotImplementedError(\n        \"classify_trajectories() needs implementation from \"\n        \"notebooks/kirc_classification_trajectory.ipynb\"\n    )\n</code></pre>"},{"location":"api/classification/#classification-models","title":"Classification Models","text":"<p>The <code>classify_trajectories</code> function supports multiple model types:</p>"},{"location":"api/classification/#random-forest","title":"Random Forest","text":"<p>Default choice for interpretability and feature importance:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='random_forest',\n    n_estimators=100,\n    max_depth=None,\n    min_samples_split=2\n)\n\n# Feature importance\nimportances = classifier.feature_importances_\n</code></pre>"},{"location":"api/classification/#logistic-regression","title":"Logistic Regression","text":"<p>For linear decision boundaries:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='logistic',\n    C=1.0,\n    penalty='l2',\n    max_iter=1000\n)\n</code></pre>"},{"location":"api/classification/#support-vector-machine","title":"Support Vector Machine","text":"<p>For complex decision boundaries:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='svm',\n    kernel='rbf',\n    C=1.0,\n    gamma='scale'\n)\n</code></pre>"},{"location":"api/classification/#gradient-boosting","title":"Gradient Boosting","text":"<p>For maximum performance:</p> <pre><code>classifier, metrics = classify_trajectories(\n    trajectories, labels,\n    model_type='gradient_boosting',\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3\n)\n</code></pre>"},{"location":"api/classification/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The classification function returns macro-metrics:</p> Metric Description <code>accuracy</code> Overall classification accuracy <code>auc_roc</code> Area under ROC curve <code>precision</code> Positive predictive value <code>recall</code> Sensitivity/True positive rate <code>f1</code> Harmonic mean of precision and recall 'Cohen Kappa' Cohen's Kappa"},{"location":"api/classification/#complete-classification-workflow","title":"Complete Classification Workflow","text":"<pre><code>import torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nfrom renalprog.modeling.train import VAE\nfrom renalprog.modeling.predict import (\n    apply_vae,\n    generate_trajectories,\n    classify_trajectories\n)\n\n# 1. Load model and data\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel.load_state_dict(torch.load(\"models/my_vae/best_model.pt\"))\n\nexpr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# 2. Generate trajectories for each patient\nearly_mask = clinical['stage'] == 'early'\nlate_mask = clinical['stage'] == 'late'\n\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=expr.values[early_mask],\n    end_data=expr.values[late_mask],\n    n_steps=50,\n    interpolation='spherical',\n    device='cuda'\n)\n\n# 3. Extract trajectory features\n# Option A: Use trajectory statistics (mean, std, slope)\ntrajectory_features = []\nfor traj in trajectories:\n    features = np.concatenate([\n        traj.mean(axis=0),  # Mean expression\n        traj.std(axis=0),   # Variance\n        (traj[-1] - traj[0])  # Net change\n    ])\n    trajectory_features.append(features)\ntrajectory_features = np.array(trajectory_features)\n\n# Option B: Use latent trajectory\nresults = apply_vae(model, expr.values, device='cuda')\nlatent_trajectories = results['latent']\n\n# 4. Create labels (e.g., based on survival)\nlabels = clinical['progressed'].values[early_mask]\n\n# 5. Train classifier\nclassifier, metrics = classify_trajectories(\n    trajectories=trajectory_features,\n    labels=labels,\n    output_dir=Path(\"models/trajectory_classifier\"),\n    model_type='random_forest',\n    test_size=0.2,\n    random_state=42\n)\n\n# 6. Feature importance (for tree-based models)\nif hasattr(classifier, 'feature_importances_'):\n    importances = pd.DataFrame({\n        'feature': [f'feature_{i}' for i in range(len(classifier.feature_importances_))],\n        'importance': classifier.feature_importances_\n    }).sort_values('importance', ascending=False)\n\n    print(\"Top 10 important features:\")\n    print(importances.head(10))\n\n# 7. Save results\nresults_df = pd.DataFrame({\n    'patient_id': clinical.index[early_mask],\n    'true_label': labels,\n    'predicted_label': classifier.predict(trajectory_features),\n    'prediction_proba': classifier.predict_proba(trajectory_features)[:, 1]\n})\nresults_df.to_csv(\"reports/classification_results.csv\", index=False)\n\nprint(\"\\nClassification Performance:\")\nprint(f\"Accuracy: {metrics['accuracy']:.3f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.3f}\")\nprint(f\"F1 Score: {metrics['f1']:.3f}\")\n</code></pre>"},{"location":"api/classification/#cross-validation","title":"Cross-Validation","text":"<p>For robust performance estimation:</p> <pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Cross-validation\ncv_scores = cross_val_score(\n    rf, trajectory_features, labels,\n    cv=5,\n    scoring='roc_auc'\n)\n\nprint(f\"CV AUC-ROC: {cv_scores.mean():.3f} \u00b1 {cv_scores.std():.3f}\")\n</code></pre>"},{"location":"api/classification/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid search\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    rf, param_grid,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(trajectory_features, labels)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV AUC-ROC: {grid_search.best_score_:.3f}\")\n</code></pre>"},{"location":"api/classification/#see-also","title":"See Also","text":"<ul> <li>Trajectories API - Generate trajectories</li> <li>Prediction API - Apply VAE models</li> <li>Plots API - Visualization tools</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/config/","title":"Configuration API","text":"<p>The <code>config</code> module provides centralized configuration management for the RenalProg pipeline, including paths, hyperparameters, and project-wide constants.</p>"},{"location":"api/config/#overview","title":"Overview","text":"<p>The configuration module is organized into several components:</p> <ul> <li>Path Management: Centralized directory structure for data, models, and outputs</li> <li>Preprocessing Configuration: Parameters for data preprocessing</li> <li>VAE Configuration: Hyperparameters for VAE model training</li> <li>Trajectory Configuration: Settings for trajectory generation</li> <li>Classification Configuration: Parameters for survival classification</li> <li>Enrichment Configuration: Settings for pathway enrichment analysis</li> </ul>"},{"location":"api/config/#path-structure","title":"Path Structure","text":"<p>The module defines a comprehensive directory structure:</p> <pre><code>from renalprog.config import PATHS\n\n# Access common paths\ndata_dir = PATHS['data']\nmodels_dir = PATHS['models']\nfigures_dir = PATHS['figures']\n</code></pre>"},{"location":"api/config/#available-paths","title":"Available Paths","text":"Key Description <code>root</code> Project root directory <code>data</code> Main data directory <code>raw</code> Raw, immutable data <code>interim</code> Intermediate processed data <code>processed</code> Final, canonical data sets <code>external</code> External data sources (pathways, gene lists) <code>models</code> Trained models and checkpoints <code>reports</code> Analysis reports <code>figures</code> Generated figures and plots <code>notebooks</code> Jupyter notebooks <code>references</code> Reference materials <code>scripts</code> Pipeline scripts"},{"location":"api/config/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/config/#preprocessingconfig","title":"PreprocessingConfig","text":"<p>Configuration for data preprocessing steps.</p> <p>Example Usage:</p> <pre><code>from renalprog.config import PreprocessingConfig\n\n# Access default preprocessing parameters\nconfig = PreprocessingConfig()\nprint(f\"Mean threshold: {config.mean_threshold}\")\nprint(f\"Outlier alpha: {config.outlier_alpha}\")\nprint(f\"Test split ratio: {config.test_size}\")\n</code></pre>"},{"location":"api/config/#renalprog.config.PreprocessingConfig","title":"PreprocessingConfig","text":"<p>Parameters for data preprocessing.</p>"},{"location":"api/config/#vaeconfig","title":"VAEConfig","text":"<p>Configuration for VAE model architecture and training.</p> <p>Example Usage:</p> <pre><code>from renalprog.config import VAEConfig\n\n# Use default VAE configuration\nconfig = VAEConfig()\n\n# Or customize for your experiment\ncustom_config = VAEConfig(\n    mid_dim=256,\n    latent_dim=10,\n    learning_rate=0.0001,\n    num_epochs=500,\n    batch_size=64\n)\n</code></pre>"},{"location":"api/config/#renalprog.config.VAEConfig","title":"VAEConfig","text":"<p>Default hyperparameters for VAE training.</p>"},{"location":"api/config/#trajectoryconfig","title":"TrajectoryConfig","text":"<p>Configuration for trajectory generation from trained VAE models.</p> <p>Example Usage:</p> <pre><code>from renalprog.config import TrajectoryConfig\n\nconfig = TrajectoryConfig(\n    num_trajectories=5000,\n    trajectory_length=100,\n    noise_scale=0.1\n)\n</code></pre>"},{"location":"api/config/#renalprog.config.TrajectoryConfig","title":"TrajectoryConfig","text":"<p>Parameters for synthetic trajectory generation.</p>"},{"location":"api/config/#classificationconfig","title":"ClassificationConfig","text":"<p>Configuration for survival classification models.</p>"},{"location":"api/config/#renalprog.config.ClassificationConfig","title":"ClassificationConfig","text":"<p>Parameters for stage classification.</p>"},{"location":"api/config/#enrichmentconfig","title":"EnrichmentConfig","text":"<p>Configuration for pathway enrichment analysis.</p>"},{"location":"api/config/#renalprog.config.EnrichmentConfig","title":"EnrichmentConfig","text":"<p>Parameters for pathway enrichment analysis.</p>"},{"location":"api/config/#utility-functions","title":"Utility Functions","text":""},{"location":"api/config/#get_dated_dir","title":"get_dated_dir","text":"<p>Create a dated directory for organizing time-stamped outputs.</p> <p>Example:</p> <pre><code>from renalprog.config import get_dated_dir, PATHS\n\n# Create a dated directory for today's model outputs\nmodel_dir = get_dated_dir(PATHS['models'], prefix='VAE_KIRC')\n# Returns: models/20251218_VAE_KIRC/\n</code></pre>"},{"location":"api/config/#renalprog.config.get_dated_dir","title":"get_dated_dir","text":"<pre><code>get_dated_dir(base_dir: Path, prefix: str = '') -&gt; Path\n</code></pre> <p>Create a dated directory for organizing outputs.</p> <p>Args:     base_dir: Base directory path     prefix: Optional prefix for the directory name</p> <p>Returns:     Path to the dated directory</p> Source code in <code>renalprog/config.py</code> <pre><code>def get_dated_dir(base_dir: Path, prefix: str = \"\") -&gt; Path:\n    \"\"\"\n    Create a dated directory for organizing outputs.\n\n    Args:\n        base_dir: Base directory path\n        prefix: Optional prefix for the directory name\n\n    Returns:\n        Path to the dated directory\n    \"\"\"\n    today = datetime.now().strftime(\"%Y%m%d\")\n    if prefix:\n        dir_name = f\"{today}_{prefix}\"\n    else:\n        dir_name = today\n    dated_dir = base_dir / dir_name\n    dated_dir.mkdir(parents=True, exist_ok=True)\n    return dated_dir\n</code></pre>"},{"location":"api/config/#kirc-specific-paths","title":"KIRC-Specific Paths","text":"<p>The module provides a <code>KIRCPaths</code> class for managing KIRC dataset-specific file locations:</p> <pre><code>from renalprog.config import KIRCPaths\n\n# Access KIRC-specific data paths\nrnaseq = KIRCPaths.RNASEQ_RAW\nclinical = KIRCPaths.CLINICAL_RAW\npathways = KIRCPaths.REACTOME_PATHWAYS\n</code></pre>"},{"location":"api/config/#best-practices","title":"Best Practices","text":""},{"location":"api/config/#using-configuration-in-scripts","title":"Using Configuration in Scripts","text":"<p>Always import configuration at the module level:</p> <pre><code>from renalprog.config import PATHS, VAEConfig, PreprocessingConfig\n\ndef main():\n    # Access paths\n    data_dir = PATHS['interim']\n\n    # Use configuration objects\n    vae_config = VAEConfig()\n    preproc_config = PreprocessingConfig()\n</code></pre>"},{"location":"api/config/#creating-custom-configurations","title":"Creating Custom Configurations","text":"<p>For experiments, create configuration variants:</p> <pre><code>from renalprog.config import VAEConfig\n\n# Base configuration\nbase_config = VAEConfig()\n\n# Experiment variations\nconfigs = {\n    'small': VAEConfig(mid_dim=128, latent_dim=5),\n    'medium': VAEConfig(mid_dim=256, latent_dim=10),\n    'large': VAEConfig(mid_dim=512, latent_dim=20)\n}\n</code></pre>"},{"location":"api/config/#saving-configuration","title":"Saving Configuration","text":"<p>Always save configuration with trained models:</p> <pre><code>from renalprog.modeling.checkpointing import save_model_config\n\nconfig = VAEConfig(experiment_name='my_experiment')\nsave_model_config(config, output_dir)\n</code></pre>"},{"location":"api/config/#see-also","title":"See Also","text":"<ul> <li>Dataset API - Data loading and preprocessing</li> <li>Training API - Model training with configuration</li> </ul>"},{"location":"api/dataset/","title":"Dataset API","text":"<p>The <code>dataset</code> module provides functions for downloading, loading, and preparing RNA-seq data for analysis.</p>"},{"location":"api/dataset/#overview","title":"Overview","text":"<p>This module handles:</p> <ul> <li>Downloading preprocessed data from Zenodo (fastest way to get started!)</li> <li>Downloading raw TCGA Pan-Cancer Atlas data</li> <li>Processing and filtering by cancer type</li> <li>Creating train/test splits</li> <li>Loading preprocessed data for modeling</li> </ul> <p>Quick Start</p> <p>Use <code>download_preprocessed_from_zenodo()</code> to quickly download ready-to-use datasets from Zenodo. This bypasses all preprocessing steps! Otherwise, use <code>download_data()</code> and <code>process_downloaded_data()</code> to process raw TCGA data with custom parameters.</p>"},{"location":"api/dataset/#core-functions","title":"Core Functions","text":""},{"location":"api/dataset/#download_preprocessed_from_zenodo","title":"download_preprocessed_from_zenodo","text":"<p>Download preprocessed RNAseq and clinical data directly from Zenodo repositories.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import download_preprocessed_from_zenodo\nfrom pathlib import Path\n\n# Download preprocessed KIRC data\nrnaseq, clinical = download_preprocessed_from_zenodo(\n    rnaseq_url='https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1',\n    clinical_url='https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1',\n    output_dir=Path('data/interim/preprocessed_KIRC_data')\n)\n\nprint(f\"RNAseq shape: {rnaseq.shape}\")\nprint(f\"Clinical shape: {clinical.shape}\")\n\n# Download preprocessed BRCA data\nrnaseq_brca, clinical_brca = download_preprocessed_from_zenodo(\n    rnaseq_url='https://zenodo.org/records/17986123/files/Static_BRCA.csv?download=1',\n    clinical_url='https://zenodo.org/records/17986123/files/nodes_metadata.csv?download=1',\n    output_dir=Path('data/interim/preprocessed_BRCA_data')\n)\n</code></pre> <p>Available Zenodo Datasets:</p> Cancer Type Zenodo Record RNAseq File Clinical File KIRC (Kidney) 17987300 <code>Static_KIRC.csv</code> <code>nodes_metadata.csv</code> BRCA (Breast) 17986123 <code>Static_BRCA.csv</code> <code>nodes_metadata.csv</code> <p>Quick Start</p> <p>This is the fastest way to get started! The data is already preprocessed, normalized, and ready for modeling.</p> <p>What's Included</p> <ul> <li>RNAseq Data: Log2-transformed, normalized gene expression matrix</li> <li>Clinical Data: Patient metadata including stage, survival, demographics</li> <li>Automatic Validation: Function checks sample overlap and data integrity</li> </ul>"},{"location":"api/dataset/#renalprog.dataset.download_preprocessed_from_zenodo","title":"download_preprocessed_from_zenodo","text":"<pre><code>download_preprocessed_from_zenodo(\n    rnaseq_url: str,\n    clinical_url: str,\n    output_dir: Path,\n    rnaseq_filename: str = \"preprocessed_rnaseq.csv\",\n    clinical_filename: str = \"clinical_data.csv\",\n    timeout: int = 300,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Download preprocessed RNAseq and clinical data from Zenodo.</p> <p>This function downloads already preprocessed datasets from Zenodo repositories, which bypasses the need for raw data processing. The downloaded data includes normalized RNAseq expression matrices and curated clinical metadata.</p> <p>Args:     rnaseq_url: Direct download URL for the preprocessed RNAseq CSV file from Zenodo.         Example: 'https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1'     clinical_url: Direct download URL for the clinical metadata CSV file from Zenodo.         Example: 'https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1'     output_dir: Directory where downloaded files will be saved.     rnaseq_filename: Filename to save the RNAseq data as (default: \"preprocessed_rnaseq.csv\").     clinical_filename: Filename to save the clinical data as (default: \"clinical_data.csv\").     timeout: Request timeout in seconds (default: 300).</p> <p>Returns:     Tuple of (rnaseq_data, clinical_data) as pandas DataFrames:         - rnaseq_data: Gene expression matrix (genes \u00d7 samples)         - clinical_data: Clinical metadata (samples \u00d7 features)</p> <p>Raises:     requests.RequestException: If download fails     IOError: If file I/O operations fail     ValueError: If downloaded data cannot be parsed</p> <p>Notes:     - Downloaded files are automatically saved to the specified output directory     - Progress is logged during download     - Data is validated after download to ensure proper format</p> <p>Examples:     &gt;&gt;&gt; # Download preprocessed KIRC data     &gt;&gt;&gt; rnaseq, clinical = download_preprocessed_from_zenodo(     ...     rnaseq_url='https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1',     ...     clinical_url='https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1',     ...     output_dir=Path('data/interim/preprocessed_KIRC_data')     ... )</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def download_preprocessed_from_zenodo(\n        rnaseq_url: str,\n        clinical_url: str,\n        output_dir: Path,\n        rnaseq_filename: str = \"preprocessed_rnaseq.csv\",\n        clinical_filename: str = \"clinical_data.csv\",\n        timeout: int = 300) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Download preprocessed RNAseq and clinical data from Zenodo.\n\n    This function downloads already preprocessed datasets from Zenodo repositories,\n    which bypasses the need for raw data processing. The downloaded data includes\n    normalized RNAseq expression matrices and curated clinical metadata.\n\n    Args:\n        rnaseq_url: Direct download URL for the preprocessed RNAseq CSV file from Zenodo.\n            Example: 'https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1'\n        clinical_url: Direct download URL for the clinical metadata CSV file from Zenodo.\n            Example: 'https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1'\n        output_dir: Directory where downloaded files will be saved.\n        rnaseq_filename: Filename to save the RNAseq data as (default: \"preprocessed_rnaseq.csv\").\n        clinical_filename: Filename to save the clinical data as (default: \"clinical_data.csv\").\n        timeout: Request timeout in seconds (default: 300).\n\n    Returns:\n        Tuple of (rnaseq_data, clinical_data) as pandas DataFrames:\n            - rnaseq_data: Gene expression matrix (genes \u00d7 samples)\n            - clinical_data: Clinical metadata (samples \u00d7 features)\n\n    Raises:\n        requests.RequestException: If download fails\n        IOError: If file I/O operations fail\n        ValueError: If downloaded data cannot be parsed\n\n    Notes:\n        - Downloaded files are automatically saved to the specified output directory\n        - Progress is logged during download\n        - Data is validated after download to ensure proper format\n\n    Examples:\n        &gt;&gt;&gt; # Download preprocessed KIRC data\n        &gt;&gt;&gt; rnaseq, clinical = download_preprocessed_from_zenodo(\n        ...     rnaseq_url='https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1',\n        ...     clinical_url='https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1',\n        ...     output_dir=Path('data/interim/preprocessed_KIRC_data')\n        ... )\n    \"\"\"\n    # Ensure output directory exists\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\"=\"*80)\n    logger.info(\"Downloading preprocessed data from Zenodo\")\n    logger.info(\"=\"*80)\n\n    # =========================================================================\n    # Download RNAseq data\n    # =========================================================================\n    rnaseq_path = output_dir / rnaseq_filename\n    logger.info(\"\\n[DOWNLOADING RNASEQ DATA]\")\n    logger.info(f\"  Source URL: {rnaseq_url}\")\n    logger.info(f\"  Destination: {rnaseq_path}\")\n\n    try:\n        response = requests.get(rnaseq_url, timeout=timeout, stream=True)\n        response.raise_for_status()\n\n        total_size = int(response.headers.get('content-length', 0))\n        with open(rnaseq_path, 'wb') as file:\n            downloaded = 0\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    file.write(chunk)\n                    downloaded += len(chunk)\n                    if total_size &gt; 0:\n                        percent = (downloaded / total_size) * 100\n                        print(f\"\\rDownloading RNAseq: {percent:.1f}%\", end='', flush=True)\n\n        print()  # New line after progress\n        logger.info(\"  \u2713 Successfully downloaded RNAseq data\")\n\n        # Load and validate the data\n        rnaseq_data = pd.read_csv(rnaseq_path, index_col=0)\n        logger.info(f\"  Shape: {rnaseq_data.shape[0]:,} genes \u00d7 {rnaseq_data.shape[1]:,} samples\")\n\n    except requests.RequestException as e:\n        logger.error(f\"Failed to download RNAseq data: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to load RNAseq data: {e}\")\n        raise\n\n    # =========================================================================\n    # Download clinical data\n    # =========================================================================\n    clinical_path = output_dir / clinical_filename\n    logger.info(\"\\n[DOWNLOADING CLINICAL DATA]\")\n    logger.info(f\"  Source URL: {clinical_url}\")\n    logger.info(f\"  Destination: {clinical_path}\")\n\n    try:\n        response = requests.get(clinical_url, timeout=timeout, stream=True)\n        response.raise_for_status()\n\n        total_size = int(response.headers.get('content-length', 0))\n        with open(clinical_path, 'wb') as file:\n            downloaded = 0\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    file.write(chunk)\n                    downloaded += len(chunk)\n                    if total_size &gt; 0:\n                        percent = (downloaded / total_size) * 100\n                        print(f\"\\rDownloading clinical: {percent:.1f}%\", end='', flush=True)\n\n        print()  # New line after progress\n        logger.info(\"  \u2713 Successfully downloaded clinical data\")\n\n        # Load and validate the data\n        clinical_data = pd.read_csv(clinical_path, index_col=0)\n        logger.info(f\"  Shape: {clinical_data.shape[0]:,} samples \u00d7 {clinical_data.shape[1]:,} features\")\n\n    except requests.RequestException as e:\n        logger.error(f\"Failed to download clinical data: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to load clinical data: {e}\")\n        raise\n\n    # =========================================================================\n    # Validate data consistency\n    # =========================================================================\n    logger.info(\"\\n[DATA VALIDATION]\")\n\n    # Check for sample overlap\n    rnaseq_samples = set(rnaseq_data.columns)\n    clinical_samples = set(clinical_data.index)\n    common_samples = rnaseq_samples &amp; clinical_samples\n\n    logger.info(f\"  RNAseq samples: {len(rnaseq_samples):,}\")\n    logger.info(f\"  Clinical samples: {len(clinical_samples):,}\")\n    logger.info(f\"  Common samples: {len(common_samples):,}\")\n\n    if len(common_samples) == 0:\n        logger.warning(\"  \u26a0 WARNING: No common samples found between RNAseq and clinical data!\")\n    elif len(common_samples) &lt; len(rnaseq_samples):\n        logger.warning(f\"  \u26a0 WARNING: Only {len(common_samples)}/{len(rnaseq_samples)} RNAseq samples have clinical data\")\n\n    logger.info(\"\\n\" + \"=\"*80)\n    logger.info(\"Preprocessed data download completed successfully\")\n    logger.info(\"=\"*80 + \"\\n\")\n\n    return rnaseq_data, clinical_data\n</code></pre>"},{"location":"api/dataset/#download_data","title":"download_data","text":"<p>Download raw TCGA datasets from UCSC Xena (TCGA data).</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import download_data\nfrom pathlib import Path\n\n# Download to default location\nrnaseq_path, clinical_path, phenotype_path = download_data(\n    destination=Path(\"data/raw\"),\n    remove_gz=True,\n    timeout=300\n)\n\nprint(f\"RNA-seq data: {rnaseq_path}\")\nprint(f\"Clinical data: {clinical_path}\")\nprint(f\"Phenotype data: {phenotype_path}\")\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.download_data","title":"download_data","text":"<pre><code>download_data(\n    destination: Path = \"data/raw\", remove_gz: bool = True, timeout: int = 300\n) -&gt; Tuple[Path, Path, Path]\n</code></pre> <p>Download the KIRC datasets to the specified destination.</p> <p>Args:     destination: Path to directory where dataset should be saved     remove_gz: Whether to remove .gz files after extraction     timeout: Request timeout in seconds</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def download_data(\n        destination: Path = \"data/raw\",\n        remove_gz: bool = True,\n        timeout: int = 300) -&gt; Tuple[Path,Path,Path]:\n    \"\"\"\n    Download the KIRC datasets to the specified destination.\n\n    Args:\n        destination: Path to directory where dataset should be saved\n        remove_gz: Whether to remove .gz files after extraction\n        timeout: Request timeout in seconds\n    \"\"\"\n    # Ensure save directory exists\n    destination = Path(destination)\n    destination.mkdir(parents=True, exist_ok=True)\n\n    datasets = [\n        (\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/\"\n         \"EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena.gz\",\n         \"Gene expression RNAseq - Batch effects normalized mRNA data\"),\n        (\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/\"\n         \"TCGA_phenotype_denseDataOnlyDownload.tsv.gz\",\n         \"TCGA phenotype data\"),\n        (\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/\"\n         \"Survival_SupplementalTable_S1_20171025_xena_sp\",\n         \"Clinical survival data\")\n    ]\n\n    for url, description in datasets:\n        try:\n            filename = url.split('/')[-1]\n            file_path = destination / filename\n            is_gzipped = filename.endswith('.gz')\n\n            logger.info(f\"Downloading dataset: {description}...\")\n            response = requests.get(url, timeout=timeout, stream=True)\n            response.raise_for_status()\n\n            total_size = int(response.headers.get('content-length', 0))\n            with open(file_path, 'wb') as file:\n                downloaded = 0\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        file.write(chunk)\n                        downloaded += len(chunk)\n                        if total_size &gt; 0:\n                            percent = (downloaded / total_size) * 100\n                            print(f\"\\rDownloading {filename}: {percent:.1f}%\", end='', flush=True)\n\n            print(f\"\\nFile downloaded successfully: {file_path}\")\n\n            if is_gzipped:\n                extracted_path = destination / filename.replace('.gz', '')\n                logger.info(\"Extracting compressed file...\")\n                with gzip.open(file_path, 'rb') as f_in, open(extracted_path, 'wb') as f_out:\n                    f_out.write(f_in.read())\n                logger.info(f\"Successfully extracted file to: {extracted_path}\")\n\n                if remove_gz:\n                    file_path.unlink()\n                    logger.info(\"Removed compressed .gz file\")\n\n        except requests.RequestException as e:\n            logger.error(f\"Failed to download {description}: {e}\")\n            raise\n        except IOError as e:\n            logger.error(f\"File I/O error for {description}: {e}\")\n            raise\n\n    # Return paths to downloaded files\n    rnaseq_path = destination / \"EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\"\n    clinical_path = destination / \"Survival_SupplementalTable_S1_20171025_xena_sp\"\n    phenotype_path = destination / \"TCGA_phenotype_denseDataOnlyDownload.tsv\"\n\n    return rnaseq_path, clinical_path, phenotype_path\n</code></pre>"},{"location":"api/dataset/#process_downloaded_data","title":"process_downloaded_data","text":"<p>Process downloaded TCGA data for a specific cancer type.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import process_downloaded_data\nfrom pathlib import Path\n\n# Process data for KIRC (Kidney Renal Clear Cell Carcinoma)\nrnaseq, clinical, phenotype = process_downloaded_data(\n    rnaseq_path=Path(\"data/raw/EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\"),\n    clinical_path=Path(\"data/raw/Survival_SupplementalTable_S1_20171025_xena_sp\"),\n    phenotype_path=Path(\"data/raw/TCGA_phenotype_denseDataOnlyDownload.tsv\"),\n    cancer_type=\"KIRC\",\n    output_dir=Path(\"data/raw\"),\n    early_late=False\n)\n\nprint(f\"RNA-seq shape: {rnaseq.shape}\")\nprint(f\"Clinical shape: {clinical.shape}\")\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.process_downloaded_data","title":"process_downloaded_data","text":"<pre><code>process_downloaded_data(\n    rnaseq_path: Path = \"data/raw/EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\",\n    clinical_path: Path = \"data/raw/Survival_SupplementalTable_S1_20171025_xena_sp\",\n    phenotype_path: Path = \"data/raw/TCGA_phenotype_denseDataOnlyDownload.tsv\",\n    cancer_type: str = \"KIRC\",\n    output_dir: Path = \"data/raw\",\n    extract_controls: bool = True,\n    early_late: bool = False,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Process TCGA Pan-Cancer Atlas data for a specific cancer type.</p> <p>This function performs comprehensive data processing and quality control on TCGA datasets, including cancer type filtering, sample type selection, stage harmonization, and optional binary classification mapping.</p> <p>Processing Steps:     1. Load raw RNA-seq, clinical, and phenotype data from TCGA Xena Hub     2. Filter samples by cancer type (KIRC or BRCA)     3. Select primary tumor samples only (exclude metastatic/recurrent)        and controls (solid tissue normal samples)     4. Remove ambiguous stage annotations (Stage X, discrepancies, missing)     5. Harmonize substages (e.g., Stage IA/IB/IC \u2192 Stage I)     6. Optionally map stages to binary early/late classification     7. Ensure consistency across all three datasets     8. Save processed data in CSV format</p> <p>Args:     rnaseq_path: Path to RNA-seq expression matrix (genes \u00d7 samples).         Expected format: tab-delimited file with gene IDs as rows and         sample IDs (TCGA barcodes) as columns.     clinical_path: Path to clinical survival data.         Expected format: tab-delimited file with survival information,         stage annotations, and patient metadata.     phenotype_path: Path to phenotype annotations.         Expected format: tab-delimited file with sample type information         (Primary Tumor, Metastatic, etc.).     cancer_type: Cancer type abbreviation. Supported values:         - \"KIRC\": Kidney Renal Clear Cell Carcinoma         - \"BRCA\": Breast Invasive Carcinoma (filters to female patients only)     output_dir: Directory where processed CSV files will be saved.     extract_controls: If True, extract and save control samples, named as         Solid Tissue Normal, into a separate \"controls\" subdirectory within         output_dir.     early_late: If True, map AJCC stages to binary classification:         - \"early\": Stage I and Stage II         - \"late\": Stage III and Stage IV         If False, retain original stage granularity (I, II, III, IV).</p> <p>Returns:     Tuple of Paths to processed files:         - rnaseq_path: Processed RNA-seq expression matrix         - clinical_path: Processed clinical annotations         - phenotype_path: Processed phenotype data         - control_path: Processed control sample data (if applicable)</p> <p>Raises:     FileNotFoundError: If input files do not exist     KeyError: If required columns are missing from input data     ValueError: If cancer_type is not supported</p> <p>Notes:     - For BRCA, only female patients with ductal or lobular carcinomas are retained     - AJCC pathologic tumor stage is used as the primary staging system     - Substages (A, B, C) are collapsed to main stages for statistical power     - All three output datasets maintain consistent sample identifiers</p> <p>Examples:     &gt;&gt;&gt; # Process KIRC data with 4-stage classification     &gt;&gt;&gt; paths = process_downloaded_data(     ...     rnaseq_path=\"data/raw/expression.xena\",     ...     clinical_path=\"data/raw/survival.tsv\",     ...     phenotype_path=\"data/raw/phenotype.tsv\",     ...     cancer_type=\"KIRC\",     ...     output_dir=\"data/processed\",     ...     early_late=False     ... )</p> <pre><code>&gt;&gt;&gt; # Process KIRC data with binary early/late classification\n&gt;&gt;&gt; paths = process_downloaded_data(\n...     cancer_type=\"KIRC\",\n...     output_dir=\"data/processed\",\n...     early_late=True\n... )\n</code></pre> Source code in <code>renalprog/dataset.py</code> <pre><code>def process_downloaded_data(\n        rnaseq_path: Path = \"data/raw/EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\",\n        clinical_path: Path = \"data/raw/Survival_SupplementalTable_S1_20171025_xena_sp\",\n        phenotype_path: Path = \"data/raw/TCGA_phenotype_denseDataOnlyDownload.tsv\",\n        cancer_type: str = \"KIRC\",\n        output_dir: Path = \"data/raw\",\n        extract_controls: bool = True,\n        early_late: bool = False) -&gt; Tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n    \"\"\"\n    Process TCGA Pan-Cancer Atlas data for a specific cancer type.\n\n    This function performs comprehensive data processing and quality control on TCGA\n    datasets, including cancer type filtering, sample type selection, stage harmonization,\n    and optional binary classification mapping.\n\n    Processing Steps:\n        1. Load raw RNA-seq, clinical, and phenotype data from TCGA Xena Hub\n        2. Filter samples by cancer type (KIRC or BRCA)\n        3. Select primary tumor samples only (exclude metastatic/recurrent)\n           and controls (solid tissue normal samples)\n        4. Remove ambiguous stage annotations (Stage X, discrepancies, missing)\n        5. Harmonize substages (e.g., Stage IA/IB/IC \u2192 Stage I)\n        6. Optionally map stages to binary early/late classification\n        7. Ensure consistency across all three datasets\n        8. Save processed data in CSV format\n\n    Args:\n        rnaseq_path: Path to RNA-seq expression matrix (genes \u00d7 samples).\n            Expected format: tab-delimited file with gene IDs as rows and\n            sample IDs (TCGA barcodes) as columns.\n        clinical_path: Path to clinical survival data.\n            Expected format: tab-delimited file with survival information,\n            stage annotations, and patient metadata.\n        phenotype_path: Path to phenotype annotations.\n            Expected format: tab-delimited file with sample type information\n            (Primary Tumor, Metastatic, etc.).\n        cancer_type: Cancer type abbreviation. Supported values:\n            - \"KIRC\": Kidney Renal Clear Cell Carcinoma\n            - \"BRCA\": Breast Invasive Carcinoma (filters to female patients only)\n        output_dir: Directory where processed CSV files will be saved.\n        extract_controls: If True, extract and save control samples, named as\n            Solid Tissue Normal, into a separate \"controls\" subdirectory within\n            output_dir.\n        early_late: If True, map AJCC stages to binary classification:\n            - \"early\": Stage I and Stage II\n            - \"late\": Stage III and Stage IV\n            If False, retain original stage granularity (I, II, III, IV).\n\n    Returns:\n        Tuple of Paths to processed files:\n            - rnaseq_path: Processed RNA-seq expression matrix\n            - clinical_path: Processed clinical annotations\n            - phenotype_path: Processed phenotype data\n            - control_path: Processed control sample data (if applicable)\n\n    Raises:\n        FileNotFoundError: If input files do not exist\n        KeyError: If required columns are missing from input data\n        ValueError: If cancer_type is not supported\n\n    Notes:\n        - For BRCA, only female patients with ductal or lobular carcinomas are retained\n        - AJCC pathologic tumor stage is used as the primary staging system\n        - Substages (A, B, C) are collapsed to main stages for statistical power\n        - All three output datasets maintain consistent sample identifiers\n\n    Examples:\n        &gt;&gt;&gt; # Process KIRC data with 4-stage classification\n        &gt;&gt;&gt; paths = process_downloaded_data(\n        ...     rnaseq_path=\"data/raw/expression.xena\",\n        ...     clinical_path=\"data/raw/survival.tsv\",\n        ...     phenotype_path=\"data/raw/phenotype.tsv\",\n        ...     cancer_type=\"KIRC\",\n        ...     output_dir=\"data/processed\",\n        ...     early_late=False\n        ... )\n\n        &gt;&gt;&gt; # Process KIRC data with binary early/late classification\n        &gt;&gt;&gt; paths = process_downloaded_data(\n        ...     cancer_type=\"KIRC\",\n        ...     output_dir=\"data/processed\",\n        ...     early_late=True\n        ... )\n    \"\"\"\n    # =========================================================================\n    # STEP 1: Load raw data from TCGA sources\n    # =========================================================================\n    logger.info(\"=\"*80)\n    logger.info(f\"Starting data processing pipeline for {cancer_type}\")\n    logger.info(\"=\"*80)\n\n    rnaseq = pd.read_table(rnaseq_path, index_col=0)\n    clinical = pd.read_csv(clinical_path, sep=\"\\t\", index_col=0)\n    pheno = pd.read_table(phenotype_path, sep=\"\\t\", index_col=0)\n\n    # Remove redundant _PATIENT column from clinical data\n    clinical.drop(\"_PATIENT\", axis=1, inplace=True, errors='ignore')\n\n    logger.info(\"\\n[INITIAL DATA SHAPES]\")\n    logger.info(f\"  RNA-seq:   {rnaseq.shape[0]:&gt;6,} genes \u00d7 {rnaseq.shape[1]:&gt;5,} samples\")\n    logger.info(f\"  Clinical:  {clinical.shape[0]:&gt;6,} patients \u00d7 {clinical.shape[1]:&gt;3,} features\")\n    logger.info(f\"  Phenotype: {pheno.shape[0]:&gt;6,} samples \u00d7 {pheno.shape[1]:&gt;3,} features\")\n\n    # =========================================================================\n    # STEP 2: Filter by cancer type\n    # =========================================================================\n    logger.info(f\"\\n[FILTERING BY CANCER TYPE: {cancer_type}]\")\n\n    if cancer_type == 'BRCA':\n        # For breast cancer, only include female patients\n        clinical = clinical[\n            (clinical[\"gender\"] == \"FEMALE\") &amp;\n            (clinical[\"cancer type abbreviation\"] == cancer_type)\n        ]\n        logger.info(f\"  Filter: Female patients with {cancer_type}\")\n    elif cancer_type == 'KIRC':\n        clinical = clinical[clinical[\"cancer type abbreviation\"] == cancer_type]\n        logger.info(f\"  Filter: Patients with {cancer_type}\")\n    else:\n        logger.warning(f\"  Cancer type '{cancer_type}' may not be fully supported\")\n        clinical = clinical[clinical[\"cancer type abbreviation\"] == cancer_type]\n\n    # Synchronize datasets to common samples\n    pheno = pheno[pheno.index.isin(clinical.index)]\n    rnaseq = rnaseq.loc[:, rnaseq.columns.isin(clinical.index)]\n    pheno = pheno[pheno.index.isin(rnaseq.columns)]\n    clinical = clinical[clinical.index.isin(rnaseq.columns)]\n\n    logger.info(\"\\n  Post-filter shapes:\")\n    logger.info(f\"    RNA-seq:   {rnaseq.shape[0]:&gt;6,} genes \u00d7 {rnaseq.shape[1]:&gt;5,} samples\")\n    logger.info(f\"    Clinical:  {clinical.shape[0]:&gt;6,} patients\")\n    logger.info(f\"    Phenotype: {pheno.shape[0]:&gt;6,} samples\")\n\n    # =========================================================================\n    # STEP 3: Select primary tumor samples only\n    # =========================================================================\n    logger.info(\"\\n[FILTERING BY SAMPLE TYPE: Primary Tumor]\")\n\n    # Log sample type distribution before filtering\n    sample_type_counts = pheno[\"sample_type\"].value_counts()\n    logger.info(\"  Sample type distribution:\")\n    for sample_type, count in sample_type_counts.items():\n        logger.info(f\"    {sample_type:&lt;30} {count:&gt;5,} samples\")\n\n    # Extract controls samples if specified\n    if extract_controls:\n        logger.info(\"\\n[EXTRACT CONTROLS]\")\n        pheno_controls = pheno[pheno[\"sample_type\"] == \"Solid Tissue Normal\"]\n        clinical_controls = clinical[clinical.index.isin(pheno.index)]\n        rnaseq_controls = rnaseq.loc[:, rnaseq.columns.isin(pheno_controls.index)]\n        logger.info(f\"  Found {pheno_controls.shape[0]:&gt;6,} control samples\")\n        # Save control data\n        control_output_dir = Path(output_dir) / \"controls\"\n        control_output_dir.mkdir(parents=True, exist_ok=True)\n\n        rnaseq_controls.to_csv(control_output_dir / f\"{cancer_type}_control_rnaseq.csv\")\n        clinical_controls.to_csv(control_output_dir / f\"{cancer_type}_control_clinical.csv\")\n        pheno_controls.to_csv(control_output_dir / f\"{cancer_type}_control_phenotype.csv\")\n\n        logger.info(f\"  Saved control data to {control_output_dir}\")\n\n    pheno = pheno[pheno[\"sample_type\"] == \"Primary Tumor\"]\n    clinical = clinical[clinical.index.isin(pheno.index)]\n    rnaseq = rnaseq.loc[:, rnaseq.columns.isin(pheno.index)]\n\n    logger.info(f\"\\n  Retained {len(pheno):,} primary tumor samples\")\n    logger.info(f\"    RNA-seq:   {rnaseq.shape[0]:&gt;6,} genes \u00d7 {rnaseq.shape[1]:&gt;5,} samples\")\n    logger.info(f\"    Clinical:  {clinical.shape[0]:&gt;6,} patients\")\n    logger.info(f\"    Phenotype: {pheno.shape[0]:&gt;6,} samples\")\n\n    # =========================================================================\n    # STEP 4: Remove ambiguous stage annotations\n    # =========================================================================\n    logger.info(\"\\n[FILTERING BY STAGE QUALITY]\")\n\n    stages_remove = ['Stage X', '[Discrepancy]', np.nan]\n    initial_stage_counts = clinical[\"ajcc_pathologic_tumor_stage\"].value_counts(dropna=False)\n\n    logger.info(\"  Initial stage distribution:\")\n    _log_stage_table(initial_stage_counts)\n\n    # Filter out problematic stage annotations\n    clinical_redux = clinical[~clinical[\"ajcc_pathologic_tumor_stage\"].isin(stages_remove)]\n    removed_count = len(clinical) - len(clinical_redux)\n    logger.info(f\"\\n  Removed {removed_count} samples with ambiguous staging\")\n\n    # =========================================================================\n    # STEP 5: Harmonize substages (collapse A/B/C to main stage)\n    # =========================================================================\n    logger.info(\"\\n[HARMONIZING SUBSTAGES]\")\n\n    stages_available = clinical_redux[\"ajcc_pathologic_tumor_stage\"].unique()\n    has_substages = any(\n        str(stage).endswith((\"A\", \"B\", \"C\"))\n        for stage in stages_available\n        if pd.notna(stage)\n    )\n\n    if has_substages:\n        logger.info(\"  Substages detected (e.g., Stage IA, Stage IIB)\")\n        logger.info(\"  Collapsing substages to main stages for statistical power\")\n\n        # Collapse substages by removing trailing A/B/C letters\n        stages_clump = [\n            str(stage)[:-1] if str(stage).endswith((\"A\", \"B\", \"C\")) else stage\n            for stage in clinical_redux[\"ajcc_pathologic_tumor_stage\"]\n        ]\n        clinical_redux[\"ajcc_pathologic_tumor_stage\"] = stages_clump\n\n        post_clump_counts = clinical_redux[\"ajcc_pathologic_tumor_stage\"].value_counts().sort_index()\n        logger.info(\"\\n  Stage distribution after harmonization:\")\n        _log_stage_table(post_clump_counts)\n    else:\n        logger.info(\"  No substages detected; stage labels are already harmonized\")\n\n    # Synchronize datasets after stage filtering\n    pheno_redux = pheno[pheno.index.isin(clinical_redux.index)]\n    rnaseq_redux = rnaseq.loc[:, rnaseq.columns.isin(clinical_redux.index)]\n\n    # =========================================================================\n    # STEP 6: Additional cancer-specific filtering (BRCA only)\n    # =========================================================================\n    if cancer_type == 'BRCA':\n        logger.info(\"\\n[BRCA-SPECIFIC FILTERING: Ductal and Lobular Carcinomas]\")\n        ductal_lobular = ['Infiltrating Ductal Carcinoma', 'Infiltrating Lobular Carcinoma']\n        patients_ductal_lobular = clinical_redux.index[\n            clinical_redux[\"histological_type\"].isin(ductal_lobular)\n        ]\n\n        clinical_redux = clinical_redux[clinical_redux.index.isin(patients_ductal_lobular)]\n        pheno_redux = pheno_redux[pheno_redux.index.isin(patients_ductal_lobular)]\n        rnaseq_redux = rnaseq_redux.loc[:, rnaseq_redux.columns.isin(patients_ductal_lobular)]\n\n        logger.info(f\"  Retained {len(patients_ductal_lobular):,} ductal/lobular samples\")\n\n    # =========================================================================\n    # STEP 7: Optional binary classification mapping (early vs. late)\n    # =========================================================================\n    if early_late:\n        logger.info(\"\\n[MAPPING TO BINARY CLASSIFICATION: Early vs. Late]\")\n        logger.info(\"  Mapping scheme:\")\n        logger.info(\"    Early: Stage I, Stage II\")\n        logger.info(\"    Late:  Stage III, Stage IV\")\n\n        mapped_stages = map_stages_to_early_late(clinical_redux[\"ajcc_pathologic_tumor_stage\"])\n        valid_mask = mapped_stages.notna()\n\n        clinical_redux = clinical_redux.loc[valid_mask, :]\n        clinical_redux[\"ajcc_pathologic_tumor_stage\"] = mapped_stages[valid_mask]\n        pheno_redux = pheno_redux[pheno_redux.index.isin(clinical_redux.index)]\n        rnaseq_redux = rnaseq_redux.loc[:, rnaseq_redux.columns.isin(clinical_redux.index)]\n\n        final_stage_counts = clinical_redux[\"ajcc_pathologic_tumor_stage\"].value_counts().sort_index()\n        logger.info(\"\\n  Final binary classification distribution:\")\n        _log_stage_table(final_stage_counts)\n    else:\n        final_stage_counts = clinical_redux[\"ajcc_pathologic_tumor_stage\"].value_counts().sort_index()\n        logger.info(\"\\n  Final stage distribution (4-class):\")\n        _log_stage_table(final_stage_counts)\n\n    # =========================================================================\n    # STEP 8: Final statistics and data saving\n    # =========================================================================\n    logger.info(\"\\n[FINAL PROCESSED DATA SHAPES]\")\n    logger.info(f\"  RNA-seq:   {rnaseq_redux.shape[0]:&gt;6,} genes \u00d7 {rnaseq_redux.shape[1]:&gt;5,} samples\")\n    logger.info(f\"  Clinical:  {clinical_redux.shape[0]:&gt;6,} patients \u00d7 {clinical_redux.shape[1]:&gt;3,} features\")\n    logger.info(f\"  Phenotype: {pheno_redux.shape[0]:&gt;6,} samples \u00d7 {pheno_redux.shape[1]:&gt;3,} features\")\n\n    # Calculate and log filtering statistics\n    initial_samples = rnaseq.shape[1]\n    final_samples = rnaseq_redux.shape[1]\n    retention_rate = (final_samples / initial_samples) * 100 if initial_samples &gt; 0 else 0\n\n    logger.info(\"\\n[FILTERING SUMMARY]\")\n    logger.info(f\"  Initial samples:  {initial_samples:&gt;5,}\")\n    logger.info(f\"  Final samples:    {final_samples:&gt;5,}\")\n    logger.info(f\"  Retention rate:   {retention_rate:&gt;5.1f}%\")\n    logger.info(f\"  Samples removed:  {initial_samples - final_samples:&gt;5,}\")\n    if extract_controls:\n        logger.info(f\"  Control samples found:  {rnaseq_controls.shape[1]:&gt;5,}\")\n    # Save processed data to disk\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    path_rnaseq = output_dir / f\"{cancer_type}_rnaseq.csv\"\n    path_clinical = output_dir / f\"{cancer_type}_clinical.csv\"\n    path_phenotype = output_dir / f\"{cancer_type}_phenotype.csv\"\n\n    logger.info(\"\\n[SAVING PROCESSED DATA]\")\n    logger.info(f\"  Output directory: {output_dir}\")\n\n    rnaseq_redux.to_csv(path_rnaseq)\n    logger.info(f\"  \u2713 Saved RNA-seq:   {path_rnaseq.name}\")\n\n    clinical_redux.to_csv(path_clinical)\n    logger.info(f\"  \u2713 Saved clinical:  {path_clinical.name}\")\n\n    pheno_redux.to_csv(path_phenotype)\n    logger.info(f\"  \u2713 Saved phenotype: {path_phenotype.name}\")\n\n    logger.info(\"\\n\" + \"=\"*80)\n    logger.info(f\"Data processing completed successfully for {cancer_type}\")\n    logger.info(\"=\"*80 + \"\\n\")\n\n    return rnaseq_redux, clinical_redux, pheno_redux\n</code></pre>"},{"location":"api/dataset/#load_rnaseq_data","title":"load_rnaseq_data","text":"<p>Load RNA-seq expression data from a file.</p>"},{"location":"api/dataset/#renalprog.dataset.load_rnaseq_data","title":"load_rnaseq_data","text":"<pre><code>load_rnaseq_data(path: Path) -&gt; pd.DataFrame\n</code></pre> <p>Load RNA-seq data from CSV file.</p> <p>Args:     path: Path to RNA-seq CSV file (genes as rows, samples as columns)</p> <p>Returns:     DataFrame with RNA-seq data</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def load_rnaseq_data(path: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Load RNA-seq data from CSV file.\n\n    Args:\n        path: Path to RNA-seq CSV file (genes as rows, samples as columns)\n\n    Returns:\n        DataFrame with RNA-seq data\n    \"\"\"\n    logger.info(f\"Loading RNA-seq data from {path}\")\n    data = pd.read_csv(path, index_col=0)\n    logger.info(f\"Loaded RNA-seq data with shape: {data.shape}\")\n    return data\n</code></pre>"},{"location":"api/dataset/#load_clinical_data","title":"load_clinical_data","text":"<p>Load clinical and survival data from a file.</p>"},{"location":"api/dataset/#renalprog.dataset.load_clinical_data","title":"load_clinical_data","text":"<pre><code>load_clinical_data(\n    path: Path,\n    stage_column: str = \"ajcc_pathologic_tumor_stage\",\n    early_late=True,\n) -&gt; pd.Series\n</code></pre> <p>Load clinical metadata.</p> <p>Args:     path: Path to clinical data CSV file     stage_column: Name of column containing stage information</p> <p>Returns:     Series with clinical stages indexed by sample ID</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def load_clinical_data(path: Path, stage_column: str = \"ajcc_pathologic_tumor_stage\", early_late = True) -&gt; pd.Series:\n    \"\"\"\n    Load clinical metadata.\n\n    Args:\n        path: Path to clinical data CSV file\n        stage_column: Name of column containing stage information\n\n    Returns:\n        Series with clinical stages indexed by sample ID\n    \"\"\"\n    logger.info(f\"Loading clinical data from {path}\")\n    data = pd.read_csv(path, index_col=0)\n\n    if stage_column not in data.columns:\n        raise ValueError(f\"Stage column '{stage_column}' not found in clinical data\")\n\n    stages = data[stage_column]\n    logger.info(f\"Loaded clinical data with {len(stages)} samples\")\n\n    if early_late:\n        # Check if the stages are already in early/late format\n        unique_stages = stages.dropna().unique()\n        if set(unique_stages).issubset({\"early\", \"late\"}):\n            logger.info(\"Stages are already in early/late format; no mapping needed\")\n        else:\n            stages = map_stages_to_early_late(stages)\n            logger.info(\"Mapped stages to binary early/late classification\")\n    logger.info(f\"Stage distribution:\\n{stages.value_counts()}\")\n\n    return stages\n</code></pre>"},{"location":"api/dataset/#create_train_test_split","title":"create_train_test_split","text":"<p>Create stratified train/test splits of the data.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import load_rnaseq_data, load_clinical_data, create_train_test_split\nfrom pathlib import Path\n\n# Load the data\nrnaseq = load_rnaseq_data(Path(\"data/raw/KIRC_rnaseq.tsv\"))\nclinical = load_clinical_data(Path(\"data/raw/KIRC_clinical.tsv\"))\n\n# Create train/test split\ncreate_train_test_split(\n    rnaseq=rnaseq,\n    clinical=clinical,\n    test_size=0.2,\n    random_state=42,\n    output_dir=Path(\"data/interim/my_experiment\")\n)\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.create_train_test_split","title":"create_train_test_split","text":"<pre><code>create_train_test_split(\n    rnaseq_path: Path,\n    clinical_path: Path,\n    stage_column: str = \"ajcc_pathologic_tumor_stage\",\n    test_size: float = 0.2,\n    seed: int = 2023,\n    use_onehot: bool = True,\n    output_dir: Optional[Path] = None,\n) -&gt; Tuple[\n    pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, pd.DataFrame, pd.Series\n]\n</code></pre> <p>Create stratified train/test split of KIRC data.</p> <p>Args:     rnaseq_path: Path to RNA-seq CSV file     clinical_path: Path to clinical CSV file     stage_column: Name of column containing stage information (default: \"ajcc_pathologic_tumor_stage\")     test_size: Fraction of data to use for testing (default: 0.2)     seed: Random seed for reproducibility (default: 2023)     use_onehot: Whether to one-hot encode the labels (default: True)     output_dir: Optional directory to save split data</p> <p>Returns:     Tuple of (X_train, X_test, y_train, y_test, full_rnaseq, full_clinical)</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def create_train_test_split(\n    rnaseq_path: Path,\n    clinical_path: Path,\n    stage_column: str = \"ajcc_pathologic_tumor_stage\",\n    test_size: float = 0.2,\n    seed: int = 2023,\n    use_onehot: bool = True,\n    output_dir: Optional[Path] = None\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create stratified train/test split of KIRC data.\n\n    Args:\n        rnaseq_path: Path to RNA-seq CSV file\n        clinical_path: Path to clinical CSV file\n        stage_column: Name of column containing stage information (default: \"ajcc_pathologic_tumor_stage\")\n        test_size: Fraction of data to use for testing (default: 0.2)\n        seed: Random seed for reproducibility (default: 2023)\n        use_onehot: Whether to one-hot encode the labels (default: True)\n        output_dir: Optional directory to save split data\n\n    Returns:\n        Tuple of (X_train, X_test, y_train, y_test, full_rnaseq, full_clinical)\n    \"\"\"\n    set_seed(seed)\n\n    # Load data\n    rnaseq = load_rnaseq_data(rnaseq_path)\n    clinical = load_clinical_data(clinical_path,stage_column)\n\n    # Ensure samples match between RNA-seq and clinical data\n    common_samples = rnaseq.columns.intersection(clinical.index)\n    if len(common_samples) &lt; len(rnaseq.columns):\n        logger.warning(\n            f\"Only {len(common_samples)} of {len(rnaseq.columns)} samples \"\n            f\"have clinical data. Filtering to common samples.\"\n        )\n\n    rnaseq = rnaseq[common_samples]\n    clinical = clinical[common_samples]\n\n    # Transpose to have samples as rows\n    rnaseq_t = rnaseq.T\n\n    # Prepare labels for stratification\n    if use_onehot:\n        # Get unique stages in sorted order for consistent encoding\n        categories = sorted(clinical.unique())\n        ohe = OneHotEncoder(\n            categories=[categories],\n            handle_unknown='ignore',\n            sparse_output=False,\n            dtype=np.int8\n        )\n        y = ohe.fit_transform(clinical.values.reshape(-1, 1))\n        logger.info(f\"One-hot encoded labels with {y.shape[1]} classes\")\n    else:\n        y = clinical.values\n\n    # Perform stratified split\n    X_train, X_test, y_train, y_test = train_test_split(\n        rnaseq_t,\n        y,\n        test_size=test_size,\n        stratify=y if use_onehot else clinical,\n        random_state=seed\n    )\n\n    logger.info(f\"Train set: {X_train.shape[0]} samples\")\n    logger.info(f\"Test set: {X_test.shape[0]} samples\")\n\n    # Save split data\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save split data\n        X_train.to_csv(output_dir / \"X_train.csv\")\n        X_test.to_csv(output_dir / \"X_test.csv\")\n\n        # Save labels WITH patient IDs as indices (matching X_train and X_test)\n        if use_onehot:\n            # Create DataFrame with patient IDs as index\n            y_train_df = pd.DataFrame(y_train, index=X_train.index)\n            y_test_df = pd.DataFrame(y_test, index=X_test.index)\n            y_train_df.to_csv(output_dir / \"y_train.csv\")\n            y_test_df.to_csv(output_dir / \"y_test.csv\")\n        else:\n            # Create Series with patient IDs as index\n            y_train_series = pd.Series(y_train, index=X_train.index)\n            y_test_series = pd.Series(y_test, index=X_test.index)\n            y_train_series.to_csv(output_dir / \"y_train.csv\")\n            y_test_series.to_csv(output_dir / \"y_test.csv\")\n\n        # Save full data for reference\n        rnaseq.to_csv(output_dir / \"data.csv\")\n        clinical.to_csv(output_dir / \"metadata.csv\")\n\n        # Save split statistics\n        _save_split_statistics(clinical, y_train, y_test, output_dir, use_onehot, categories if use_onehot else None)\n\n        logger.info(f\"Saved train/test split to {output_dir}\")\n\n    return X_train, X_test, y_train, y_test, rnaseq, clinical\n</code></pre>"},{"location":"api/dataset/#load_train_test_split","title":"load_train_test_split","text":"<p>Load previously saved train/test split data.</p> <p>Example Usage:</p> <pre><code>from renalprog.dataset import load_train_test_split\nfrom pathlib import Path\n\n# Load existing split\ntrain_expr, test_expr, train_clin, test_clin = load_train_test_split(\n    Path(\"data/interim/my_experiment\")\n)\n\nprint(f\"Training samples: {len(train_expr)}\")\nprint(f\"Test samples: {len(test_expr)}\")\n</code></pre>"},{"location":"api/dataset/#renalprog.dataset.load_train_test_split","title":"load_train_test_split","text":"<pre><code>load_train_test_split(\n    split_dir: Path,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Load previously saved train/test split.</p> <p>Args:     split_dir: Directory containing saved split files</p> <p>Returns:     Tuple of (X_train, X_test, y_train, y_test)</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def load_train_test_split(split_dir: Path) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Load previously saved train/test split.\n\n    Args:\n        split_dir: Directory containing saved split files\n\n    Returns:\n        Tuple of (X_train, X_test, y_train, y_test)\n    \"\"\"\n    split_dir = Path(split_dir)\n\n    X_train = pd.read_csv(split_dir / \"X_train.csv\", index_col=0)\n    X_test = pd.read_csv(split_dir / \"X_test.csv\", index_col=0)\n    y_train = pd.read_csv(split_dir / \"y_train.csv\", index_col=0)\n    y_test = pd.read_csv(split_dir / \"y_test.csv\", index_col=0)\n\n    logger.info(f\"Loaded train/test split from {split_dir}\")\n    logger.info(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"api/dataset/#map_stages_to_early_late","title":"map_stages_to_early_late","text":"<p>Map cancer stages to binary early/late categories.</p>"},{"location":"api/dataset/#renalprog.dataset.map_stages_to_early_late","title":"map_stages_to_early_late","text":"<pre><code>map_stages_to_early_late(stages: Series) -&gt; pd.Series\n</code></pre> <p>Map detailed stages (I, II, III, IV) to binary early/late classification.</p> <p>Args:     stages: Series with stage labels (e.g., \"Stage I\", \"Stage II\", etc.)</p> <p>Returns:     Series with mapped stages (\"early\" or \"late\")</p> Source code in <code>renalprog/dataset.py</code> <pre><code>def map_stages_to_early_late(stages: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Map detailed stages (I, II, III, IV) to binary early/late classification.\n\n    Args:\n        stages: Series with stage labels (e.g., \"Stage I\", \"Stage II\", etc.)\n\n    Returns:\n        Series with mapped stages (\"early\" or \"late\")\n    \"\"\"\n    stage_mapping = PreprocessingConfig.STAGE_MAPPING\n    mapped_stages = stages.map(stage_mapping)\n\n    # Check for unmapped values\n    unmapped = mapped_stages.isna() &amp; stages.notna()\n    if unmapped.any():\n        logger.warning(f\"Found {unmapped.sum()} unmapped stage values:\")\n        logger.warning(stages[unmapped].unique())\n\n    return mapped_stages\n</code></pre>"},{"location":"api/dataset/#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"api/dataset/#rna-seq-expression-data","title":"RNA-seq Expression Data","text":"<p>Expected format: - Rows: Genes (with gene symbols or Ensembl IDs) - Columns: Samples (patient IDs) - Values: Log2-transformed TPM or FPKM expression values</p> <pre><code># Example structure\n#              TCGA-A1-A0SB  TCGA-A1-A0SD  TCGA-A1-A0SE  ...\n# GENE_A       5.234         4.891         6.123         ...\n# GENE_B       2.456         2.789         2.634         ...\n# GENE_C       8.912         9.234         8.756         ...\n</code></pre>"},{"location":"api/dataset/#clinical-data","title":"Clinical Data","text":"<p>Expected columns: - <code>sample</code>: Patient ID matching expression columns - <code>OS</code>: Overall survival status (0=alive, 1=deceased) - <code>OS.time</code>: Overall survival time (days)</p> <p>Optional columns: - <code>age_at_initial_pathologic_diagnosis</code>: Age at diagnosis - <code>gender</code>: Patient gender - <code>tumor_stage</code>: Tumor stage</p> <pre><code># Example structure\n#    sample          OS  OS.time  age  gender  stage\n# 0  TCGA-A1-A0SB    1   1825     65   MALE    IV\n# 1  TCGA-A1-A0SD    0   2190     58   FEMALE  II\n</code></pre>"},{"location":"api/dataset/#traintest-splitting","title":"Train/Test Splitting","text":"<p>The module provides stratified splitting to maintain class balance:</p> <pre><code>from renalprog.dataset import (\n    load_rnaseq_data, \n    load_clinical_data, \n    create_train_test_split\n)\nfrom pathlib import Path\n\n# Load the data\nrnaseq = load_rnaseq_data(Path(\"data/raw/KIRC_rnaseq.tsv\"))\nclinical = load_clinical_data(Path(\"data/raw/KIRC_clinical.tsv\"))\n\n# Create stratified split preserving early/late survival distribution\ncreate_train_test_split(\n    rnaseq=rnaseq,\n    clinical=clinical,\n    test_size=0.2,  # 20% test set\n    random_state=42,  # For reproducibility\n    output_dir=Path(\"data/interim/my_split\")\n)\n\n# Load the split data\ntrain_expr, test_expr, train_clin, test_clin = load_train_test_split(\n    Path(\"data/interim/my_split\")\n)\n\n# Check class distribution\nimport pandas as pd\ntrain_dist = train_clin.value_counts(normalize=True)\ntest_dist = test_clin.value_counts(normalize=True)\n\nprint(\"Training set distribution:\")\nprint(train_dist)\nprint(\"\\nTest set distribution:\")\nprint(test_dist)\n</code></pre>"},{"location":"api/dataset/#data-preprocessing-pipeline","title":"Data Preprocessing Pipeline","text":""},{"location":"api/dataset/#option-1-quick-start-with-zenodo","title":"Option 1: Quick Start with Zenodo","text":"<p>Download preprocessed data directly from Zenodo - no preprocessing needed!</p> <pre><code>from pathlib import Path\nfrom renalprog.dataset import download_preprocessed_from_zenodo\n\n# Download and you're ready to go!\nrnaseq, clinical = download_preprocessed_from_zenodo(\n    rnaseq_url='https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1',\n    clinical_url='https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1',\n    output_dir=Path('data/interim/preprocessed_KIRC_data')\n)\n\n# Data is already preprocessed and ready for modeling\nprint(f\"Ready to use: {rnaseq.shape[0]:,} genes \u00d7 {rnaseq.shape[1]:,} samples\")\n</code></pre>"},{"location":"api/dataset/#option-2-custom-preprocessing-pipeline-advanced","title":"Option 2: Custom Preprocessing Pipeline (Advanced)","text":"<p>The standard preprocessing pipeline for raw TCGA data:</p> <ol> <li>Download raw data from TCGA</li> <li>Filter by cancer type (e.g., KIRC)</li> <li>Filter low expression genes (see Features API)</li> <li>Remove outliers using Mahalanobis distance</li> <li>Create train/test split with stratification</li> <li>Normalize using MinMaxScaler (0-1 range)</li> <li>Save preprocessed data for modeling</li> </ol> <p>Complete Example:</p> <pre><code>from pathlib import Path\nfrom renalprog.dataset import (\n    download_data, \n    process_downloaded_data,\n    load_rnaseq_data,\n    load_clinical_data,\n    create_train_test_split,\n    load_train_test_split\n)\nfrom renalprog.features import filter_low_expression, detect_outliers_mahalanobis\n\n# Step 1: Download\nrnaseq_path, clinical_path, phenotype_path = download_data(\n    destination=Path(\"data/raw\")\n)\n\n# Step 2: Process for KIRC\nrnaseq, clinical, _ = process_downloaded_data(\n    rnaseq_path=rnaseq_path,\n    clinical_path=clinical_path,\n    phenotype_path=phenotype_path,\n    cancer_type=\"KIRC\",\n    output_dir=Path(\"data/raw\")\n)\n\n# Step 3: Filter low expression\nrnaseq_filtered = filter_low_expression(\n    rnaseq,\n    mean_threshold=0.5,\n    var_threshold=0.5\n)\n\n# Step 4: Remove outliers\nrnaseq_clean, outliers, _ = detect_outliers_mahalanobis(\n    rnaseq_filtered,\n    alpha=0.05\n)\n\n# Step 5: Create train/test split\nrnaseq_clean_path = Path(\"data/interim/rnaseq_clean.csv\")\nclinical_path = Path(\"data/raw/KIRC_clinical.tsv\")\nrnaseq_clean.to_csv(rnaseq_clean_path)\n\n# Load and split\nrnaseq_final = load_rnaseq_data(rnaseq_clean_path)\nclinical_final = load_clinical_data(clinical_path)\n\ncreate_train_test_split(\n    rnaseq=rnaseq_final,\n    clinical=clinical_final,\n    test_size=0.2,\n    random_state=42,\n    output_dir=Path(\"data/interim/20251218_experiment\")\n)\n</code></pre>"},{"location":"api/dataset/#see-also","title":"See Also","text":"<ul> <li>Features API - Gene filtering and outlier detection</li> <li>Preprocessing Tutorial - Step-by-step data preparation</li> <li>Configuration API - Data paths and preprocessing parameters</li> </ul>"},{"location":"api/enrichment/","title":"Enrichment Analysis","text":"<p>Pathway enrichment analysis using PyDESeq2 and GSEA.</p>"},{"location":"api/enrichment/#overview","title":"Overview","text":"<p>The <code>renalprog.enrichment</code> module provides functions for differential expression analysis and Gene Set Enrichment Analysis (GSEA) to identify biological pathways enriched in cancer progression trajectories.</p>"},{"location":"api/enrichment/#pipeline-usage","title":"Pipeline Usage","text":"<p>For running the complete enrichment pipeline, see the scripts in <code>scripts/enrichment/</code>:</p> <ul> <li><code>py_deseq.py</code> - Process trajectories of synthetic patient trajectories (or real ones, if you could ever get these).</li> <li><code>py_deseq_real.py</code> - Process real patient data.</li> <li><code>trajectory_formatting.py</code> - Combine GSEA results across multiple patients or trajectories.</li> </ul>"},{"location":"api/enrichment/#running-the-pipeline","title":"Running the Pipeline","text":"<p>For synthetic trajectories: <pre><code>python scripts/enrichment/py_deseq.py \\\n  --cancer_type kirc \\\n  --traj_dir data/interim/trajectories \\\n  --source_target_file data/interim/source_target.csv \\\n  --patient_stage_file data/interim/patient_stages.csv \\\n  --stage_transition early_to_late \\\n  --file data/interim/trajectories/early_to_late/patient_001.csv \\\n  --nperm 1000 \\\n  --pathway_file data/external/ReactomePathways.gmt\n</code></pre></p> <p>For real patients: <pre><code>python scripts/enrichment/py_deseq_real.py \\\n  --cancer_type kirc \\\n  --data_dir data/interim/preprocessed/rnaseq.csv \\\n  --metadata_dir data/interim/preprocessed/clinical.csv \\\n  --nperm 1000 \\\n  --pathway_file data/external/ReactomePathways.gmt\n</code></pre></p>"},{"location":"api/enrichment/#tutorial","title":"Tutorial","text":"<p>For a complete step-by-step guide, see the Step 6: Enrichment Analysis Tutorial.</p>"},{"location":"api/enrichment/#api-reference","title":"API Reference","text":""},{"location":"api/enrichment/#core-functions","title":"Core Functions","text":""},{"location":"api/enrichment/#build_gsea_command","title":"build_gsea_command","text":"<p>Example: <pre><code>from renalprog.enrichment import build_gsea_command\n\n# Basic usage with defaults\ncmd = build_gsea_command(\n    save_path_rnk_fun=\"data/interim/analysis/patient_001\",\n    filename_rnk_fun=\"patient_001.rnk\"\n)\n\n# Custom GSEA parameters\ncmd = build_gsea_command(\n    save_path_rnk_fun=\"data/interim/analysis/patient_001\",\n    filename_rnk_fun=\"patient_001.rnk\",\n    pathway_file=\"data/external/c2.cp.kegg_legacy.v2025.1.Hs.symbols.gmt\",\n    nperm=5000,\n    rnd_seed=\"42\",\n    set_max=800,\n    set_min=10\n)\nprint(cmd)\n</code></pre></p>"},{"location":"api/enrichment/#renalprog.enrichment.build_gsea_command","title":"build_gsea_command","text":"<pre><code>build_gsea_command(\n    save_path_rnk_fun: str,\n    filename_rnk_fun: str,\n    pathway_file: str = \"data/external/ReactomePathways.gmt\",\n    mode: str = \"Max_probe\",\n    norm: str = \"meandiv\",\n    nperm: int = 1000,\n    rnd_seed: str = \"timestamp\",\n    scoring_scheme: str = \"weighted\",\n    set_max: int = 500,\n    set_min: int = 15,\n)\n</code></pre> <p>Build a command string for running GSEA (Gene Set Enrichment Analysis) with pre-ranked gene lists.</p> <p>Parameters: - save_path_rnk_fun (str): The directory where the pre-ranked gene list file is stored. - filename_rnk_fun (str): The name of the pre-ranked gene list file. - pathway_file (str, optional): Path to the gene set file in GMT format. Default is 'data/external/ReactomePathways.gmt'. - mode (str, optional): GSEA mode. Default is 'Max_probe'. - norm (str, optional): Normalization method. Default is 'meandiv'. - nperm (int, optional): Number of permutations. Default is 1000. - rnd_seed (str, optional): Random seed for reproducibility. Default is 'timestamp'. - scoring_scheme (str, optional): Scoring scheme for GSEA. Default is 'weighted'. - set_max (int, optional): Maximum size of gene sets to consider. Default is 500. - set_min (int, optional): Minimum size of gene sets to consider. Default is 15.</p> <p>Returns: str: The GSEA command string.</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def build_gsea_command(save_path_rnk_fun:str, filename_rnk_fun:str,\n                       pathway_file:str='data/external/ReactomePathways.gmt',\n                       mode:str='Max_probe',norm:str='meandiv', nperm:int=1000, rnd_seed:str='timestamp',\n                       scoring_scheme:str='weighted', set_max:int=500, set_min:int=15):\n    \"\"\"\n    Build a command string for running GSEA (Gene Set Enrichment Analysis) with pre-ranked gene lists.\n\n    Parameters:\n    - save_path_rnk_fun (str): The directory where the pre-ranked gene list file is stored.\n    - filename_rnk_fun (str): The name of the pre-ranked gene list file.\n    - pathway_file (str, optional): Path to the gene set file in GMT format. Default is 'data/external/ReactomePathways.gmt'.\n    - mode (str, optional): GSEA mode. Default is 'Max_probe'.\n    - norm (str, optional): Normalization method. Default is 'meandiv'.\n    - nperm (int, optional): Number of permutations. Default is 1000.\n    - rnd_seed (str, optional): Random seed for reproducibility. Default is 'timestamp'.\n    - scoring_scheme (str, optional): Scoring scheme for GSEA. Default is 'weighted'.\n    - set_max (int, optional): Maximum size of gene sets to consider. Default is 500.\n    - set_min (int, optional): Minimum size of gene sets to consider. Default is 15.\n\n    Returns:\n    str: The GSEA command string.\n    \"\"\"\n\n    # Start of the GSEA command\n    start = 'gsea GSEAPreranked '\n\n    # File path of the pre-ranked gene list\n    rnk = '-rnk ' + os.path.join(save_path_rnk_fun, filename_rnk_fun) + ' '\n\n    # Path to the gene set file in GMT format\n    gmx = f'-gmx {pathway_file} '\n\n    # GSEA options\n    options = (\n            '-collapse No_Collapse '\n            + f'-mode {mode} '\n            + f'-norm {norm} -nperm {nperm} '\n            + f'-rnd_seed {rnd_seed} '\n            + f'-scoring_scheme {scoring_scheme} '\n    )\n\n    # Report label option\n    label = \"-rpt_label \" + filename_rnk_fun[:-4] + \" \"\n\n    # Additional GSEA options\n    options2 = (\n            '-create_svgs false '\n            + '-include_only_symbols true '\n            + '-make_sets false '\n            + '-plot_top_x 20 '\n            + f'-set_max {set_max} '\n            + f'-set_min {set_min} '\n            + '-zip_report false '\n    )\n\n    # Output directory option\n    out = '-out ' + os.path.join(save_path_rnk_fun, 'reports/')\n\n    # Concatenate all components to form the complete GSEA command\n    gsea_command = start + rnk + gmx + options + label + options2 + out\n\n    return gsea_command\n</code></pre>"},{"location":"api/enrichment/#get_rnk_single_patient","title":"get_rnk_single_patient","text":"<p>Example: <pre><code>from renalprog.enrichment import get_rnk_single_patient\nimport pandas as pd\n\n# Prepare fold change data (from DESeq2 analysis)\nfold_change_df = pd.DataFrame({\n    'log2FoldChange': [2.5, -1.8, 0.5, 3.2, -2.1]\n}, index=['GENE1', 'GENE2', 'GENE3', 'GENE4', 'GENE5'])\n\n# Generate GSEA command for single patient\ngsea_cmd = get_rnk_single_patient(\n    path_above=\"data/interim/analysis\",\n    genes_here=['GENE1', 'GENE2', 'GENE3', 'GENE4', 'GENE5'],\n    samples_real_l=fold_change_df,\n    pat_i=\"TCGA-A3-3306\",\n    index_pat=None,\n    foldchange=True,\n    nperm=1000\n)\n</code></pre></p>"},{"location":"api/enrichment/#renalprog.enrichment.get_rnk_single_patient","title":"get_rnk_single_patient","text":"<pre><code>get_rnk_single_patient(\n    path_above,\n    genes_here,\n    samples_real_l,\n    pat_i,\n    index_pat=None,\n    foldchange=True,\n    pathway_file=\"data/external/ReactomePathways.gmt\",\n    mode=\"Max_probe\",\n    norm=\"meandiv\",\n    nperm=1000,\n    rnd_seed=\"timestamp\",\n    scoring_scheme=\"weighted\",\n    set_max=500,\n    set_min=15,\n)\n</code></pre> <p>Generate GSEA command for a single patient based on gene expression data.</p> <p>Parameters: - path_above (str): The parent directory for saving patient-specific data. - genes_here (list): List of gene names. - samples_real_l (pd.DataFrame): DataFrame containing gene expression data for single patient sample. - pat_i (str): Patient identifier. - index_pat (int, optional): Index of the patient. Default is None. - foldchange (bool, optional): If True, log2 fold change values are used; otherwise, raw gene expression values. - pathway_file (str, optional): Path to the GMT file containing gene sets. Default is 'data/external/ReactomePathways.gmt'. - mode (str, optional): GSEA collapse mode. Default is 'Max_probe'. - norm (str, optional): GSEA normalization mode. Default is 'meandiv'. - nperm (int, optional): Number of permutations. Default is 1000. - rnd_seed (str, optional): Random seed for GSEA. Default is 'timestamp'. - scoring_scheme (str, optional): GSEA scoring scheme. Default is 'weighted'. - set_max (int, optional): Maximum size of gene sets. Default is 500. - set_min (int, optional): Minimum size of gene sets. Default is 15.</p> <p>Returns: str: The GSEA command string for the given patient and gene expression data.</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def get_rnk_single_patient(path_above, genes_here, samples_real_l, pat_i, index_pat=None, foldchange=True,\n                          pathway_file='data/external/ReactomePathways.gmt',\n                          mode='Max_probe', norm='meandiv', nperm=1000, rnd_seed='timestamp',\n                          scoring_scheme='weighted', set_max=500, set_min=15):\n    \"\"\"\n    Generate GSEA command for a single patient based on gene expression data.\n\n    Parameters:\n    - path_above (str): The parent directory for saving patient-specific data.\n    - genes_here (list): List of gene names.\n    - samples_real_l (pd.DataFrame): DataFrame containing gene expression data for single patient sample.\n    - pat_i (str): Patient identifier.\n    - index_pat (int, optional): Index of the patient. Default is None.\n    - foldchange (bool, optional): If True, log2 fold change values are used; otherwise, raw gene expression values.\n    - pathway_file (str, optional): Path to the GMT file containing gene sets. Default is 'data/external/ReactomePathways.gmt'.\n    - mode (str, optional): GSEA collapse mode. Default is 'Max_probe'.\n    - norm (str, optional): GSEA normalization mode. Default is 'meandiv'.\n    - nperm (int, optional): Number of permutations. Default is 1000.\n    - rnd_seed (str, optional): Random seed for GSEA. Default is 'timestamp'.\n    - scoring_scheme (str, optional): GSEA scoring scheme. Default is 'weighted'.\n    - set_max (int, optional): Maximum size of gene sets. Default is 500.\n    - set_min (int, optional): Minimum size of gene sets. Default is 15.\n\n    Returns:\n    str: The GSEA command string for the given patient and gene expression data.\n    \"\"\"\n\n    #############################################################\n    # IF FOLDCHANGE IS TRUE\n    #############################################################\n    if foldchange:\n        # Create directories if they don't exist\n        os.makedirs(path_above, exist_ok=True)\n\n        # DataFrame for patient i\n        df_i = pd.DataFrame(samples_real_l['log2FoldChange'])\n        df_i.index.name = '#'\n        # print('path_above:', path_above)\n        # print('pat_i:', pat_i)\n        path_i = os.path.join(path_above, pat_i)\n\n        os.makedirs(path_i, exist_ok=True)\n\n        os.makedirs(os.path.join(path_i, 'reports'), exist_ok=True)\n\n        # Save DataFrame as .rnk file\n        if index_pat is not None:\n            df_i.to_csv(os.path.join(path_i, pat_i + '_' + str(index_pat) + '.rnk'), sep='\\t')\n            # Get GSEA command\n            gsea_command_k = build_gsea_command(\n                save_path_rnk_fun=path_i,\n                filename_rnk_fun=pat_i + '_' + str(index_pat) + '.rnk',\n                pathway_file=pathway_file, mode=mode, norm=norm, nperm=nperm, rnd_seed=rnd_seed,\n                scoring_scheme=scoring_scheme, set_max=set_max, set_min=set_min\n            )\n        else:\n            df_i.to_csv(os.path.join(path_i, pat_i + '.rnk'), sep='\\t')\n            # Get GSEA command\n            gsea_command_k = build_gsea_command(\n                save_path_rnk_fun=path_i,\n                filename_rnk_fun=pat_i + '.rnk',\n                pathway_file=pathway_file, mode=mode, norm=norm, nperm=nperm, rnd_seed=rnd_seed,\n                scoring_scheme=scoring_scheme, set_max=set_max, set_min=set_min\n            )\n\n\n\n    #############################################################\n    # ELSE FOLDCHANGE FALSE\n    #############################################################\n    else:\n        raise \"This option has been deprecated. GSEA should use fold change values from DESeq2 analysis.\"\n\n    return gsea_command_k\n</code></pre>"},{"location":"api/enrichment/#fun_apply_deseq","title":"fun_apply_deseq","text":"<p>Example: <pre><code>from renalprog.enrichment import fun_apply_deseq\nimport pandas as pd\nimport numpy as np\n\n# Prepare RNA-seq data (log2(RSEM+1) format)\nrnaseq_data = pd.DataFrame(\n    np.random.uniform(0, 10, (100, 5)),  # 100 genes, 5 samples\n    index=[f'GENE{i}' for i in range(100)],\n    columns=['Sample1', 'Sample2', 'Sample3', 'Sample4', 'Sample5']\n)\n\n# Prepare clinical metadata\nclinical_data = pd.DataFrame({\n    'ajcc_pathologic_tumor_stage': ['Stage I', 'Stage I', 'Stage III', 'Stage III', 'Stage III']\n}, index=['Sample1', 'Sample2', 'Sample3', 'Sample4', 'Sample5'])\n\n# Run DESeq2 analysis\nresults_df = fun_apply_deseq(\n    rnaseq_in=rnaseq_data,\n    clinical_in=clinical_data\n)\n\n# Results contain: log2FoldChange, pvalue, padj, etc.\nprint(results_df[['log2FoldChange', 'pvalue', 'padj']].head())\n</code></pre></p>"},{"location":"api/enrichment/#renalprog.enrichment.fun_apply_deseq","title":"fun_apply_deseq","text":"<pre><code>fun_apply_deseq(rnaseq_in, clinical_in)\n</code></pre> <p>Perform DESeq2 analysis on RNA-seq data.</p> <p>Parameters: - rnaseq_in (pd.DataFrame): RNA-seq count data. - clinical_in (pd.DataFrame): Clinical metadata, including the cancer stage.</p> <p>Returns: pd.DataFrame: DataFrame containing DESeq2 analysis results.</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def fun_apply_deseq(rnaseq_in, clinical_in):\n    '''\n    Perform DESeq2 analysis on RNA-seq data.\n\n    Parameters:\n    - rnaseq_in (pd.DataFrame): RNA-seq count data.\n    - clinical_in (pd.DataFrame): Clinical metadata, including the cancer stage.\n\n    Returns:\n    pd.DataFrame: DataFrame containing DESeq2 analysis results.\n    '''\n\n    # Re-scale to have RSEM\n    # The input values are in log2(RSEM+1) format, so we need to convert them to RSEM, rounding to the nearest integer.\n    rsem = pd.DataFrame(np.round(2 ** rnaseq_in - 1), dtype=int)\n    # Get maximum value in rnaseq_in\n    # Print dtype of rnaseq_in\n    # Extract the condition (cancer stage)\n    condition = pd.DataFrame(clinical_in['ajcc_pathologic_tumor_stage'])\n    condition.columns = ['condition']\n\n    # Construct DESEQDataSet Object\n    dds_i = DeseqDataSet(\n        counts=rsem,\n        metadata=condition,\n        design_factors='condition',\n        refit_cooks=True,\n        quiet=False\n    )\n\n    # Once a DeseqDataSet was initialized, run the deseq2() method to fit dispersions and LFCs\n    dds_i.deseq2()\n\n    # Statistical analysis\n    stat_res = DeseqStats(dds_i, alpha=0.05, cooks_filter=True, independent_filter=True, quiet=False)\n\n    # Summary of results\n    stat_res.summary()\n\n    return stat_res.results_df\n</code></pre>"},{"location":"api/enrichment/#fun_single_patient_and_gsea","title":"fun_single_patient_and_gsea","text":"<p>Example: <pre><code>from renalprog.enrichment import fun_single_patient_and_gsea\nimport pandas as pd\n\n# Load your data\nrnaseq_df = pd.read_csv('data/interim/rnaseq.csv', index_col=0)\nclinical_controls_df = pd.read_csv('data/processed/controls/clinical_control.csv', index_col=0)\nrnaseq_controls_df = pd.read_csv('data/processed/controls/rna_control.csv', index_col=0)\ngenes = rnaseq_df.columns.tolist()\n\n# Analyze single patient with custom GSEA parameters\ngsea_cmd = fun_single_patient_and_gsea(\n    patient_here='TCGA-A3-3306',\n    patient_stage='Stage II',\n    rnaseq_data=rnaseq_df,\n    path_above_in='data/interim/analysis',\n    clinical_controls=clinical_controls_df,\n    rnaseq_controls=rnaseq_controls_df,\n    gene_list=genes,\n    foldchange=True,\n    # GSEA parameters\n    pathway_file='data/external/ReactomePathways.gmt',\n    nperm=5000,\n    rnd_seed='42'\n)\n\n# The function generates .rnk files and returns the GSEA command to run\nprint(gsea_cmd)\n</code></pre></p>"},{"location":"api/enrichment/#renalprog.enrichment.fun_single_patient_and_gsea","title":"fun_single_patient_and_gsea","text":"<pre><code>fun_single_patient_and_gsea(\n    patient_here,\n    patient_stage,\n    rnaseq_data,\n    path_above_in,\n    clinical_controls,\n    rnaseq_controls,\n    gene_list,\n    foldchange=True,\n    pathway_file=\"data/external/ReactomePathways.gmt\",\n    mode=\"Max_probe\",\n    norm=\"meandiv\",\n    nperm=1000,\n    rnd_seed=\"timestamp\",\n    scoring_scheme=\"weighted\",\n    set_max=500,\n    set_min=15,\n)\n</code></pre> <p>Perform analysis for a single patient and generate GSEA-related files.</p> <p>Parameters: - patient_here: Current patient ID. - patient_stage: DataFrame containing patient stage information. - rnaseq_data: DataFrame containing RNAseq data. - path_above_in: Path to the directory containing patient data. - clinical_controls: DataFrame containing clinical control data. - rnaseq_controls: DataFrame containing RNAseq control data. - gene_list: List of genes. - foldchange: Boolean flag indicating whether to use fold change data in the analysis. Default is True. - pathway_file (str, optional): Path to the GMT file containing gene sets. Default is 'data/external/ReactomePathways.gmt'. - mode (str, optional): GSEA collapse mode. Default is 'Max_probe'. - norm (str, optional): GSEA normalization mode. Default is 'meandiv'. - nperm (int, optional): Number of permutations. Default is 1000. - rnd_seed (str, optional): Random seed for GSEA. Default is 'timestamp'. - scoring_scheme (str, optional): GSEA scoring scheme. Default is 'weighted'. - set_max (int, optional): Maximum size of gene sets. Default is 500. - set_min (int, optional): Minimum size of gene sets. Default is 15.</p> <p>Returns: - bash_file_l: List of bash commands generated during the analysis.</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def fun_single_patient_and_gsea(\n        patient_here,\n        patient_stage,\n        rnaseq_data,\n        path_above_in,\n        clinical_controls,\n        rnaseq_controls,\n        gene_list,\n        foldchange=True,\n        pathway_file='data/external/ReactomePathways.gmt',\n        mode='Max_probe',\n        norm='meandiv',\n        nperm=1000,\n        rnd_seed='timestamp',\n        scoring_scheme='weighted',\n        set_max=500,\n        set_min=15\n):\n    '''\n    Perform analysis for a single patient and generate GSEA-related files.\n\n    Parameters:\n    - patient_here: Current patient ID.\n    - patient_stage: DataFrame containing patient stage information.\n    - rnaseq_data: DataFrame containing RNAseq data.\n    - path_above_in: Path to the directory containing patient data.\n    - clinical_controls: DataFrame containing clinical control data.\n    - rnaseq_controls: DataFrame containing RNAseq control data.\n    - gene_list: List of genes.\n    - foldchange: Boolean flag indicating whether to use fold change data in the analysis. Default is True.\n    - pathway_file (str, optional): Path to the GMT file containing gene sets. Default is 'data/external/ReactomePathways.gmt'.\n    - mode (str, optional): GSEA collapse mode. Default is 'Max_probe'.\n    - norm (str, optional): GSEA normalization mode. Default is 'meandiv'.\n    - nperm (int, optional): Number of permutations. Default is 1000.\n    - rnd_seed (str, optional): Random seed for GSEA. Default is 'timestamp'.\n    - scoring_scheme (str, optional): GSEA scoring scheme. Default is 'weighted'.\n    - set_max (int, optional): Maximum size of gene sets. Default is 500.\n    - set_min (int, optional): Minimum size of gene sets. Default is 15.\n\n    Returns:\n    - bash_file_l: List of bash commands generated during the analysis.\n    '''\n    # Get stage of the current patient\n    # stage_pat_i= str(patient_stage['stage'][patient_stage['name'] == patient_here].values[0])\n    stage_pat_i = patient_stage\n    # print('Stage ',stage_pat_i)\n\n    # Prepare RNAseq and clinical data for analysis\n    rna_kk = pd.DataFrame(rnaseq_data.loc[patient_here].values.reshape(1, -1),\n                          index=[patient_here], columns=gene_list)\n    rna_kk = pd.concat([rna_kk, rnaseq_controls], axis=0, ignore_index=False)\n\n    cli_kk = pd.concat(\n        [\n            # pd.DataFrame([stage_pat_i], columns=['ajcc_pathologic_tumor_stage'], index=[patient_here]),\n            pd.DataFrame(stage_pat_i, columns=['ajcc_pathologic_tumor_stage'], index=[patient_here]),\n            clinical_controls,\n        ],\n        axis=0,\n        ignore_index=False,\n    )\n    # print('cli_kk head():', cli_kk.head())\n    # Use DESeq to get fold change for each interpolated patient\n    fold_change_df_i = fun_apply_deseq(rnaseq_in=rna_kk, clinical_in=cli_kk)\n\n    # Generate GSEA-related files and bash commands\n    # print('path_above_in:', path_above_in)\n    # print('stage_pat_i:', stage_pat_i)\n    path_above = os.path.join(path_above_in, stage_pat_i)\n    bash_file_l = get_rnk_single_patient(\n        path_above=path_above,\n        genes_here=gene_list,\n        samples_real_l=fold_change_df_i,\n        pat_i=patient_here,\n        foldchange=foldchange,\n        pathway_file=pathway_file,\n        mode=mode,\n        norm=norm,\n        nperm=nperm,\n        rnd_seed=rnd_seed,\n        scoring_scheme=scoring_scheme,\n        set_max=set_max,\n        set_min=set_min\n    )\n\n    # print('Done with patient ' + stage_pat_i)\n    return bash_file_l\n</code></pre>"},{"location":"api/enrichment/#fun_synth_single_patient_and_gsea","title":"fun_synth_single_patient_and_gsea","text":"<p>Example: <pre><code>from renalprog.enrichment import fun_synth_single_patient_and_gsea\nimport pandas as pd\n\n# Load synthetic trajectory data\nsynth_traj = pd.read_csv('data/interim/trajectories/early_to_late/traj_001.csv', index_col=0)\nclinical_controls_df = pd.read_csv('data/processed/controls/clinical_control.csv', index_col=0)\nrnaseq_controls_df = pd.read_csv('data/processed/controls/rna_control.csv', index_col=0)\ngenes = synth_traj.index.tolist()\n\n# Analyze synthetic patient at interpolation point 25\nsynth_patient_data = pd.DataFrame(synth_traj.iloc[:, 25])  # Column 25\ngsea_cmd = fun_synth_single_patient_and_gsea(\n    patient_here='TCGA-A3-3306_to_TCGA-A3-3307_25',\n    stage_trans_i='early_to_late',\n    rnaseq_data=synth_patient_data,\n    path_above_in='data/interim/enrichment',\n    clinical_controls=clinical_controls_df,\n    rnaseq_controls=rnaseq_controls_df,\n    gene_list=genes,\n    index_pat=25,\n    foldchange=True,\n    # GSEA parameters\n    pathway_file='data/external/ReactomePathways.gmt',\n    nperm=1000,\n    rnd_seed='timestamp'\n)\n</code></pre></p>"},{"location":"api/enrichment/#renalprog.enrichment.fun_synth_single_patient_and_gsea","title":"fun_synth_single_patient_and_gsea","text":"<pre><code>fun_synth_single_patient_and_gsea(\n    patient_here,\n    stage_trans_i,\n    rnaseq_data,\n    path_above_in,\n    clinical_controls,\n    rnaseq_controls,\n    gene_list,\n    index_pat=None,\n    foldchange=True,\n    pathway_file=\"data/external/ReactomePathways.gmt\",\n    mode=\"Max_probe\",\n    norm=\"meandiv\",\n    nperm=1000,\n    rnd_seed=\"timestamp\",\n    scoring_scheme=\"weighted\",\n    set_max=500,\n    set_min=15,\n)\n</code></pre> <p>Perform analysis for a single patient and generate GSEA-related files.</p> <p>Parameters: - patient_here: str Current synth patient ID (like: pat_to_pat_interpolnum). - stage_trans_i: str containing synthetic patient stage transition information. 'I_to_II' or 'II_to_III'. - rnaseq_data: DataFrame containing RNAseq data only of synthetic patient to analyze. - path_above_in: Path to the directory containing patient data. - clinical_controls: DataFrame containing clinical control data. - rnaseq_controls: DataFrame containing RNAseq control data. - gene_list: List of genes. - index_pat: int, optional: Index of the synthetic patient. Default is None. - foldchange: Boolean flag indicating whether to use fold change data in the analysis. Default is True. - pathway_file (str, optional): Path to the GMT file containing gene sets. Default is 'data/external/ReactomePathways.gmt'. - mode (str, optional): GSEA collapse mode. Default is 'Max_probe'. - norm (str, optional): GSEA normalization mode. Default is 'meandiv'. - nperm (int, optional): Number of permutations. Default is 1000. - rnd_seed (str, optional): Random seed for GSEA. Default is 'timestamp'. - scoring_scheme (str, optional): GSEA scoring scheme. Default is 'weighted'. - set_max (int, optional): Maximum size of gene sets. Default is 500. - set_min (int, optional): Minimum size of gene sets. Default is 15.</p> <p>Returns: - bash_file_l: List of bash commands generated during the analysis.</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def fun_synth_single_patient_and_gsea(\n        patient_here,\n        stage_trans_i,\n        rnaseq_data,\n        path_above_in,\n        clinical_controls,\n        rnaseq_controls,\n        gene_list,\n        index_pat=None,\n        foldchange=True,\n        pathway_file='data/external/ReactomePathways.gmt',\n        mode='Max_probe',\n        norm='meandiv',\n        nperm=1000,\n        rnd_seed='timestamp',\n        scoring_scheme='weighted',\n        set_max=500,\n        set_min=15\n):\n    '''\n    Perform analysis for a single patient and generate GSEA-related files.\n\n    Parameters:\n    - patient_here: str Current synth patient ID (like: pat_to_pat_interpolnum).\n    - stage_trans_i: str containing synthetic patient stage transition information. 'I_to_II' or 'II_to_III'.\n    - rnaseq_data: DataFrame containing RNAseq data only of synthetic patient to analyze.\n    - path_above_in: Path to the directory containing patient data.\n    - clinical_controls: DataFrame containing clinical control data.\n    - rnaseq_controls: DataFrame containing RNAseq control data.\n    - gene_list: List of genes.\n    - index_pat: int, optional: Index of the synthetic patient. Default is None.\n    - foldchange: Boolean flag indicating whether to use fold change data in the analysis. Default is True.\n    - pathway_file (str, optional): Path to the GMT file containing gene sets. Default is 'data/external/ReactomePathways.gmt'.\n    - mode (str, optional): GSEA collapse mode. Default is 'Max_probe'.\n    - norm (str, optional): GSEA normalization mode. Default is 'meandiv'.\n    - nperm (int, optional): Number of permutations. Default is 1000.\n    - rnd_seed (str, optional): Random seed for GSEA. Default is 'timestamp'.\n    - scoring_scheme (str, optional): GSEA scoring scheme. Default is 'weighted'.\n    - set_max (int, optional): Maximum size of gene sets. Default is 500.\n    - set_min (int, optional): Minimum size of gene sets. Default is 15.\n\n    Returns:\n    - bash_file_l: List of bash commands generated during the analysis.\n    '''\n    # Get stage of the current patient\n    # stage_pat_i = str(patient_stage['stage'][patient_stage['name'] == patient_here].values[0])\n    assert stage_trans_i in ['I', 'II', 'III', 'I_to_II', 'II_to_III', '1_to_2', '2_to_3', 'early_to_late',\n                             'early_to_early'], 'Stage not recognized'\n\n    # Prepare RNAseq and clinical data for analysis\n    # rna_kk = pd.DataFrame(rnaseq_data.loc[patient_here].values.reshape(1, -1),\n    #                       index=[patient_here], columns=gene_list)\n    rna_kk = pd.DataFrame(rnaseq_data.values.reshape(1, -1))\n    rna_kk.columns = gene_list\n    rna_kk.index = [patient_here]\n    rna_kk = pd.concat([rna_kk, rnaseq_controls], axis=0, ignore_index=False)\n    assert ~rna_kk.isna().any().any(), \"NAs detected in input data. Make sure all data is numeric\"\n    # Build clinical data with the patient's stage and the control data\n    # The string stage_pat_i is transformed into a DataFrame with the same index as the\n    # patient, and is then concatenated with the control data.\n    cli_kk = pd.concat(\n        [\n            pd.DataFrame([stage_trans_i],\n                         columns=['ajcc_pathologic_tumor_stage'],\n                         index=[patient_here]),\n            clinical_controls,\n        ],\n        axis=0,\n        ignore_index=False,\n    )\n\n    # Use DESeq to get fold change for each interpolated patient\n    print(f\"Applying DESeq for synthetic patient {patient_here} with stage transition {stage_trans_i}\")\n    fold_change_df_i = fun_apply_deseq(rnaseq_in=rna_kk, clinical_in=cli_kk)\n\n    # Generate GSEA-related files and bash commands\n    bash_file_l = get_rnk_single_patient(\n        path_above=os.path.join(path_above_in, stage_trans_i),\n        genes_here=gene_list,\n        samples_real_l=fold_change_df_i,\n        pat_i=patient_here,\n        index_pat=index_pat,\n        foldchange=foldchange,\n        pathway_file=pathway_file,\n        mode=mode,\n        norm=norm,\n        nperm=nperm,\n        rnd_seed=rnd_seed,\n        scoring_scheme=scoring_scheme,\n        set_max=set_max,\n        set_min=set_min\n    )\n\n    # print('Done with patient ' + stage_trans_i)\n    return bash_file_l\n</code></pre>"},{"location":"api/enrichment/#pathway-visualization","title":"Pathway Visualization","text":""},{"location":"api/enrichment/#generate_pathway_heatmap","title":"generate_pathway_heatmap","text":"<p>Example: <pre><code>from renalprog.enrichment import generate_pathway_heatmap\nimport pandas as pd\n\n# Load enrichment results (combined GSEA output)\nenrichment_df = pd.read_csv(\n    'data/processed/enrichment/trajectory_enrichment.csv',\n    index_col=0\n)\n\n# Generate pathway heatmaps\nheatmap_data, figures = generate_pathway_heatmap(\n    enrichment_df=enrichment_df,\n    output_dir='data/processed/enrichment/heatmaps',\n    fdr_threshold=0.05,\n    colorbar=True,\n    legend=False,\n    yticks_fontsize=10,\n    show=False\n)\n\n# Save figures\nfor name, fig in figures.items():\n    fig.savefig(f'data/processed/enrichment/heatmaps/{name}.png', dpi=300, bbox_inches='tight')\n\n# Inspect top pathways\nprint(heatmap_data['top_50_changing'].head())\n</code></pre></p>"},{"location":"api/enrichment/#renalprog.enrichment.generate_pathway_heatmap","title":"generate_pathway_heatmap","text":"<pre><code>generate_pathway_heatmap(\n    enrichment_df: DataFrame,\n    output_dir: str,\n    fdr_threshold: float = 0.05,\n    colorbar: bool = True,\n    legend: bool = False,\n    yticks_fontsize: int = 12,\n    show: bool = False,\n) -&gt; Tuple[pd.DataFrame, Dict[str, matplotlib.figure.Figure]]\n</code></pre> <p>Generate multiple pathway enrichment heatmaps from GSEA results.</p> <p>This function creates several heatmaps showing the sum of NES (Normalized Enrichment Score) across all trajectories for each pathway at each timepoint:</p> <ol> <li>Top 50 most changing pathways (first vs last timepoint)</li> <li>Top 50 most upregulated pathways (average NES &gt; 0)</li> <li>Top 50 most downregulated pathways (average NES &lt; 0)</li> <li>Selected pathways (high-level Reactome + literature pathways)</li> </ol> <p>The heatmaps have: - Rows: Pathway names - Columns: Timepoints (pseudo-time from early to late) - Values: Sum of NES across all trajectories at each timepoint</p> <p>Args:     enrichment_df: DataFrame with columns [Patient, Idx, Transition, NAME, ES, NES, FDR q-val]     output_dir: Output directory for heatmap files     fdr_threshold: FDR q-value threshold for significance (default: 0.05)     colorbar: Whether to show colorbar (default: True)     legend: Whether to show legend (default: False)     yticks_fontsize: Font size for y-axis tick labels (default: 12)     show: Whether to display the plot (default: False)</p> <p>Returns:     Tuple of (heatmap_data, figures_dict):         - heatmap_data: DataFrame with summed NES values (pathways \u00d7 timepoints)         - figures_dict: Dictionary mapping figure names to Matplotlib Figure objects</p> <p>Example:     &gt;&gt;&gt; enrichment_df = pd.read_csv('trajectory_enrichment.csv')     &gt;&gt;&gt; heatmap_data, figs = generate_pathway_heatmap(     ...     enrichment_df=enrichment_df,     ...     output_dir='results/',     ...     fdr_threshold=0.05     ... )     &gt;&gt;&gt; print(f\"Generated {len(figs)} heatmaps\")</p> Source code in <code>renalprog/enrichment.py</code> <pre><code>def generate_pathway_heatmap(\n    enrichment_df: pd.DataFrame,\n    output_dir: str,\n    fdr_threshold: float = 0.05,\n    colorbar: bool = True,\n    legend: bool = False,\n    yticks_fontsize: int = 12,\n    show: bool = False,\n) -&gt; Tuple[pd.DataFrame, Dict[str, \"matplotlib.figure.Figure\"]]:\n    \"\"\"\n    Generate multiple pathway enrichment heatmaps from GSEA results.\n\n    This function creates several heatmaps showing the sum of NES (Normalized Enrichment Score)\n    across all trajectories for each pathway at each timepoint:\n\n    1. Top 50 most changing pathways (first vs last timepoint)\n    2. Top 50 most upregulated pathways (average NES &gt; 0)\n    3. Top 50 most downregulated pathways (average NES &lt; 0)\n    4. Selected pathways (high-level Reactome + literature pathways)\n\n    The heatmaps have:\n    - Rows: Pathway names\n    - Columns: Timepoints (pseudo-time from early to late)\n    - Values: Sum of NES across all trajectories at each timepoint\n\n    Args:\n        enrichment_df: DataFrame with columns [Patient, Idx, Transition, NAME, ES, NES, FDR q-val]\n        output_dir: Output directory for heatmap files\n        fdr_threshold: FDR q-value threshold for significance (default: 0.05)\n        colorbar: Whether to show colorbar (default: True)\n        legend: Whether to show legend (default: False)\n        yticks_fontsize: Font size for y-axis tick labels (default: 12)\n        show: Whether to display the plot (default: False)\n\n    Returns:\n        Tuple of (heatmap_data, figures_dict):\n            - heatmap_data: DataFrame with summed NES values (pathways \u00d7 timepoints)\n            - figures_dict: Dictionary mapping figure names to Matplotlib Figure objects\n\n    Example:\n        &gt;&gt;&gt; enrichment_df = pd.read_csv('trajectory_enrichment.csv')\n        &gt;&gt;&gt; heatmap_data, figs = generate_pathway_heatmap(\n        ...     enrichment_df=enrichment_df,\n        ...     output_dir='results/',\n        ...     fdr_threshold=0.05\n        ... )\n        &gt;&gt;&gt; print(f\"Generated {len(figs)} heatmaps\")\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as mpatches\n    import matplotlib.colors as mcolors\n\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"Generating pathway enrichment heatmaps (FDR &lt; {fdr_threshold})...\")\n\n    # Define pathway lists\n    highest_pathways = [\n        \"Autophagy\",\n        \"Cell Cycle\",\n        \"Cell-Cell communication\",\n        \"Cellular responses to stimuli\",\n        \"Chromatin organization\",\n        \"Circadian Clock\",\n        \"DNA Repair\",\n        \"DNA Replication\",\n        \"Developmental Biology\",\n        \"Digestion and absorption\",\n        \"Disease\",\n        \"Drug ADME\",\n        \"Extracellular matrix organization\",\n        \"Gene expression (Transcription)\",\n        \"Hemostasis\",\n        \"Immune System\",\n        \"Metabolism\",\n        \"Metabolism of RNA\",\n        \"Metabolism of proteins\",\n        \"Muscle contraction\",\n        \"Neuronal System\",\n        \"Organelle biogenesis and maintenance\",\n        \"Programmed Cell Death\",\n        \"Protein localization\",\n        \"Reproduction\",\n        \"Sensory Perception\",\n        \"Signal Transduction\",\n        \"Transport of small molecules\",\n        \"Vesicle-mediated transport\",\n    ]\n\n    pathways_literature = [\n        # VHL/HIF pathway\n        \"CELLULAR RESPONSE TO HYPOXIA\",\n        \"OXYGEN-DEPENDENT PROLINE HYDROXYLATION OF HYPOXIA-INDUCIBLE FACTOR ALPHA\",\n        \"REGULATION OF GENE EXPRESSION BY HYPOXIA-INDUCIBLE FACTOR\",\n        # PI3K/AKT/MTOR Pathway\n        \"PI3K/AKT ACTIVATION\",\n        \"PI3K/AKT SIGNALING IN CANCER\",\n        \"MTOR SIGNALLING\",\n        # Warburg effect\n        \"TP53 REGULATES METABOLIC GENES\",\n        \"GLYCOLYSIS\",\n        \"GLUCOSE METABOLISM\",\n        # TCA/Krebs cycle\n        \"CITRIC ACID CYCLE (TCA CYCLE)\",\n        \"THE CITRIC ACID (TCA) CYCLE AND RESPIRATORY ELECTRON TRANSPORT\",\n        # Pentose phosphate pathway\n        \"NFE2L2 REGULATES PENTOSE PHOSPHATE PATHWAY GENES\",\n        \"PENTOSE PHOSPHATE PATHWAY\",\n        \"PENTOSE PHOSPHATE PATHWAY DISEASE\",\n        # Fatty Acid Metabolism\n        \"FATTY ACID METABOLISM\",\n        # Glutamine metabolism\n        \"GLUTAMATE AND GLUTAMINE METABOLISM\",\n        # EGFR\n        \"SIGNALING BY EGFR\",\n        \"SIGNALING BY EGFR IN CANCER\",\n        \"EGFR DOWNREGULATION\",\n        # TGF-\u03b2 signaling\n        \"SIGNALING BY TGF-BETA RECEPTOR COMPLEX\",\n        \"TGF-BETA RECEPTOR SIGNALING IN EMT (EPITHELIAL TO MESENCHYMAL TRANSITION)\",\n        \"SIGNALING BY TGF-BETA RECEPTOR COMPLEX IN CANCER\",\n        \"SIGNALING BY TGFB FAMILY MEMBERS\",\n        \"TGF-BETA RECEPTOR SIGNALING ACTIVATES SMADS\",\n        # Wnt/\u03b2-catenin pathway\n        \"BETA-CATENIN INDEPENDENT WNT SIGNALING\",\n        \"SIGNALING BY WNT\",\n        # SLIT-2-ROBO1 pathways\n        \"REGULATION OF EXPRESSION OF SLITS AND ROBOS\",\n        # DNA repair\n        \"DNA REPAIR\",\n        # Energy homeostasis\n        \"ION HOMEOSTASIS\",\n        # Apoptosis\n        \"APOPTOSIS\",\n        # Angiogenesis\n        \"SIGNALING BY VEGF\",\n    ]\n\n    # Step 1: Ensure numeric types for NES and FDR q-val\n    enrichment_df = enrichment_df.copy()\n    enrichment_df[\"NES\"] = pd.to_numeric(enrichment_df[\"NES\"], errors=\"coerce\")\n    enrichment_df[\"FDR q-val\"] = pd.to_numeric(\n        enrichment_df[\"FDR q-val\"], errors=\"coerce\"\n    )\n    enrichment_df[\"Idx\"] = pd.to_numeric(enrichment_df[\"Idx\"], errors=\"coerce\")\n\n    # Log any rows that had non-numeric values\n    invalid_nes = enrichment_df[\"NES\"].isna().sum()\n    invalid_fdr = enrichment_df[\"FDR q-val\"].isna().sum()\n    if invalid_nes &gt; 0:\n        logger.warning(\n            f\"Found {invalid_nes} rows with non-numeric NES values (converted to NaN)\"\n        )\n    if invalid_fdr &gt; 0:\n        logger.warning(\n            f\"Found {invalid_fdr} rows with non-numeric FDR q-val values (converted to NaN)\"\n        )\n\n    # Step 2: Filter by FDR threshold\n    significant = enrichment_df[enrichment_df[\"FDR q-val\"] &lt; fdr_threshold].copy()\n\n    logger.info(\n        f\"Found {significant.shape[0]} significant pathway enrichments (FDR &lt; {fdr_threshold})\"\n    )\n\n    if significant.empty:\n        logger.warning(\"No significant pathways found. Cannot generate heatmap.\")\n        # Return empty results\n        empty_df = pd.DataFrame()\n        empty_dict = {}\n        return empty_df, empty_dict\n\n    # Step 3: Group by Timepoint (Idx) and Pathway (NAME), sum NES across all trajectories\n    pathway_summary = significant.groupby([\"Idx\", \"NAME\"])[\"NES\"].sum().reset_index()\n\n    logger.info(\n        f\"Aggregated results for {pathway_summary['Idx'].nunique()} timepoints \"\n        f\"and {pathway_summary['NAME'].nunique()} pathways\"\n    )\n\n    # Step 4: Pivot to create matrix (pathways \u00d7 timepoints)\n    heatmap_data = pathway_summary.pivot(\n        index=\"NAME\", columns=\"Idx\", values=\"NES\"\n    ).fillna(0)  # Fill missing with 0\n\n    logger.info(\n        f\"Full heatmap dimensions: {heatmap_data.shape[0]} pathways \u00d7 {heatmap_data.shape[1]} timepoints\"\n    )\n\n    # Save full summary data\n    summary_file = output_dir / \"pathway_nes_summary.csv\"\n    heatmap_data.to_csv(summary_file)\n    logger.info(f\"Saved pathway NES summary to: {summary_file}\")\n\n    # Dictionary to store all figures\n    figures = {}\n\n    # Helper function to create and save a heatmap\n    def plot_heatmap_regulation(\n        df_plot,\n        unique_pathways,\n        cmap_here=\"viridis\",\n        save_name=None,\n        colorbar_title=\"Sum of NES\",\n    ):\n        \"\"\"Plot heatmap following paper_figures.ipynb style\"\"\"\n        # Generate a range of locations for the ticks\n        tick_locations = range(len(unique_pathways))\n        z_min, z_max = df_plot.min().min(), df_plot.max().max()\n\n        # Make the range symmetric around 0\n        if z_min &lt; 0 and z_max &gt; 0:\n            abs_max = max(abs(z_min), abs(z_max))\n            z_min, z_max = -abs_max, abs_max\n            norm = mcolors.TwoSlopeNorm(vmin=z_min, vcenter=0, vmax=z_max)\n        else:\n            # If all values are positive or all negative, use regular normalization\n            norm = mcolors.Normalize(vmin=z_min, vmax=z_max)\n            logger.warning(\n                f\"Cannot center colormap at zero: range [{z_min:.3f}, {z_max:.3f}] does not cross zero\"\n            )\n\n        fig, ax = plt.subplots(figsize=(30, 10))\n\n        # Make heatmap\n        cax = ax.imshow(df_plot.values, cmap=cmap_here, norm=norm, aspect=\"auto\")\n\n        # Set the y-ticks\n        plt.yticks(tick_locations, unique_pathways, fontsize=yticks_fontsize)\n\n        # Set the x-ticks at specific positions\n        num_timepoints = df_plot.shape[1]\n        ax.set_xticks([0, num_timepoints - 1])\n        ax.set_xticklabels(\n            [\"early\", \"late\"], fontsize=yticks_fontsize * 1.33, rotation=45\n        )\n        ax.set_xlabel(\"Pseudo-Time\", fontsize=yticks_fontsize * 1.33)\n\n        # Get x and y axis range\n        ymin, ymax = ax.get_ylim()\n        xmin, xmax = ax.get_xlim()\n\n        # Custom x and y ticks\n        x_custom = np.arange(xmin, xmax, step=1)\n        y_custom = np.arange(ymax, ymin, step=1)\n\n        # set minor ticks at custom locations:\n        ax.set_xticks(x_custom, minor=True)\n        ax.set_yticks(y_custom, minor=True)\n\n        # Add grid lines at both major and minor ticks\n        plt.grid(False, which=\"major\")\n        plt.grid(True, which=\"minor\", color=\"black\", linestyle=\"-\", linewidth=1)\n\n        # Remove ticks\n        ax.tick_params(axis=\"both\", which=\"minor\", length=0)\n\n        # Add colorbar\n        if colorbar:\n            cbar = plt.colorbar(cax, shrink=0.7)\n            cbar.set_label(colorbar_title, rotation=270, labelpad=20, fontsize=16)\n\n        # Add legend\n        if legend:\n            colors = np.append(\n                plt.get_cmap(cmap_here)([0, 0.5, 1]), np.array([[1, 1, 1, 1]]), axis=0\n            )\n            labels = [\"Downregulated\", \"No change\", \"Upregulated\", \"No data\"]\n            patches = [\n                mpatches.Patch(facecolor=colors[i], label=labels[i], edgecolor=\"black\")\n                for i in range(len(labels))\n            ]\n            ax.legend(\n                handles=patches,\n                bbox_to_anchor=(1.05, 1),\n                loc=2,\n                borderaxespad=0.0,\n                title=\"Regulation\",\n                fontsize=yticks_fontsize * 2,\n                title_fontsize=24,\n            )\n\n        # Save figures\n        if save_name:\n            plt.savefig(output_dir / f\"{save_name}.pdf\", bbox_inches=\"tight\")\n            plt.savefig(output_dir / f\"{save_name}.png\", bbox_inches=\"tight\", dpi=600)\n            plt.savefig(\n                output_dir / f\"{save_name}.svg\",\n                bbox_inches=\"tight\",\n                format=\"svg\",\n                transparent=True,\n            )\n            logger.info(f\"Saved heatmap to: {output_dir / save_name}.{{pdf,png,svg}}\")\n\n        if show:\n            plt.show()\n        else:\n            plt.close()\n\n        return fig\n\n    # 1. Top 50 most changing pathways (first vs last timepoint)\n    logger.info(\"Creating heatmap 1/5: Top 50 most changing pathways...\")\n    first_col = heatmap_data.columns[0]\n    last_col = heatmap_data.columns[-1]\n    change = (heatmap_data[last_col] - heatmap_data[first_col]).abs()\n    top_changing = change.nlargest(50).index.tolist()\n\n    df_top_changing = heatmap_data.loc[top_changing]\n    fig1 = plot_heatmap_regulation(\n        df_top_changing,\n        top_changing,\n        cmap_here=\"RdBu_r\",\n        save_name=\"top50_most_changing_pathways\",\n        colorbar_title=\"Sum of NES\",\n    )\n    figures[\"top50_changing\"] = fig1\n\n    # 2. Top 50 most upregulated pathways (average NES &gt; 0)\n    logger.info(\"Creating heatmap 2/5: Top 50 most upregulated pathways...\")\n    avg_nes = heatmap_data.mean(axis=1)\n    upregulated = avg_nes[avg_nes &gt; 0].nlargest(50).index.tolist()\n\n    df_upregulated = heatmap_data.loc[upregulated]\n    fig2 = plot_heatmap_regulation(\n        df_upregulated,\n        upregulated,\n        cmap_here=\"YlGn\",\n        save_name=\"top50_most_upregulated_pathways\",\n        colorbar_title=\"Sum of NES\",\n    )\n    figures[\"top50_upregulated\"] = fig2\n\n    # 3. Top 50 most downregulated pathways (average NES &lt; 0)\n    logger.info(\"Creating heatmap 3/5: Top 50 most downregulated pathways...\")\n    downregulated = avg_nes[avg_nes &lt; 0].nsmallest(50).index.tolist()\n\n    df_downregulated = heatmap_data.loc[downregulated]\n    fig3 = plot_heatmap_regulation(\n        df_downregulated,\n        downregulated,\n        cmap_here=\"YlOrBr\",\n        save_name=\"top50_most_downregulated_pathways\",\n        colorbar_title=\"Sum of NES\",\n    )\n    figures[\"top50_downregulated\"] = fig3\n\n    # 4. High-level pathways (29 pathways from Reactome highest level)\n    logger.info(\"Creating heatmap 4/5: High-level pathways...\")\n    available_highest = [p for p in highest_pathways if p in heatmap_data.index]\n\n    if available_highest:\n        df_highest = heatmap_data.loc[available_highest]\n        fig4 = plot_heatmap_regulation(\n            df_highest,\n            available_highest,\n            cmap_here=\"RdBu_r\",\n            save_name=\"selected_pathways_highest_level\",\n            colorbar_title=\"Sum of NES\",\n        )\n        figures[\"selected_highest_level\"] = fig4\n        logger.info(\n            f\"Found {len(available_highest)}/{len(highest_pathways)} high-level pathways in data\"\n        )\n    else:\n        logger.warning(\"No high-level pathways found in the data\")\n\n    # 5. Literature pathways (33 pathways from literature review)\n    logger.info(\"Creating heatmap 5/5: Literature pathways...\")\n    available_literature = [p for p in pathways_literature if p in heatmap_data.index]\n\n    if available_literature:\n        df_literature = heatmap_data.loc[available_literature]\n        fig5 = plot_heatmap_regulation(\n            df_literature,\n            available_literature,\n            cmap_here=\"RdBu_r\",\n            save_name=\"selected_pathways_literature\",\n            colorbar_title=\"Sum of NES\",\n        )\n        figures[\"selected_literature\"] = fig5\n        logger.info(\n            f\"Found {len(available_literature)}/{len(pathways_literature)} literature pathways in data\"\n        )\n    else:\n        logger.warning(\"No literature pathways found in the data\")\n\n    logger.info(\n        f\"Pathway heatmap generation complete. Created {len(figures)} heatmaps.\"\n    )\n\n    return heatmap_data, figures\n</code></pre>"},{"location":"api/enrichment/#gsea-parameters","title":"GSEA Parameters","text":"<p>All enrichment functions support customizable GSEA parameters. See the Tutorial Step 6: Enrichment Tutorial for detailed information.</p>"},{"location":"api/enrichment/#common-parameters","title":"Common Parameters","text":"Parameter Default Description <code>pathway_file</code> <code>'data/external/ReactomePathways.gmt'</code> Path to gene set database <code>mode</code> <code>'Max_probe'</code> Collapse mode for multiple probes <code>norm</code> <code>'meandiv'</code> Normalization method <code>nperm</code> <code>1000</code> Number of permutations <code>rnd_seed</code> <code>'timestamp'</code> Random seed <code>scoring_scheme</code> <code>'weighted'</code> Scoring scheme <code>set_max</code> <code>500</code> Maximum gene set size <code>set_min</code> <code>15</code> Minimum gene set size"},{"location":"api/enrichment/#see-also","title":"See Also","text":"<ul> <li>GSEA Installation Guide</li> <li>Tutorial Step 6: Enrichment Tutorial</li> <li>Dynamic Enrichment Analysis Overview</li> </ul>"},{"location":"api/features/","title":"Features API","text":"<p>The <code>features</code> module provides functions for feature engineering and quality control of gene expression data.</p>"},{"location":"api/features/#overview","title":"Overview","text":"<p>This module includes:</p> <ul> <li>Low expression gene filtering</li> <li>Mahalanobis distance outlier detection</li> <li>Gene clustering with tsfresh</li> <li>Feature extraction for trajectory analysis</li> </ul>"},{"location":"api/features/#core-functions","title":"Core Functions","text":""},{"location":"api/features/#filter_low_expression","title":"filter_low_expression","text":"<p>Filter genes with low or invariant expression across samples.</p> <p>Example Usage:</p> <pre><code>import pandas as pd\nfrom renalprog.features import filter_low_expression\n\n# Load raw expression data (genes \u00d7 samples)\nrnaseq = pd.read_csv(\"data/raw/KIRC_rnaseq.tsv\", sep=\"\\t\", index_col=0)\nprint(f\"Original: {rnaseq.shape[0]} genes\")\n\n# Filter low expression genes\nfiltered = filter_low_expression(\n    rnaseq,\n    mean_threshold=0.5,      # Minimum mean expression\n    var_threshold=0.5,       # Minimum variance\n    min_sample_fraction=0.2  # Maximum fraction of zero values\n)\nprint(f\"Filtered: {filtered.shape[0]} genes\")\n</code></pre> <p>Filtering Criteria:</p> <ol> <li>Zero expression threshold: Remove genes with &gt;20% samples at zero</li> <li>Mean expression threshold: Keep genes with mean \u2265 0.5</li> <li>Variance threshold: Keep genes with variance \u2265 0.5</li> </ol>"},{"location":"api/features/#renalprog.features.filter_low_expression","title":"filter_low_expression","text":"<pre><code>filter_low_expression(\n    data: DataFrame,\n    mean_threshold: float = 0.5,\n    var_threshold: float = 0.5,\n    min_sample_fraction: float = 0.2,\n) -&gt; pd.DataFrame\n</code></pre> <p>Filter out genes with low expression across samples.</p> <p>Args:     data: DataFrame with genes as rows and samples as columns     mean_threshold: Minimum expression value to consider gene as expressed     var_threshold: Minimum variance value to consider gene as expressed     min_sample_fraction: Minimum fraction of non-expressed samples to filter gene</p> <p>Returns:     Filtered DataFrame with only genes meeting expression criteria</p> Source code in <code>renalprog/features.py</code> <pre><code>def filter_low_expression(\n    data: pd.DataFrame,\n    mean_threshold: float = 0.5,\n    var_threshold: float = 0.5,\n    min_sample_fraction: float = 0.2\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out genes with low expression across samples.\n\n    Args:\n        data: DataFrame with genes as rows and samples as columns\n        mean_threshold: Minimum expression value to consider gene as expressed\n        var_threshold: Minimum variance value to consider gene as expressed\n        min_sample_fraction: Minimum fraction of non-expressed samples to filter gene\n\n    Returns:\n        Filtered DataFrame with only genes meeting expression criteria\n    \"\"\"\n    logger.info(f\"Starting with {data.shape[0]} genes\")\n    logger.info(f\"Initial data shape: {data.shape}\")\n    # Remove lowly expressed genes\n    filtered_data = data[(data == 0).sum(axis=1) / data.shape[1] &lt;= min_sample_fraction]\n\n    filtered_data= filtered_data.iloc[\n        (np.mean(filtered_data, axis=1).values &gt;= mean_threshold) &amp;\n        (np.var(filtered_data, axis=1).values &gt;= var_threshold)]\n\n    n_removed = data.shape[0] - filtered_data.shape[0]\n    logger.info(f\"Removed {n_removed} lowly expressed genes\")\n    logger.info(f\"Retained {filtered_data.shape[0]} genes\")\n\n    return filtered_data\n</code></pre>"},{"location":"api/features/#detect_outliers_mahalanobis","title":"detect_outliers_mahalanobis","text":"<p>Detect and remove outlier samples using robust Mahalanobis distance.</p> <p>Example Usage:</p> <pre><code>import pandas as pd\nfrom renalprog.features import detect_outliers_mahalanobis\n\n# Load gene expression data (genes \u00d7 samples)\nrnaseq = pd.read_csv(\"data/interim/filtered_expression.csv\", index_col=0)\n\n# Detect outliers\ncleaned_data, outlier_ids, mahal_distances = detect_outliers_mahalanobis(\n    rnaseq,\n    alpha=0.05,           # Significance level\n    support_fraction=None,  # Auto-determine robust subset\n    transpose=True,       # Transpose to samples \u00d7 genes\n    seed=42\n)\n\nprint(f\"Original samples: {rnaseq.shape[1]}\")\nprint(f\"Outliers detected: {len(outlier_ids)}\")\nprint(f\"Clean samples: {cleaned_data.shape[1]}\")\nprint(f\"Outlier IDs: {outlier_ids}\")\n</code></pre> <p>How It Works:</p> <ol> <li>Minimum Covariance Determinant (MCD): Computes robust covariance estimate</li> <li>Mahalanobis Distance: Calculates distance of each sample from the robust center</li> <li>Chi-square Test: Identifies outliers exceeding chi-square threshold at significance level \u03b1</li> </ol> <p>Visualization:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import chi2\n\n# Plot Mahalanobis distances\nn_features = rnaseq.shape[0]\ncutoff = chi2.ppf(1 - 0.05, n_features)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.hist(mahal_distances, bins=50, edgecolor='black')\nplt.axvline(cutoff, color='red', linestyle='--', label=f'Cutoff (\u03b1=0.05)')\nplt.xlabel('Mahalanobis Distance')\nplt.ylabel('Count')\nplt.legend()\nplt.title('Distribution of Mahalanobis Distances')\n\nplt.subplot(1, 2, 2)\nplt.scatter(range(len(mahal_distances)), sorted(mahal_distances))\nplt.axhline(cutoff, color='red', linestyle='--')\nplt.xlabel('Sample Index (sorted)')\nplt.ylabel('Mahalanobis Distance')\nplt.title('Sorted Mahalanobis Distances')\nplt.tight_layout()\nplt.savefig('outlier_detection.png', dpi=300)\n</code></pre>"},{"location":"api/features/#renalprog.features.detect_outliers_mahalanobis","title":"detect_outliers_mahalanobis","text":"<pre><code>detect_outliers_mahalanobis(\n    data: DataFrame,\n    alpha: float = 0.05,\n    support_fraction: float = None,\n    transpose: bool = False,\n    seed: int = 2023,\n) -&gt; Tuple[pd.DataFrame, List[str], np.ndarray]\n</code></pre> <p>Detect and remove outlier samples using Mahalanobis distance.</p> <p>Uses Minimum Covariance Determinant (MCD) for robust covariance estimation, then identifies outliers based on Mahalanobis distance and chi-square distribution.</p> <p>Args:     data: DataFrame with samples as columns and genes as rows (will be transposed)     alpha: Significance level for chi-square test (default: 0.05)     support_fraction: Fraction of samples to use in MCD estimation (default: 0.75)     transpose: Whether to transpose data before processing (default: True)     seed: Random seed for reproducibility (default: 2023)</p> <p>Returns:     Tuple of:     - cleaned_data: DataFrame with outliers removed     - outlier_ids: List of outlier sample IDs     - mahalanobis_distances: Array of Mahalanobis distances for all samples</p> Source code in <code>renalprog/features.py</code> <pre><code>def detect_outliers_mahalanobis(\n    data: pd.DataFrame,\n    alpha: float = 0.05,\n    support_fraction: float = None,\n    transpose: bool = False,\n    seed: int = 2023\n) -&gt; Tuple[pd.DataFrame, List[str], np.ndarray]:\n    \"\"\"\n    Detect and remove outlier samples using Mahalanobis distance.\n\n    Uses Minimum Covariance Determinant (MCD) for robust covariance estimation,\n    then identifies outliers based on Mahalanobis distance and chi-square distribution.\n\n    Args:\n        data: DataFrame with samples as columns and genes as rows (will be transposed)\n        alpha: Significance level for chi-square test (default: 0.05)\n        support_fraction: Fraction of samples to use in MCD estimation (default: 0.75)\n        transpose: Whether to transpose data before processing (default: True)\n        seed: Random seed for reproducibility (default: 2023)\n\n    Returns:\n        Tuple of:\n        - cleaned_data: DataFrame with outliers removed\n        - outlier_ids: List of outlier sample IDs\n        - mahalanobis_distances: Array of Mahalanobis distances for all samples\n    \"\"\"\n    logger.info(f\"Detecting outliers with Mahalanobis distance (alpha={alpha})\")\n\n    # Transpose if needed (we need samples as rows)\n    if transpose:\n        data_for_mcd = data.T\n    else:\n        data_for_mcd = data.copy()\n    logger.info(f\"Data shape for MCD: {data_for_mcd.shape} (features x samples)\")\n    # Define chi-square cutoff\n    n_features = data_for_mcd.shape[0] # this is number of features (genes)\n    cutoff = chi2.ppf(1 - alpha, n_features - 1)\n    logger.info(f\"Chi-square cutoff (df={n_features}, alpha={alpha}): {cutoff:.2f}\")\n\n    # Minimum Covariance Determinant for robust covariance estimation\n    logger.info(\"Fitting MinCovDet estimator...\")\n    mcd = MinCovDet(support_fraction=support_fraction,random_state = seed)\n    mcd.fit(data_for_mcd) # fit with (n_samples, n_features)\n\n    # Calculate Mahalanobis distances\n    mahalanobis_distances = mcd.dist_\n\n    # Identify outliers\n    outlier_mask = mahalanobis_distances &gt; cutoff\n    outlier_indices = np.where(outlier_mask)[0]\n    outlier_ids = data_for_mcd.index[outlier_indices].tolist()\n\n    logger.info(f\"Detected {len(outlier_ids)} outlier samples\")\n    logger.info(f\"Outlier IDs: {outlier_ids[:10]}{'...' if len(outlier_ids) &gt; 10 else ''}\")\n\n    # Remove outliers\n    if transpose:\n        # Remove columns (samples) from original data\n        cleaned_data = data.drop(columns=outlier_ids)\n    else:\n        # Remove rows from original data\n        cleaned_data = data.drop(index=outlier_ids)\n\n    logger.info(f\"Cleaned data shape: {cleaned_data.shape}\")\n\n    return cleaned_data, outlier_ids, mahalanobis_distances\n</code></pre>"},{"location":"api/features/#quality-control-workflow","title":"Quality Control Workflow","text":"<p>Complete QC pipeline for gene expression data:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nfrom renalprog.features import filter_low_expression, detect_outliers_mahalanobis\nfrom renalprog.config import PreprocessingConfig\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef qc_pipeline(rnaseq_path: Path, output_dir: Path):\n    \"\"\"Run complete quality control pipeline.\"\"\"\n\n    # Load configuration\n    config = PreprocessingConfig()\n\n    # Load data\n    logger.info(\"Loading RNA-seq data...\")\n    rnaseq = pd.read_csv(rnaseq_path, sep=\"\\t\", index_col=0)\n    logger.info(f\"Initial shape: {rnaseq.shape}\")\n\n    # Filter low expression genes\n    logger.info(\"Filtering low expression genes...\")\n    rnaseq_filtered = filter_low_expression(\n        rnaseq,\n        mean_threshold=config.mean_threshold,\n        var_threshold=config.var_threshold,\n        min_sample_fraction=config.min_sample_fraction\n    )\n    logger.info(f\"After filtering: {rnaseq_filtered.shape}\")\n\n    # Detect and remove outliers\n    logger.info(\"Detecting outliers...\")\n    rnaseq_clean, outliers, distances = detect_outliers_mahalanobis(\n        rnaseq_filtered,\n        alpha=config.outlier_alpha,\n        seed=config.random_state\n    )\n    logger.info(f\"Outliers removed: {len(outliers)}\")\n    logger.info(f\"Final shape: {rnaseq_clean.shape}\")\n\n    # Save results\n    output_dir.mkdir(parents=True, exist_ok=True)\n    rnaseq_clean.to_csv(output_dir / \"expression_qc.csv\")\n\n    # Save QC report\n    qc_report = {\n        'initial_genes': rnaseq.shape[0],\n        'initial_samples': rnaseq.shape[1],\n        'filtered_genes': rnaseq_filtered.shape[0],\n        'genes_removed': rnaseq.shape[0] - rnaseq_filtered.shape[0],\n        'outlier_samples': len(outliers),\n        'outlier_ids': outliers,\n        'final_genes': rnaseq_clean.shape[0],\n        'final_samples': rnaseq_clean.shape[1]\n    }\n\n    pd.DataFrame([qc_report]).to_csv(output_dir / \"qc_report.csv\", index=False)\n\n    return rnaseq_clean, qc_report\n\n# Run pipeline\nif __name__ == \"__main__\":\n    rnaseq_clean, report = qc_pipeline(\n        rnaseq_path=Path(\"data/raw/KIRC_rnaseq.tsv\"),\n        output_dir=Path(\"data/interim/qc_results\")\n    )\n</code></pre>"},{"location":"api/features/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/features/#custom-filtering-criteria","title":"Custom Filtering Criteria","text":"<p>Define custom filtering thresholds for different datasets:</p> <pre><code>from renalprog.features import filter_low_expression\n\n# Strict filtering for high-quality datasets\nstrict_filtered = filter_low_expression(\n    rnaseq,\n    mean_threshold=1.0,\n    var_threshold=1.0,\n    min_sample_fraction=0.1\n)\n\n# Lenient filtering for smaller datasets\nlenient_filtered = filter_low_expression(\n    rnaseq,\n    mean_threshold=0.1,\n    var_threshold=0.1,\n    min_sample_fraction=0.3\n)\n</code></pre>"},{"location":"api/features/#outlier-detection-with-custom-parameters","title":"Outlier Detection with Custom Parameters","text":"<p>Adjust sensitivity of outlier detection:</p> <pre><code>from renalprog.features import detect_outliers_mahalanobis\n\n# Conservative (fewer outliers)\nconservative, outliers_con, _ = detect_outliers_mahalanobis(\n    rnaseq, alpha=0.01, support_fraction=0.8\n)\n\n# Liberal (more outliers)\nliberal, outliers_lib, _ = detect_outliers_mahalanobis(\n    rnaseq, alpha=0.10, support_fraction=0.6\n)\n\nprint(f\"Conservative: {len(outliers_con)} outliers\")\nprint(f\"Liberal: {len(outliers_lib)} outliers\")\n</code></pre>"},{"location":"api/features/#see-also","title":"See Also","text":"<ul> <li>Dataset API - Data loading and preparation</li> <li>Configuration API - Preprocessing configuration</li> <li>Data Requirements Tutorial - Data preparation guide</li> </ul>"},{"location":"api/models/","title":"Models API","text":"<p>The <code>modeling</code> module provides neural network architectures and training functions for variational autoencoders (VAEs).</p>"},{"location":"api/models/#overview","title":"Overview","text":"<p>This module includes:</p> <ul> <li>VAE architectures (standard, conditional, simple)</li> <li>Training and evaluation functions</li> <li>Loss functions (reconstruction, KL divergence)</li> <li>Checkpoint management</li> <li>Post-processing networks</li> </ul>"},{"location":"api/models/#model-architectures","title":"Model Architectures","text":""},{"location":"api/models/#vae","title":"VAE","text":"<p>Standard Variational Autoencoder with encoder-decoder architecture.</p> <p>Example Usage:</p> <pre><code>import torch\nfrom renalprog.modeling.train import VAE\n\n# Create VAE model\nmodel = VAE(\n    input_dim=20000,  # Number of genes\n    mid_dim=1024,     # Hidden layer size\n    features=128,     # Latent dimension\n    dropout=0.1\n)\n\n# Forward pass\nx = torch.randn(32, 20000)  # Batch of gene expression\nreconstruction, mu, log_var, z = model(x)\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.VAE","title":"VAE","text":"<pre><code>VAE(input_dim: int, mid_dim: int, features: int, output_layer=nn.ReLU)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Variational Autoencoder (VAE).</p> <p>Standard VAE implementation with encoder-decoder architecture and reparameterization trick for sampling from the latent space.</p> <p>Args:     input_dim: Dimension of input data (number of genes)     mid_dim: Dimension of hidden layer     features: Dimension of latent space     output_layer: Output activation function (default: nn.ReLU)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(\n    self, input_dim: int, mid_dim: int, features: int, output_layer=nn.ReLU\n):\n    super().__init__()\n    self.input_dim = input_dim\n    self.mid_dim = mid_dim\n    self.features = features\n    self.output_layer = output_layer\n\n    # Encoder: input -&gt; mid_dim -&gt; (mu, logvar)\n    self.encoder = nn.Sequential(\n        nn.Linear(in_features=input_dim, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=features * 2),\n    )\n\n    # Decoder: latent -&gt; mid_dim -&gt; reconstruction\n    self.decoder = nn.Sequential(\n        nn.Linear(in_features=features, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=input_dim),\n        output_layer(),\n    )\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.VAE-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.VAE.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Forward pass through VAE.</p> <p>Args:     x: Input data (batch_size, input_dim)</p> <p>Returns:     Tuple of (reconstruction, mu, log_var, z)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(\n    self, x: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through VAE.\n\n    Args:\n        x: Input data (batch_size, input_dim)\n\n    Returns:\n        Tuple of (reconstruction, mu, log_var, z)\n    \"\"\"\n    # Encode\n    encoded = self.encoder(x)\n    mu_logvar = encoded.view(-1, 2, self.features)\n    mu = mu_logvar[:, 0, :]\n    log_var = mu_logvar[:, 1, :]\n\n    # Sample from latent space\n    z = self.reparametrize(mu, log_var)\n\n    # Decode\n    reconstruction = self.decoder(z)\n\n    return reconstruction, mu, log_var, z\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.VAE.reparametrize","title":"reparametrize","text":"<pre><code>reparametrize(mu: Tensor, log_var: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Reparameterization trick: sample from N(mu, var) using N(0,1).</p> <p>Args:     mu: Mean of the latent distribution     log_var: Log variance of the latent distribution</p> <p>Returns:     Sampled latent vector</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def reparametrize(self, mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Reparameterization trick: sample from N(mu, var) using N(0,1).\n\n    Args:\n        mu: Mean of the latent distribution\n        log_var: Log variance of the latent distribution\n\n    Returns:\n        Sampled latent vector\n    \"\"\"\n    if self.training:\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    else:\n        # During evaluation, return mean directly\n        return mu\n</code></pre>"},{"location":"api/models/#cvae","title":"CVAE","text":"<p>Conditional VAE that incorporates clinical covariates.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.train import CVAE\n\n# Create conditional VAE\nmodel = ConditionalVAE(\n    input_dim=20000,\n    mid_dim=1024,\n    features=128,\n    condition_dim=2,  # e.g., one-hot encoded stage\n    dropout=0.1\n)\n\n# Forward pass with condition\nx = torch.randn(32, 20000)\ncondition = torch.randn(32, 2)  # Clinical covariates\nreconstruction, mu, log_var, z = model(x, condition)\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.CVAE","title":"CVAE","text":"<pre><code>CVAE(\n    input_dim: int,\n    mid_dim: int,\n    features: int,\n    num_classes: int,\n    output_layer=nn.ReLU,\n)\n</code></pre> <p>               Bases: <code>VAE</code></p> <p>Conditional Variational Autoencoder.</p> <p>VAE that conditions on additional information (e.g., clinical data).</p> <p>Args:     input_dim: Dimension of input data     mid_dim: Dimension of hidden layer     features: Dimension of latent space     num_classes: Number of condition classes     output_layer: Output activation function</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    mid_dim: int,\n    features: int,\n    num_classes: int,\n    output_layer=nn.ReLU,\n):\n    super().__init__(input_dim, mid_dim, features, output_layer)\n    self.num_classes = num_classes\n\n    # Modified encoder: accepts input + condition\n    self.encoder = nn.Sequential(\n        nn.Linear(in_features=input_dim + num_classes, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=features * 2),\n    )\n\n    # Modified decoder: accepts latent + condition\n    self.decoder = nn.Sequential(\n        nn.Linear(in_features=features + num_classes, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=input_dim),\n    )\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.CVAE-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.CVAE.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor, condition: Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Forward pass through CVAE.</p> <p>Args:     x: Input data     condition: Conditioning information (one-hot encoded)</p> <p>Returns:     Tuple of (reconstruction, mu, log_var, z)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, condition: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through CVAE.\n\n    Args:\n        x: Input data\n        condition: Conditioning information (one-hot encoded)\n\n    Returns:\n        Tuple of (reconstruction, mu, log_var, z)\n    \"\"\"\n    # Concatenate input with condition\n    x_cond = torch.cat([x, condition], dim=1)\n\n    # Encode\n    encoded = self.encoder(x_cond)\n    mu_logvar = encoded.view(-1, 2, self.features)\n    mu = mu_logvar[:, 0, :]\n    log_var = mu_logvar[:, 1, :]\n\n    # Sample\n    z = self.reparametrize(mu, log_var)\n\n    # Concatenate latent with condition\n    z_cond = torch.cat([z, condition], dim=1)\n\n    # Decode\n    reconstruction = self.decoder(z_cond)\n    reconstruction = self.output_layer()(reconstruction)\n\n    return reconstruction, mu, log_var, z\n</code></pre>"},{"location":"api/models/#ae","title":"AE","text":"<p>Simplified autoencoder without variational component.</p>"},{"location":"api/models/#renalprog.modeling.train.AE","title":"AE","text":"<pre><code>AE(input_dim: int, mid_dim: int, features: int, output_layer=nn.ReLU)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Standard Autoencoder (without variational inference).</p> <p>Similar architecture to VAE but without reparameterization trick.</p> <p>Args:     input_dim: Dimension of input data     mid_dim: Dimension of hidden layer     features: Dimension of latent space     output_layer: Output activation function</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(\n    self, input_dim: int, mid_dim: int, features: int, output_layer=nn.ReLU\n):\n    super().__init__()\n    self.input_dim = input_dim\n    self.mid_dim = mid_dim\n    self.features = features\n    self.output_layer = output_layer\n\n    self.encoder = nn.Sequential(\n        nn.Linear(in_features=input_dim, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=features),\n    )\n\n    self.decoder = nn.Sequential(\n        nn.Linear(in_features=features, out_features=mid_dim),\n        nn.ReLU(),\n        nn.Linear(in_features=mid_dim, out_features=input_dim),\n        output_layer(),\n    )\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.AE-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.AE.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tuple[torch.Tensor, None, None, torch.Tensor]\n</code></pre> <p>Forward pass through AE.</p> <p>Args:     x: Input data</p> <p>Returns:     Tuple of (reconstruction, None, None, z)     None values for mu and logvar to maintain consistency with VAE</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, None, None, torch.Tensor]:\n    \"\"\"Forward pass through AE.\n\n    Args:\n        x: Input data\n\n    Returns:\n        Tuple of (reconstruction, None, None, z)\n        None values for mu and logvar to maintain consistency with VAE\n    \"\"\"\n    z = self.encoder(x)\n    reconstruction = self.decoder(z)\n    return reconstruction, None, None, z\n</code></pre>"},{"location":"api/models/#loss-functions","title":"Loss Functions","text":""},{"location":"api/models/#vae_loss","title":"vae_loss","text":"<p>Complete VAE loss combining reconstruction and KL divergence.</p>"},{"location":"api/models/#renalprog.modeling.train.vae_loss","title":"vae_loss","text":"<pre><code>vae_loss(\n    reconstruction: Tensor,\n    x: Tensor,\n    mu: Tensor,\n    log_var: Tensor,\n    beta: float = 1.0,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Calculate VAE loss: reconstruction loss + KL divergence.</p> <p>Args:     reconstruction: Reconstructed output     x: Original input     mu: Mean of latent distribution     log_var: Log variance of latent distribution     beta: Weight for KL divergence term (beta-VAE)</p> <p>Returns:     Tuple of (total_loss, reconstruction_loss, kl_divergence)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def vae_loss(\n    reconstruction: torch.Tensor,\n    x: torch.Tensor,\n    mu: torch.Tensor,\n    log_var: torch.Tensor,\n    beta: float = 1.0,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Calculate VAE loss: reconstruction loss + KL divergence.\n\n    Args:\n        reconstruction: Reconstructed output\n        x: Original input\n        mu: Mean of latent distribution\n        log_var: Log variance of latent distribution\n        beta: Weight for KL divergence term (beta-VAE)\n\n    Returns:\n        Tuple of (total_loss, reconstruction_loss, kl_divergence)\n    \"\"\"\n    # Reconstruction loss (MSE)\n    recon_loss = nn.functional.mse_loss(reconstruction, x, reduction=\"sum\")\n\n    # KL divergence: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n    # Total loss\n    total_loss = recon_loss + beta * kl_div\n\n    return total_loss, recon_loss, kl_div\n</code></pre>"},{"location":"api/models/#reconstruction_loss","title":"reconstruction_loss","text":"<p>MSE-based reconstruction loss.</p>"},{"location":"api/models/#renalprog.modeling.train.reconstruction_loss","title":"reconstruction_loss","text":"<pre><code>reconstruction_loss(\n    reconstruction: Tensor, x: Tensor, reduction: str = \"sum\"\n) -&gt; torch.Tensor\n</code></pre> <p>Calculate reconstruction loss (MSE).</p> <p>Args:     reconstruction: Reconstructed output     x: Original input     reduction: Reduction method ('sum' or 'mean')</p> <p>Returns:     Reconstruction loss</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def reconstruction_loss(\n    reconstruction: torch.Tensor, x: torch.Tensor, reduction: str = \"sum\"\n) -&gt; torch.Tensor:\n    \"\"\"Calculate reconstruction loss (MSE).\n\n    Args:\n        reconstruction: Reconstructed output\n        x: Original input\n        reduction: Reduction method ('sum' or 'mean')\n\n    Returns:\n        Reconstruction loss\n    \"\"\"\n    return nn.functional.mse_loss(reconstruction, x, reduction=reduction)\n</code></pre>"},{"location":"api/models/#kl_divergence","title":"kl_divergence","text":"<p>KL divergence between latent distribution and prior.</p>"},{"location":"api/models/#renalprog.modeling.train.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(mu: Tensor, log_var: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Calculate KL divergence between approximate posterior and prior.</p> <p>Args:     mu: Mean of approximate posterior     log_var: Log variance of approximate posterior</p> <p>Returns:     KL divergence</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def kl_divergence(mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Calculate KL divergence between approximate posterior and prior.\n\n    Args:\n        mu: Mean of approximate posterior\n        log_var: Log variance of approximate posterior\n\n    Returns:\n        KL divergence\n    \"\"\"\n    return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n</code></pre>"},{"location":"api/models/#training-functions","title":"Training Functions","text":""},{"location":"api/models/#train_vae","title":"train_vae","text":"<p>Main training function for VAE models.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.train import train_vae\nfrom pathlib import Path\nimport pandas as pd\n\n# Load training data\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\n\n# Train VAE\nhistory, best_model, checkpoints = train_vae(\n    train_data=train_expr.values,\n    val_data=test_expr.values,\n    input_dim=train_expr.shape[1],\n    mid_dim=1024,\n    features=128,\n    output_dir=Path(\"models/my_vae\"),\n    n_epochs=100,\n    batch_size=32,\n    learning_rate=1e-3,\n    use_scheduler=True,\n    use_checkpoint=True,\n    early_stopping_patience=20\n)\n\nprint(f\"Final validation loss: {history['val_loss'][-1]:.4f}\")\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.train_vae","title":"train_vae","text":"<pre><code>train_vae(\n    X_train: ndarray,\n    X_test: ndarray,\n    y_train: Optional[ndarray] = None,\n    y_test: Optional[ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]\n</code></pre> <p>Train a VAE model with full checkpointing support.</p> <p>Args:     X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame     X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame     y_train: Optional training labels for CVAE     y_test: Optional test labels for CVAE     config: Training configuration     save_dir: Directory to save checkpoints     resume_from: Optional checkpoint path to resume training     force_cpu: Force CPU usage even if CUDA is available (for compatibility)</p> <p>Returns:     Tuple of (trained_model, training_history)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_vae(\n    X_train: np.ndarray,\n    X_test: np.ndarray,\n    y_train: Optional[np.ndarray] = None,\n    y_test: Optional[np.ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]:\n    \"\"\"Train a VAE model with full checkpointing support.\n\n    Args:\n        X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame\n        X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame\n        y_train: Optional training labels for CVAE\n        y_test: Optional test labels for CVAE\n        config: Training configuration\n        save_dir: Directory to save checkpoints\n        resume_from: Optional checkpoint path to resume training\n        force_cpu: Force CPU usage even if CUDA is available (for compatibility)\n\n    Returns:\n        Tuple of (trained_model, training_history)\n    \"\"\"\n    # Convert DataFrames to numpy arrays if needed\n    if hasattr(X_train, \"values\"):  # Check if it's a DataFrame\n        X_train = X_train.values\n    if hasattr(X_test, \"values\"):  # Check if it's a DataFrame\n        X_test = X_test.values\n    if y_train is not None and hasattr(y_train, \"values\"):\n        y_train = y_train.values\n    if y_test is not None and hasattr(y_test, \"values\"):\n        y_test = y_test.values\n\n    if config is None:\n        config = VAEConfig()\n        config.INPUT_DIM = X_train.shape[1]\n\n    set_seed(config.SEED)\n\n    # Setup save directory\n    if save_dir is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d\")\n        save_dir = Path(f\"models/{timestamp}_VAE_KIRC\")\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save config\n    save_model_config(config, save_dir / \"config.json\")\n\n    # Setup device\n    device = get_device(force_cpu=force_cpu)\n    logger.info(f\"Using device: {device}\")\n\n    # Initialize model\n    model = VAE(\n        input_dim=config.INPUT_DIM,\n        mid_dim=config.MID_DIM,\n        features=config.LATENT_DIM,\n    ).to(device)\n\n    logger.info(\n        f\"Model: VAE(input_dim={config.INPUT_DIM}, mid_dim={config.MID_DIM}, latent_dim={config.LATENT_DIM})\"\n    )\n    logger.info(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    # Setup optimizer\n    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n\n    # Setup checkpointer\n    checkpointer = ModelCheckpointer(\n        save_dir=save_dir,\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_freq=config.CHECKPOINT_FREQ,\n        keep_last_n=3,\n    )\n\n    # Resume from checkpoint if provided\n    start_epoch = 0\n    if resume_from is not None:\n        checkpoint_info = checkpointer.load_checkpoint(\n            resume_from, model, optimizer, device=str(device)\n        )\n        start_epoch = checkpoint_info[\"epoch\"] + 1\n        logger.info(f\"Resuming training from epoch {start_epoch}\")\n\n    # Create dataloaders\n    train_loader = create_dataloader(X_train, y_train, config.BATCH_SIZE, shuffle=True)\n    test_loader = create_dataloader(X_test, y_test, config.BATCH_SIZE, shuffle=False)\n\n    # Training history\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"train_recon_loss\": [],\n        \"train_kl_loss\": [],\n        \"val_recon_loss\": [],\n        \"val_kl_loss\": [],\n        \"beta_schedule\": [],  # Track beta values\n    }\n\n    # Setup beta annealing schedule\n    if config.USE_BETA_ANNEALING:\n        beta_schedule = frange_cycle_linear(\n            start=config.BETA_START,\n            stop=config.BETA,\n            n_epoch=config.EPOCHS,\n            n_cycle=config.BETA_CYCLES,\n            ratio=config.BETA_RATIO,\n        )\n        logger.info(\n            f\"Using cyclical beta annealing: \"\n            f\"{config.BETA_START} -&gt; {config.BETA} over {config.BETA_CYCLES} cycles\"\n        )\n    else:\n        # Constant beta\n        beta_schedule = np.ones(config.EPOCHS) * config.BETA\n        logger.info(f\"Using constant beta: {config.BETA}\")\n\n    # Training loop\n    logger.info(f\"Starting training for {config.EPOCHS} epochs\")\n\n    # Add epoch progress bar\n    epoch_pbar = tqdm(range(start_epoch, config.EPOCHS), desc=\"Epochs\", position=0)\n    for epoch in epoch_pbar:\n        # Get beta for this epoch from schedule\n        current_beta = beta_schedule[epoch]\n\n        # Train\n        train_metrics = train_epoch(\n            model, train_loader, optimizer, device, config, beta=current_beta\n        )\n\n        # Validate\n        val_metrics = evaluate_model(\n            model, test_loader, device, config, beta=current_beta\n        )\n\n        # Update history\n        history[\"train_loss\"].append(train_metrics[\"loss\"])\n        history[\"val_loss\"].append(val_metrics[\"loss\"])\n        history[\"train_recon_loss\"].append(train_metrics[\"recon_loss\"])\n        history[\"train_kl_loss\"].append(train_metrics[\"kl_loss\"])\n        history[\"val_recon_loss\"].append(val_metrics[\"recon_loss\"])\n        history[\"val_kl_loss\"].append(val_metrics[\"kl_loss\"])\n        history[\"beta_schedule\"].append(float(current_beta))\n\n        # Update epoch progress bar\n        epoch_pbar.set_postfix(\n            {\n                \"train_loss\": f\"{train_metrics['loss']:.4f}\",\n                \"val_loss\": f\"{val_metrics['loss']:.4f}\",\n                \"beta\": f\"{current_beta:.3f}\",\n            }\n        )\n\n        # Log progress\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            logger.info(\n                f\"Epoch {epoch + 1}/{config.EPOCHS} - \"\n                f\"train_loss: {train_metrics['loss']:.4f}, \"\n                f\"val_loss: {val_metrics['loss']:.4f}\"\n            )\n\n        # Combine metrics for checkpointing\n        current_metrics = {\n            \"train_loss\": train_metrics[\"loss\"],\n            \"val_loss\": val_metrics[\"loss\"],\n            \"train_recon\": train_metrics[\"recon_loss\"],\n            \"train_kl\": train_metrics[\"kl_loss\"],\n            \"val_recon\": val_metrics[\"recon_loss\"],\n            \"val_kl\": val_metrics[\"kl_loss\"],\n        }\n\n        # # Save periodic checkpoint\n        # if checkpointer.should_save_checkpoint(epoch):\n        #     checkpointer.save_checkpoint(\n        #         epoch, model, optimizer, current_metrics, config\n        #     )\n\n    # Save final model\n    checkpointer.save_checkpoint(\n        config.EPOCHS - 1, model, optimizer, current_metrics, config, is_final=True\n    )\n\n    logger.info(\"Training complete!\")\n\n    return model, history\n</code></pre>"},{"location":"api/models/#train_epoch","title":"train_epoch","text":"<p>Train the model for one epoch.</p>"},{"location":"api/models/#renalprog.modeling.train.train_epoch","title":"train_epoch","text":"<pre><code>train_epoch(\n    model: Module,\n    dataloader: DataLoader,\n    optimizer: Optimizer,\n    device: str,\n    config: VAEConfig,\n    beta: Optional[float] = None,\n) -&gt; Dict[str, float]\n</code></pre> <p>Train model for one epoch.</p> <p>Args:     model: VAE model     dataloader: Training DataLoader     optimizer: Optimizer     device: Device to use     config: Training configuration     beta: Beta value for this epoch (if None, uses config.BETA)</p> <p>Returns:     Dictionary with loss metrics</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_epoch(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: str,\n    config: VAEConfig,\n    beta: Optional[float] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"Train model for one epoch.\n\n    Args:\n        model: VAE model\n        dataloader: Training DataLoader\n        optimizer: Optimizer\n        device: Device to use\n        config: Training configuration\n        beta: Beta value for this epoch (if None, uses config.BETA)\n\n    Returns:\n        Dictionary with loss metrics\n    \"\"\"\n    if beta is None:\n        beta = config.BETA\n    model.train()\n    total_loss = 0.0\n    total_recon = 0.0\n    total_kl = 0.0\n\n    # Add progress bar\n    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n    for batch in pbar:\n        if len(batch) == 2:\n            data, _ = batch\n        else:\n            data = batch[0]\n\n        data = data.to(device)\n\n        # Forward pass\n        optimizer.zero_grad()\n        reconstruction, mu, log_var, z = model(data)\n\n        # Calculate loss (use beta parameter instead of config.BETA)\n        loss, recon, kl = vae_loss(reconstruction, data, mu, log_var, beta)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate losses\n        total_loss += loss.item()\n        total_recon += recon.item()\n        total_kl += kl.item()\n\n        # Update progress bar\n        pbar.set_postfix(\n            {\n                \"loss\": f\"{loss.item() / len(data):.4f}\",\n                \"recon\": f\"{recon.item() / len(data):.4f}\",\n                \"kl\": f\"{kl.item() / len(data):.4f}\",\n            }\n        )\n\n    # Average losses\n    n_samples = len(dataloader.dataset)\n    metrics = {\n        \"loss\": total_loss / n_samples,\n        \"recon_loss\": total_recon / n_samples,\n        \"kl_loss\": total_kl / n_samples,\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/models/#evaluate_model","title":"evaluate_model","text":"<p>Evaluate model on validation/test data.</p>"},{"location":"api/models/#renalprog.modeling.train.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: Module,\n    dataloader: DataLoader,\n    device: str,\n    config: VAEConfig,\n    beta: Optional[float] = None,\n) -&gt; Dict[str, float]\n</code></pre> <p>Evaluate model on validation/test set.</p> <p>Args:     model: VAE model     dataloader: Validation DataLoader     device: Device to use     config: Training configuration     beta: Beta value for this epoch (if None, uses config.BETA)</p> <p>Returns:     Dictionary with loss metrics</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def evaluate_model(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    device: str,\n    config: VAEConfig,\n    beta: Optional[float] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model on validation/test set.\n\n    Args:\n        model: VAE model\n        dataloader: Validation DataLoader\n        device: Device to use\n        config: Training configuration\n        beta: Beta value for this epoch (if None, uses config.BETA)\n\n    Returns:\n        Dictionary with loss metrics\n    \"\"\"\n    if beta is None:\n        beta = config.BETA\n    model.eval()\n    total_loss = 0.0\n    total_recon = 0.0\n    total_kl = 0.0\n\n    with torch.no_grad():\n        # Add progress bar\n        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n        for batch in pbar:\n            if len(batch) == 2:\n                data, _ = batch\n            else:\n                data = batch[0]\n\n            data = data.to(device)\n\n            # Forward pass\n            reconstruction, mu, log_var, z = model(data)\n\n            # Calculate loss (use beta parameter instead of config.BETA)\n            loss, recon, kl = vae_loss(reconstruction, data, mu, log_var, beta)\n\n            # Accumulate losses\n            total_loss += loss.item()\n            total_recon += recon.item()\n            total_kl += kl.item()\n\n            # Update progress bar\n            pbar.set_postfix(\n                {\n                    \"loss\": f\"{loss.item() / len(data):.4f}\",\n                    \"recon\": f\"{recon.item() / len(data):.4f}\",\n                    \"kl\": f\"{kl.item() / len(data):.4f}\",\n                }\n            )\n\n    # Average losses\n    n_samples = len(dataloader.dataset)\n    metrics = {\n        \"loss\": total_loss / n_samples,\n        \"recon_loss\": total_recon / n_samples,\n        \"kl_loss\": total_kl / n_samples,\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/models/#train_vae_with_postprocessing","title":"train_vae_with_postprocessing","text":"<p>Train VAE and post-processing network together.</p>"},{"location":"api/models/#renalprog.modeling.train.train_vae_with_postprocessing","title":"train_vae_with_postprocessing","text":"<pre><code>train_vae_with_postprocessing(\n    X_train: ndarray,\n    X_test: ndarray,\n    vae_config: Optional[VAEConfig] = None,\n    reconstruction_network_dims: Optional[List[int]] = None,\n    reconstruction_epochs: int = 200,\n    reconstruction_lr: float = 0.0001,\n    batch_size_reconstruction: int = 8,\n    save_dir: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, nn.Module, Dict[str, list], Dict[str, list]]\n</code></pre> <p>Train VAE followed by postprocessing network (full pipeline).</p> <p>This implements the complete training pipeline as in train_vae.sh: 1. Train VAE on gene expression data 2. Get VAE reconstructions 3. Train NetworkReconstruction to adjust VAE output</p> <p>Args:     X_train: Training data (numpy array or pandas DataFrame)     X_test: Test data (numpy array or pandas DataFrame)     vae_config: VAE configuration     reconstruction_network_dims: Architecture for reconstruction network         If None, defaults to [input_dim, 4096, 1024, 4096, input_dim]     reconstruction_epochs: Epochs for training reconstruction network     reconstruction_lr: Learning rate for reconstruction network     save_dir: Directory to save models     force_cpu: Force CPU usage</p> <p>Returns:     Tuple of (vae_model, reconstruction_network, vae_history, reconstruction_history)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_vae_with_postprocessing(\n    X_train: np.ndarray,\n    X_test: np.ndarray,\n    vae_config: Optional[VAEConfig] = None,\n    reconstruction_network_dims: Optional[List[int]] = None,\n    reconstruction_epochs: int = 200,\n    reconstruction_lr: float = 1e-4,\n    batch_size_reconstruction: int = 8,\n    save_dir: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, nn.Module, Dict[str, list], Dict[str, list]]:\n    \"\"\"\n    Train VAE followed by postprocessing network (full pipeline).\n\n    This implements the complete training pipeline as in train_vae.sh:\n    1. Train VAE on gene expression data\n    2. Get VAE reconstructions\n    3. Train NetworkReconstruction to adjust VAE output\n\n    Args:\n        X_train: Training data (numpy array or pandas DataFrame)\n        X_test: Test data (numpy array or pandas DataFrame)\n        vae_config: VAE configuration\n        reconstruction_network_dims: Architecture for reconstruction network\n            If None, defaults to [input_dim, 4096, 1024, 4096, input_dim]\n        reconstruction_epochs: Epochs for training reconstruction network\n        reconstruction_lr: Learning rate for reconstruction network\n        save_dir: Directory to save models\n        force_cpu: Force CPU usage\n\n    Returns:\n        Tuple of (vae_model, reconstruction_network, vae_history, reconstruction_history)\n    \"\"\"\n    logger.info(\"Starting full VAE + postprocessing pipeline\")\n\n    # Convert DataFrames to numpy arrays if needed\n    if hasattr(X_train, \"values\"):  # Check if it's a DataFrame\n        X_train = X_train.values\n    if hasattr(X_test, \"values\"):  # Check if it's a DataFrame\n        X_test = X_test.values\n\n    # Setup\n    if save_dir is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d\")\n        save_dir = Path(f\"models/{timestamp}_VAE_with_reconstruction\")\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Step 1: Train VAE\n    logger.info(\"Step 1: Training VAE\")\n    vae_model, vae_history = train_vae(\n        X_train,\n        X_test,\n        config=vae_config,\n        save_dir=save_dir / \"vae\",\n        force_cpu=force_cpu,\n    )\n\n    # Step 2: Get VAE reconstructions\n    logger.info(\"Step 2: Getting VAE reconstructions\")\n    device = get_device(force_cpu=force_cpu)\n    vae_model.eval()\n\n    # Normalize data before passing to VAE (same as during training)\n    # The VAE was trained on normalized [0,1] data, so inference must use the same scale\n    logger.info(\"Normalizing data for VAE inference (same as training)\")\n    scaler = MinMaxScaler()\n    X_train_normalized = scaler.fit_transform(X_train)\n    X_test_normalized = scaler.transform(X_test)\n    logger.info(\n        f\"Data normalized: min={X_train_normalized.min():.4f}, max={X_train_normalized.max():.4f}\"\n    )\n\n    with torch.no_grad():\n        X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32).to(device)\n        X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n\n        train_recon_normalized, _, _, _ = vae_model(X_train_tensor)\n        test_recon_normalized, _, _, _ = vae_model(X_test_tensor)\n\n        # Denormalize VAE output to match original data scale\n        train_recon = scaler.inverse_transform(train_recon_normalized.cpu().numpy())\n        test_recon = scaler.inverse_transform(test_recon_normalized.cpu().numpy())\n\n    # Convert to DataFrames\n    train_indices = [f\"train_{i}\" for i in range(len(X_train))]\n    test_indices = [f\"test_{i}\" for i in range(len(X_test))]\n\n    all_recon = np.vstack([train_recon, test_recon])\n    all_original = np.vstack([X_train, X_test])\n    all_indices = train_indices + test_indices\n\n    df_reconstruction = pd.DataFrame(all_recon, index=all_indices)\n    df_original = pd.DataFrame(all_original, index=all_indices)\n\n    # Step 3: Train reconstruction network\n    logger.info(\"Step 3: Training reconstruction network\")\n    input_dim = X_train.shape[1]\n\n    if reconstruction_network_dims is None:\n        reconstruction_network_dims = [input_dim, 4096, 1024, 4096, input_dim]\n\n    network = NetworkReconstruction(reconstruction_network_dims)\n\n    network, loss_train, loss_test = train_reconstruction_network(\n        network=network,\n        vae_reconstructions=df_reconstruction,\n        original_data=df_original,\n        train_indices=train_indices,\n        test_indices=test_indices,\n        epochs=reconstruction_epochs,\n        lr=reconstruction_lr,\n        batch_size=batch_size_reconstruction,\n        device=str(device),\n    )\n\n    # Step 4: Save everything\n    logger.info(\"Step 4: Saving models and results\")\n\n    # Save reconstruction network\n    torch.save(network.state_dict(), save_dir / \"reconstruction_network.pth\")\n\n    # Save network dimensions\n    pd.DataFrame(\n        [reconstruction_network_dims],\n        columns=[\"in_dim\", \"layer1_dim\", \"layer2_dim\", \"layer3_dim\", \"out_dim\"],\n    ).to_csv(save_dir / \"network_dims.csv\", index=False)\n\n    # Save losses\n    pd.DataFrame({\"train_loss\": loss_train, \"test_loss\": loss_test}).to_csv(\n        save_dir / \"reconstruction_losses.csv\", index=False\n    )\n\n    # Plot losses using Plotly\n    from renalprog.plots import plot_reconstruction_losses\n\n    plot_reconstruction_losses(\n        loss_train, loss_test, save_path=save_dir / \"reconstruction_losses\"\n    )\n\n    reconstruction_history = {\"train_loss\": loss_train, \"test_loss\": loss_test}\n\n    logger.info(f\"Full pipeline complete! Models saved to {save_dir}\")\n\n    return vae_model, network, vae_history, reconstruction_history\n</code></pre>"},{"location":"api/models/#utility-functions","title":"Utility Functions","text":""},{"location":"api/models/#create_dataloader","title":"create_dataloader","text":"<p>Create PyTorch DataLoader from numpy arrays.</p>"},{"location":"api/models/#renalprog.modeling.train.create_dataloader","title":"create_dataloader","text":"<pre><code>create_dataloader(\n    X: ndarray,\n    y: Optional[ndarray] = None,\n    batch_size: int = 32,\n    shuffle: bool = True,\n) -&gt; torch.utils.data.DataLoader\n</code></pre> <p>Create DataLoader with MinMax normalization.</p> <p>Args:     X: Input data (samples x features)     y: Optional labels     batch_size: Batch size     shuffle: Whether to shuffle data</p> <p>Returns:     DataLoader</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def create_dataloader(\n    X: np.ndarray,\n    y: Optional[np.ndarray] = None,\n    batch_size: int = 32,\n    shuffle: bool = True,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Create DataLoader with MinMax normalization.\n\n    Args:\n        X: Input data (samples x features)\n        y: Optional labels\n        batch_size: Batch size\n        shuffle: Whether to shuffle data\n\n    Returns:\n        DataLoader\n    \"\"\"\n    # Normalize with MinMaxScaler\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Convert to tensors\n    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n\n    if y is not None:\n        y_tensor = torch.tensor(y, dtype=torch.float32)\n        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n    else:\n        dataset = torch.utils.data.TensorDataset(X_tensor)\n\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle\n    )\n\n    return dataloader\n</code></pre>"},{"location":"api/models/#frange_cycle_linear","title":"frange_cycle_linear","text":"<p>Generate cyclical annealing schedule for KL divergence.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.train import frange_cycle_linear\n\n# Create annealing schedule\nschedule = frange_cycle_linear(\n    n_iter=1000,\n    start=0.0,\n    stop=1.0,\n    n_cycle=4,\n    ratio=0.5\n)\n\n# Use in training loop\nfor i, beta in enumerate(schedule):\n    loss = reconstruction_loss + beta * kl_loss\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.frange_cycle_linear","title":"frange_cycle_linear","text":"<pre><code>frange_cycle_linear(\n    start: float,\n    stop: float,\n    n_epoch: int,\n    n_cycle: int = 4,\n    ratio: float = 0.5,\n) -&gt; np.ndarray\n</code></pre> <p>Generate a linear cyclical schedule for beta hyperparameter.</p> <p>This creates a cyclical annealing schedule where beta increases linearly from start to stop over a portion of each cycle (controlled by ratio), then stays constant at stop for the remainder of the cycle.</p> <p>Args:     start: Initial value of beta (typically 0.0)     stop: Final/maximum value of beta (typically 1.0)     n_epoch: Total number of epochs     n_cycle: Number of cycles (default: 4)     ratio: Ratio of cycle spent increasing beta (default: 0.5)            - 0.5 means half cycle increasing, half constant            - 1.0 means entire cycle increasing</p> <p>Returns:     Array of beta values for each epoch</p> <p>Example:     &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle     &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)     &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0     &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0     &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def frange_cycle_linear(\n    start: float, stop: float, n_epoch: int, n_cycle: int = 4, ratio: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a linear cyclical schedule for beta hyperparameter.\n\n    This creates a cyclical annealing schedule where beta increases linearly\n    from start to stop over a portion of each cycle (controlled by ratio),\n    then stays constant at stop for the remainder of the cycle.\n\n    Args:\n        start: Initial value of beta (typically 0.0)\n        stop: Final/maximum value of beta (typically 1.0)\n        n_epoch: Total number of epochs\n        n_cycle: Number of cycles (default: 4)\n        ratio: Ratio of cycle spent increasing beta (default: 0.5)\n               - 0.5 means half cycle increasing, half constant\n               - 1.0 means entire cycle increasing\n\n    Returns:\n        Array of beta values for each epoch\n\n    Example:\n        &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle\n        &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)\n        &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0\n    \"\"\"\n    L = np.ones(n_epoch) * stop  # Initialize all to stop value\n    period = n_epoch / n_cycle\n    step = (stop - start) / (period * ratio)  # Linear schedule\n\n    for c in range(n_cycle):\n        v, i = start, 0\n        while v &lt;= stop and (int(i + c * period) &lt; n_epoch):\n            L[int(i + c * period)] = v\n            v += step\n            i += 1\n\n    return L\n</code></pre>"},{"location":"api/models/#post-processing-network","title":"Post-Processing Network","text":""},{"location":"api/models/#networkreconstruction","title":"NetworkReconstruction","text":"<p>Neural network for refining VAE reconstructions.</p>"},{"location":"api/models/#renalprog.modeling.train.NetworkReconstruction","title":"NetworkReconstruction","text":"<pre><code>NetworkReconstruction(layer_dims: List[int])\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Deep neural network to adjust VAE reconstruction.</p> <p>This network is trained on top of VAE output to improve reconstruction quality by learning a mapping from VAE reconstruction to original data.</p> <p>Args:     layer_dims: List of layer dimensions [input_dim, hidden1, hidden2, ..., output_dim]</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def __init__(self, layer_dims: List[int]):\n    super().__init__()\n    layers = []\n    for i in range(len(layer_dims) - 1):\n        layers.append(nn.Linear(layer_dims[i], layer_dims[i + 1]))\n        # Add ReLU after all layers including the final output layer\n        # This ensures all outputs are non-negative (suitable for gene expression)\n        layers.append(nn.ReLU())\n    self.network = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/models/#renalprog.modeling.train.NetworkReconstruction-functions","title":"Functions","text":""},{"location":"api/models/#renalprog.modeling.train.NetworkReconstruction.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through network.</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through network.\"\"\"\n    return self.network(x)\n</code></pre>"},{"location":"api/models/#train_reconstruction_network","title":"train_reconstruction_network","text":"<p>Train post-processing network.</p>"},{"location":"api/models/#renalprog.modeling.train.train_reconstruction_network","title":"train_reconstruction_network","text":"<pre><code>train_reconstruction_network(\n    network: Module,\n    vae_reconstructions: DataFrame,\n    original_data: DataFrame,\n    train_indices: List,\n    test_indices: List,\n    epochs: int = 200,\n    lr: float = 0.0001,\n    batch_size: int = 32,\n    device: str = \"cpu\",\n) -&gt; Tuple[nn.Module, List[float], List[float]]\n</code></pre> <p>Train reconstruction network to adjust VAE output.</p> <p>Args:     network: NetworkReconstruction model     vae_reconstructions: DataFrame with VAE reconstructions (samples x genes)     original_data: DataFrame with original gene expression (samples x genes)     train_indices: List of training sample indices     test_indices: List of test sample indices     epochs: Number of training epochs     lr: Learning rate     batch_size: Batch size     device: Device to use</p> <p>Returns:     Tuple of (trained_network, train_losses, test_losses)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_reconstruction_network(\n    network: nn.Module,\n    vae_reconstructions: pd.DataFrame,\n    original_data: pd.DataFrame,\n    train_indices: List,\n    test_indices: List,\n    epochs: int = 200,\n    lr: float = 1e-4,\n    batch_size: int = 32,\n    device: str = \"cpu\",\n) -&gt; Tuple[nn.Module, List[float], List[float]]:\n    \"\"\"\n    Train reconstruction network to adjust VAE output.\n\n    Args:\n        network: NetworkReconstruction model\n        vae_reconstructions: DataFrame with VAE reconstructions (samples x genes)\n        original_data: DataFrame with original gene expression (samples x genes)\n        train_indices: List of training sample indices\n        test_indices: List of test sample indices\n        epochs: Number of training epochs\n        lr: Learning rate\n        batch_size: Batch size\n        device: Device to use\n\n    Returns:\n        Tuple of (trained_network, train_losses, test_losses)\n    \"\"\"\n    network = network.to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(network.parameters(), lr=lr)\n\n    # Create dataloaders\n    train_dataset = torch.utils.data.TensorDataset(\n        torch.tensor(\n            vae_reconstructions.loc[train_indices].values, dtype=torch.float32\n        ),\n        torch.tensor(original_data.loc[train_indices].values, dtype=torch.float32),\n    )\n    test_dataset = torch.utils.data.TensorDataset(\n        torch.tensor(vae_reconstructions.loc[test_indices].values, dtype=torch.float32),\n        torch.tensor(original_data.loc[test_indices].values, dtype=torch.float32),\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False\n    )\n\n    loss_train = []\n    loss_test = []\n\n    logger.info(f\"Training reconstruction network for {epochs} epochs\")\n\n    # Add epoch progress bar\n    epoch_pbar = tqdm(range(epochs), desc=\"Reconstruction Network Training\", position=0)\n    for epoch in epoch_pbar:\n        # Training\n        network.train()\n        running_loss = 0.0\n\n        # Add batch progress bar\n        train_pbar = tqdm(train_loader, desc=\"Train\", leave=False, position=1)\n        for vae_recon, original in train_pbar:\n            vae_recon = vae_recon.to(device)\n            original = original.to(device)\n\n            optimizer.zero_grad()\n            output = network(vae_recon)\n            loss = criterion(output, original)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # Update batch progress bar\n            train_pbar.set_postfix({\"batch_loss\": f\"{loss.item():.6f}\"})\n\n        train_loss = running_loss / len(train_loader)\n        loss_train.append(train_loss)\n\n        # Validation\n        network.eval()\n        running_loss = 0.0\n        with torch.no_grad():\n            # Add validation batch progress bar\n            val_pbar = tqdm(test_loader, desc=\"Val\", leave=False, position=1)\n            for vae_recon, original in val_pbar:\n                vae_recon = vae_recon.to(device)\n                original = original.to(device)\n\n                output = network(vae_recon)\n                loss = criterion(output, original)\n                running_loss += loss.item()\n\n                # Update validation progress bar\n                val_pbar.set_postfix({\"batch_loss\": f\"{loss.item():.6f}\"})\n\n        test_loss = running_loss / len(test_loader)\n        loss_test.append(test_loss)\n\n        # Update epoch progress bar with current metrics\n        epoch_pbar.set_postfix(\n            {\"train_loss\": f\"{train_loss:.6f}\", \"test_loss\": f\"{test_loss:.6f}\"}\n        )\n\n        if (epoch + 1) % 20 == 0:\n            logger.info(\n                f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\"\n            )\n\n    logger.info(\"Reconstruction network training complete\")\n    return network, loss_train, loss_test\n</code></pre>"},{"location":"api/models/#see-also","title":"See Also","text":"<ul> <li>Training API - Complete training pipeline</li> <li>Prediction API - Using trained models</li> <li>Configuration - Model hyperparameters</li> </ul>"},{"location":"api/plots/","title":"Plots API","text":"<p>Visualization functions for gene expression analysis, model training, and results presentation.</p>"},{"location":"api/plots/#overview","title":"Overview","text":"<p>The plots module provides publication-quality visualization for:</p> <ul> <li>Training history and loss curves</li> <li>Latent space representations</li> <li>Gene expression heatmaps</li> <li>Trajectories and pathways</li> <li>Confusion matrices</li> <li>Enrichment results</li> </ul>"},{"location":"api/plots/#core-plotting-functions","title":"Core Plotting Functions","text":""},{"location":"api/plots/#save_plot","title":"save_plot","text":"<p>Utility function for saving plots with consistent formatting.</p> <p>Example Usage:</p> <pre><code>from renalprog.plots import save_plot\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot([1, 2, 3], [4, 5, 6])\nax.set_title(\"My Plot\")\n\nsave_plot(\n    fig=fig,\n    output_path=Path(\"reports/figures/my_plot.png\"),\n    dpi=300,\n    bbox_inches='tight'\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.save_plot","title":"save_plot","text":"<pre><code>save_plot(\n    fig: Figure,\n    save_path: Union[str, Path],\n    formats: List[str] = [\"html\", \"png\", \"pdf\", \"svg\"],\n    width: int = DEFAULT_WIDTH,\n    height: int = DEFAULT_HEIGHT,\n) -&gt; None\n</code></pre> <p>Save plotly figure in multiple formats.</p> <p>Args:     fig: Plotly figure object     save_path: Base path for saving (without extension)     formats: List of formats to save ['html', 'png', 'pdf', 'svg']     width: Width in pixels for static formats     height: Height in pixels for static formats</p> Source code in <code>renalprog/plots.py</code> <pre><code>def save_plot(\n    fig: go.Figure,\n    save_path: Union[str, Path],\n    formats: List[str] = [\"html\", \"png\", \"pdf\", \"svg\"],\n    width: int = DEFAULT_WIDTH,\n    height: int = DEFAULT_HEIGHT,\n) -&gt; None:\n    \"\"\"\n    Save plotly figure in multiple formats.\n\n    Args:\n        fig: Plotly figure object\n        save_path: Base path for saving (without extension)\n        formats: List of formats to save ['html', 'png', 'pdf', 'svg']\n        width: Width in pixels for static formats\n        height: Height in pixels for static formats\n    \"\"\"\n    save_path = Path(save_path)\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Remove extension if present\n    base_path = save_path.with_suffix(\"\")\n\n    for fmt in formats:\n        output_path = base_path.with_suffix(f\".{fmt}\")\n        try:\n            if fmt == \"html\":\n                fig.write_html(str(output_path))\n            elif fmt in [\"png\", \"pdf\", \"svg\"]:\n                fig.write_image(str(output_path), width=width, height=height)\n            logger.info(f\"Saved plot to {output_path}\")\n        except Exception as e:\n            logger.warning(f\"Failed to save {fmt} format: {e}\")\n            if fmt in [\"png\", \"pdf\", \"svg\"]:\n                logger.warning(\n                    \"Note: Static image export requires kaleido package: pip install kaleido\"\n                )\n</code></pre>"},{"location":"api/plots/#training-visualization","title":"Training Visualization","text":""},{"location":"api/plots/#plot_training_history","title":"plot_training_history","text":"<p>Visualize VAE training progress.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_training_history\nfrom pathlib import Path\n\n# After training\nhistory, model, checkpoints = train_vae(...)\n\n# Plot training curves\nplot_training_history(\n    history=history,\n    output_path=Path(\"reports/figures/training_history.png\"),\n    title=\"VAE Training Progress\"\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_training_history","title":"plot_training_history","text":"<pre><code>plot_training_history(\n    history: Dict[str, List[float]],\n    save_path: Optional[Path] = None,\n    title: str = \"Training History\",\n    log_scale: bool = False,\n) -&gt; go.Figure\n</code></pre> <p>Plot training and validation losses over epochs.</p> <p>Args:     history: Dictionary with 'train_loss' and 'val_loss' keys     save_path: Optional path to save figure     title: Plot title     log_scale: Whether to use log scale for y-axis</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_training_history(\n    history: Dict[str, List[float]],\n    save_path: Optional[Path] = None,\n    title: str = \"Training History\",\n    log_scale: bool = False,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot training and validation losses over epochs.\n\n    Args:\n        history: Dictionary with 'train_loss' and 'val_loss' keys\n        save_path: Optional path to save figure\n        title: Plot title\n        log_scale: Whether to use log scale for y-axis\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n\n    fig = go.Figure()\n\n    ## Total loss = KL + reconstruction\n\n    # Train loss\n    fig.add_trace(\n        go.Scatter(\n            x=epochs,\n            y=history[\"train_loss\"],\n            mode=\"lines\",\n            name=\"Train Loss\",\n            line=dict(color=\"#1f77b4\", width=2),\n            marker=dict(size=4),\n        )\n    )\n\n    # Validation loss\n    if \"val_loss\" in history:\n        fig.add_trace(\n            go.Scatter(\n                x=epochs,\n                y=history[\"val_loss\"],\n                mode=\"lines\",\n                name=\"Val Loss\",\n                line=dict(color=\"#ff7f0e\", width=2),\n                marker=dict(size=4),\n            )\n        )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Epoch\",\n        yaxis_title=\"Loss\",\n        yaxis_type=\"log\" if log_scale else \"linear\",\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode=\"x unified\",\n    )\n\n    # KL Divergence plot\n    fig_kl = go.Figure()\n    if \"train_kl_loss\" in history and \"val_kl_loss\" in history:\n        fig_kl.add_trace(\n            go.Scatter(\n                x=epochs,\n                y=history[\"train_kl_loss\"],\n                mode=\"lines\",\n                name=\"Train KL Divergence\",\n                line=dict(color=\"#1f77b4\", width=2),\n                marker=dict(size=4),\n            )\n        )\n        fig_kl.add_trace(\n            go.Scatter(\n                x=epochs,\n                y=history[\"val_kl_loss\"],\n                mode=\"lines\",\n                name=\"Val KL Divergence\",\n                line=dict(color=\"#ff7f0e\", width=2),\n                marker=dict(size=4),\n            )\n        )\n\n    # Reconstruction Loss plot\n    fig_rec = go.Figure()\n    if \"train_recon_loss\" in history and \"val_recon_loss\" in history:\n        fig_rec.add_trace(\n            go.Scatter(\n                x=epochs,\n                y=history[\"train_recon_loss\"],\n                mode=\"lines\",\n                name=\"Train Reconstruction Loss\",\n                line=dict(color=\"#1f77b4\", width=2),\n                marker=dict(size=4),\n            )\n        )\n        fig_rec.add_trace(\n            go.Scatter(\n                x=epochs,\n                y=history[\"val_recon_loss\"],\n                mode=\"lines\",\n                name=\"Val Reconstruction Loss\",\n                line=dict(color=\"#ff7f0e\", width=2),\n                marker=dict(size=4),\n            )\n        )\n\n    if save_path:\n        save_plot(fig, save_path / \"total_loss\")\n        save_plot(fig_kl, save_path / \"kl_divergence\")\n        save_plot(fig_rec, save_path / \"reconstruction_loss\")\n\n    return fig\n</code></pre>"},{"location":"api/plots/#plot_reconstruction_losses","title":"plot_reconstruction_losses","text":"<p>Compare reconstruction losses across samples.</p>"},{"location":"api/plots/#renalprog.plots.plot_reconstruction_losses","title":"plot_reconstruction_losses","text":"<pre><code>plot_reconstruction_losses(\n    loss_train: List[float],\n    loss_test: List[float],\n    save_path: Optional[Path] = None,\n    title: str = \"Reconstruction Network Losses\",\n) -&gt; go.Figure\n</code></pre> <p>Plot training and test losses for reconstruction network.</p> <p>Args:     loss_train: List of training losses     loss_test: List of test losses     save_path: Optional path to save figure     title: Plot title</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_reconstruction_losses(\n    loss_train: List[float],\n    loss_test: List[float],\n    save_path: Optional[Path] = None,\n    title: str = \"Reconstruction Network Losses\",\n) -&gt; go.Figure:\n    \"\"\"\n    Plot training and test losses for reconstruction network.\n\n    Args:\n        loss_train: List of training losses\n        loss_test: List of test losses\n        save_path: Optional path to save figure\n        title: Plot title\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    epochs = list(range(1, len(loss_train) + 1))\n\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Scatter(\n            x=epochs,\n            y=loss_train,\n            mode=\"lines\",\n            name=\"Train\",\n            line=dict(color=\"#1f77b4\", width=2),\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=epochs,\n            y=loss_test,\n            mode=\"lines\",\n            name=\"Test\",\n            line=dict(color=\"#ff7f0e\", width=2),\n        )\n    )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Epoch\",\n        yaxis_title=\"Loss\",\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode=\"x unified\",\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#plot_umap_plotly","title":"plot_umap_plotly","text":"<p>Interactive UMAP visualization.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_umap_plotly\n\n# Create interactive plot\nfig = plot_umap_plotly(\n    latent=latent,\n    labels=clinical['stage'],\n    sample_names=clinical.index.tolist(),\n    title=\"Interactive Latent Space\"\n)\n\n# Save as HTML\nfig.write_html(\"reports/figures/latent_space_interactive.html\")\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_umap_plotly","title":"plot_umap_plotly","text":"<pre><code>plot_umap_plotly(\n    data,\n    clinical,\n    colors_dict,\n    shapes_dict=None,\n    n_components=2,\n    save_fig=False,\n    save_as=None,\n    seed=None,\n    title=\"UMAP\",\n    show=True,\n    marker_size=8,\n)\n</code></pre> <p>Plot UMAP of the data with Plotly using different colors for the different groups.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Features as rows and samples as columns (same as in plot_umap).</p> required <code>clinical</code> <code>Series</code> <p>Category per sample (index must match data.columns).</p> required <code>colors_dict</code> <code>dict</code> <p>Mapping {group_name: color_hex_or_name}.</p> required <code>shapes_dict</code> <p>Mapping {group_name: shape}.</p> <code>None</code> <code>n_components</code> <code>int</code> <p>2 or 3, by default 2.</p> <code>2</code> <code>save_fig</code> <code>bool</code> <p>If True, save HTML/PNG/PDF/SVG, by default False.</p> <code>False</code> <code>save_as</code> <code>str or None</code> <p>Base path (without extension) for saving, by default None.</p> <code>None</code> <code>seed</code> <code>int or None</code> <p>Random state for UMAP, by default None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title, by default 'UMAP'.</p> <code>'UMAP'</code> <code>show</code> <code>bool</code> <p>If True, display the plot, by default True.</p> <code>True</code> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_umap_plotly(\n    data,\n    clinical,\n    colors_dict,\n    shapes_dict=None,\n    n_components=2,\n    save_fig=False,\n    save_as=None,\n    seed=None,\n    title=\"UMAP\",\n    show=True,\n    marker_size=8,\n):\n    \"\"\"\n    Plot UMAP of the data with Plotly using different colors for the different groups.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        Features as rows and samples as columns (same as in plot_umap).\n    clinical : pandas.Series\n        Category per sample (index must match data.columns).\n    colors_dict : dict\n        Mapping {group_name: color_hex_or_name}.\n    shapes_dict: dict\n        Mapping {group_name: shape}.\n    n_components : int, optional\n        2 or 3, by default 2.\n    save_fig : bool, optional\n        If True, save HTML/PNG/PDF/SVG, by default False.\n    save_as : str or None, optional\n        Base path (without extension) for saving, by default None.\n    seed : int or None, optional\n        Random state for UMAP, by default None.\n    title : str, optional\n        Plot title, by default 'UMAP'.\n    show : bool, optional\n        If True, display the plot, by default True.\n    \"\"\"\n\n    # Check number of samples is the first dimension of data:\n    if data.shape[0] != clinical.shape[0]:\n        data = data.T\n        if data.shape[0] != clinical.shape[0]:\n            raise ValueError(\n                \"Data and clinical metadata must have the same number of samples\"\n            )\n\n    if n_components not in (2, 3):\n        raise ValueError(\"n_components must be 2 or 3 for plot_umap_plotly\")\n\n    datetime.now().strftime(\"%Y%m%d\")\n    if save_as is None:\n        pass\n\n    if seed is not None:\n        umap_ = umap.UMAP(n_components=n_components, random_state=seed)\n    else:\n        umap_ = umap.UMAP(n_components=n_components)\n\n    # data: samples x features\n    X_umap = umap_.fit_transform(data)\n    print(\"X_umap.shape\", X_umap.shape)\n\n    # Determine color and shape series from clinical\n    if isinstance(clinical, pd.DataFrame):\n        color_col = clinical.columns[0]\n        color_series = clinical[color_col]\n        # use second column for shapes if provided and shapes_dict is given\n        if shapes_dict is not None and clinical.shape[1] &gt;= 2:\n            shape_col = clinical.columns[1]\n            shape_series = clinical[shape_col]\n        else:\n            shape_series = None\n    elif isinstance(clinical, pd.Series):\n        color_series = clinical\n        shape_series = None\n    else:\n        raise ValueError(\"clinical must be a pandas Series or DataFrame\")\n    print(\"color_series.shape\", color_series.shape)\n\n    # Build plotting DataFrame\n    all_patients = data.index.tolist()\n    print(\"len(all_patients)\", len(all_patients))\n    print(\n        \"color_series.loc[all_patients].values.shape\",\n        color_series.loc[all_patients].values.shape,\n    )\n    df_plot = pd.DataFrame(\n        {\n            \"sample\": all_patients,\n            \"group\": color_series.loc[all_patients].values,\n            \"UMAP_1\": X_umap[:, 0],\n            \"UMAP_2\": X_umap[:, 1],\n        }\n    )\n    if n_components == 3:\n        df_plot[\"UMAP_3\"] = X_umap[:, 2]\n\n    # Attach shape column if available\n    if shape_series is not None:\n        df_plot[\"shape\"] = shape_series.loc[all_patients].values\n\n    # Build color sequence in the order of unique groups\n    unique_groups = df_plot[\"group\"].unique()\n    color_sequence = [colors_dict[g] for g in unique_groups]\n\n    # Prepare symbol mapping if shapes are used\n    symbol_map = None\n    if \"shape\" in df_plot.columns and shapes_dict is not None:\n        # convert common Matplotlib markers to Plotly symbols if needed\n        matplot_to_plotly = {\n            \"o\": \"circle\",\n            \"s\": \"square\",\n            \"^\": \"triangle-up\",\n            \"v\": \"triangle-down\",\n            \"D\": \"diamond\",\n            \"d\": \"diamond-wide\",\n            \"X\": \"x\",\n            \"x\": \"x\",\n            \"*\": \"star\",\n            \"+\": \"cross\",\n            \"p\": \"pentagon\",\n            \"h\": \"hexagon\",\n            \"H\": \"hexagon2\",\n        }\n        unique_shapes = df_plot[\"shape\"].unique()\n        symbol_map = {}\n        for sh in unique_shapes:\n            # get marker definition from shapes_dict; fallback to the value itself\n            marker = shapes_dict.get(sh, shapes_dict.get(str(sh), sh))\n            # translate matplotlib marker codes to plotly symbol names when possible\n            symbol = matplot_to_plotly.get(marker, marker)\n            symbol_map[sh] = symbol\n\n    # Create plotly figure with optional symbols\n    if n_components == 2:\n        fig = px.scatter(\n            df_plot,\n            x=\"UMAP_1\",\n            y=\"UMAP_2\",\n            color=\"group\",\n            color_discrete_sequence=color_sequence,\n            hover_name=\"sample\",\n            template=\"simple_white\",\n            width=800,\n            height=800,\n            symbol=\"shape\"\n            if \"shape\" in df_plot.columns and symbol_map is not None\n            else None,\n            symbol_map=symbol_map if symbol_map is not None else None,\n        )\n        fig.update_layout(\n            title=title,\n            xaxis_title=\"UMAP 1\",\n            yaxis_title=\"UMAP 2\",\n        )\n    else:\n        fig = px.scatter_3d(\n            df_plot,\n            x=\"UMAP_1\",\n            y=\"UMAP_2\",\n            z=\"UMAP_3\",\n            color=\"group\",\n            color_discrete_sequence=color_sequence,\n            hover_name=\"sample\",\n            template=\"simple_white\",\n            width=800,\n            height=800,\n            symbol=\"shape\"\n            if \"shape\" in df_plot.columns and symbol_map is not None\n            else None,\n            symbol_map=symbol_map if symbol_map is not None else None,\n        )\n        fig.update_layout(\n            title=title,\n            scene=dict(\n                xaxis_title=\"UMAP 1\",\n                yaxis_title=\"UMAP 2\",\n                zaxis_title=\"UMAP 3\",\n            ),\n        )\n    fig.update_traces(marker=dict(size=marker_size))\n    # Optional saving\n    if save_fig:\n        base_dir = os.path.dirname(save_as)\n        if base_dir and not os.path.exists(base_dir):\n            os.makedirs(base_dir, exist_ok=True)\n        # Save as HTML\n        fig.write_html(f\"{save_as}.html\")\n        # Save static images\n        for extension in [\"png\", \"pdf\", \"svg\"]:\n            print(f\"Saved UMAP plotly figure to: {save_as}.{extension}\")\n            fig.write_image(f\"{save_as}.{extension}\", scale=2)\n    if show:\n        fig.show()\n</code></pre>"},{"location":"api/plots/#trajectory-visualization","title":"Trajectory Visualization","text":""},{"location":"api/plots/#plot_trajectory","title":"plot_trajectory","text":"<p>Visualize disease progression trajectory.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_trajectory\n\n# Plot single trajectory\nplot_trajectory(\n    trajectory=trajectories[0],  # Shape: (n_steps, n_genes)\n    feature_names=selected_genes,\n    output_path=Path(\"reports/figures/trajectory_001.png\"),\n    title=\"Disease Progression Trajectory\",\n    highlight_genes=['TP53', 'VEGFA', 'HIF1A']\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_trajectory","title":"plot_trajectory","text":"<pre><code>plot_trajectory(\n    trajectory: ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure\n</code></pre> <p>Plot gene expression changes along a trajectory.</p> <p>Args:     trajectory: Array of shape (n_timepoints, n_genes)     gene_names: Optional list of gene names     save_path: Optional path to save figure     title: Plot title     n_genes_to_show: Number of top varying genes to display</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_trajectory(\n    trajectory: np.ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot gene expression changes along a trajectory.\n\n    Args:\n        trajectory: Array of shape (n_timepoints, n_genes)\n        gene_names: Optional list of gene names\n        save_path: Optional path to save figure\n        title: Plot title\n        n_genes_to_show: Number of top varying genes to display\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    n_timepoints, n_genes = trajectory.shape\n\n    # Calculate variance for each gene\n    gene_variance = np.var(trajectory, axis=0)\n    top_genes_idx = np.argsort(gene_variance)[-n_genes_to_show:]\n\n    if gene_names is None:\n        gene_names = [f\"Gene_{i}\" for i in range(n_genes)]\n\n    fig = go.Figure()\n\n    timepoints = list(range(n_timepoints))\n\n    for idx in top_genes_idx:\n        fig.add_trace(\n            go.Scatter(\n                x=timepoints,\n                y=trajectory[:, idx],\n                mode=\"lines\",\n                name=gene_names[idx],\n                line=dict(width=1.5),\n            )\n        )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Timepoint\",\n        yaxis_title=\"Expression Level\",\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode=\"x unified\",\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#pca-visualization","title":"PCA Visualization","text":""},{"location":"api/plots/#plot_pca_variance","title":"plot_pca_variance","text":"<p>Visualize PCA variance explained.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_pca_variance\nfrom sklearn.decomposition import PCA\n\n# Perform PCA\npca = PCA(n_components=50)\npca.fit(expression_data)\n\n# Plot variance explained\nplot_pca_variance(\n    pca=pca,\n    output_path=Path(\"reports/figures/pca_variance.png\"),\n    n_components=20\n)\n</code></pre>"},{"location":"api/plots/#renalprog.plots.plot_pca_variance","title":"plot_pca_variance","text":"<pre><code>plot_pca_variance(\n    explained_variance_ratio: ndarray,\n    save_path: Optional[Path] = None,\n    title: str = \"PCA Explained Variance\",\n    n_components: int = 20,\n) -&gt; go.Figure\n</code></pre> <p>Plot explained variance ratio from PCA.</p> <p>Args:     explained_variance_ratio: Array of explained variance ratios     save_path: Optional path to save figure     title: Plot title     n_components: Number of components to show</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_pca_variance(\n    explained_variance_ratio: np.ndarray,\n    save_path: Optional[Path] = None,\n    title: str = \"PCA Explained Variance\",\n    n_components: int = 20,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot explained variance ratio from PCA.\n\n    Args:\n        explained_variance_ratio: Array of explained variance ratios\n        save_path: Optional path to save figure\n        title: Plot title\n        n_components: Number of components to show\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    n_show = min(n_components, len(explained_variance_ratio))\n    components = list(range(1, n_show + 1))\n\n    # Individual variance\n    fig = make_subplots(\n        rows=1,\n        cols=2,\n        subplot_titles=(\n            \"Individual Explained Variance\",\n            \"Cumulative Explained Variance\",\n        ),\n    )\n\n    fig.add_trace(\n        go.Bar(\n            x=components,\n            y=explained_variance_ratio[:n_show],\n            name=\"Individual\",\n            marker_color=\"#1f77b4\",\n        ),\n        row=1,\n        col=1,\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=components,\n            y=np.cumsum(explained_variance_ratio[:n_show]),\n            mode=\"lines+markers\",\n            name=\"Cumulative\",\n            line=dict(color=\"#ff7f0e\", width=2),\n            marker=dict(size=6),\n        ),\n        row=1,\n        col=2,\n    )\n\n    fig.update_xaxes(title_text=\"Principal Component\", row=1, col=1)\n    fig.update_xaxes(title_text=\"Principal Component\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Explained Variance Ratio\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Cumulative Variance\", row=1, col=2)\n\n    fig.update_layout(\n        title_text=title,\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH * 2,\n        height=DEFAULT_HEIGHT,\n        showlegend=False,\n    )\n\n    if save_path:\n        save_plot(fig, save_path, width=DEFAULT_WIDTH * 2)\n\n    return fig\n</code></pre>"},{"location":"api/plots/#complete-visualization-workflow","title":"Complete Visualization Workflow","text":"<pre><code>import torch\nimport pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE, train_vae\nfrom renalprog.modeling.predict import apply_vae, generate_trajectories\nfrom renalprog.plots import (\n    plot_training_history,\n    plot_trajectory,\n    plot_umap_plotly\n)\n\n# Create output directory\noutput_dir = Path(\"reports/figures\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# 1. Load data\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# 2. Train model and plot history\nhistory, model, checkpoints = train_vae(\n    train_data=train_expr.values,\n    val_data=test_expr.values,\n    input_dim=train_expr.shape[1],\n    mid_dim=1024,\n    features=128,\n    output_dir=Path(\"models/my_vae\"),\n    n_epochs=100\n)\n\nplot_training_history(\n    history=history,\n    output_path=output_dir / \"training_history.png\"\n)\n\n# 3. Encode to latent space and visualize\nresults = apply_vae(model, test_expr.values, device='cuda')\n\nplot_umap_plotly(\n    latent=results['latent'],\n    labels=clinical['stage'],\n    sample_names=clinical.index.tolist(),\n    title=\"Interactive Latent Space\"\n).write_html(output_dir / \"latent_space_interactive.html\")\n\n\n# 4. Generate and plot trajectories\nearly_mask = clinical['stage'] == 'early'\nlate_mask = clinical['stage'] == 'late'\n\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=test_expr.values[early_mask],\n    end_data=test_expr.values[late_mask],\n    n_steps=50,\n    device='cuda'\n)\n\n# Plot first trajectory\nplot_trajectory(\n    trajectory=trajectories[0],\n    feature_names=top_genes.tolist(),\n    output_path=output_dir / \"trajectory_001.png\",\n    title=\"Disease Progression Trajectory\"\n)\n\nprint(f\"All figures saved to {output_dir}\")\n</code></pre>"},{"location":"api/plots/#see-also","title":"See Also","text":"<ul> <li>Training API - Generate training history</li> <li>Prediction API - Generate predictions to plot</li> <li>Trajectories API - Generate trajectories</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/prediction/","title":"Prediction API","text":"<p>Functions for applying trained VAE models to generate latent representations and trajectories.</p>"},{"location":"api/prediction/#overview","title":"Overview","text":"<p>This module provides:</p> <ul> <li>Apply trained VAE to encode data</li> <li>Generate disease progression trajectories</li> <li>Evaluate reconstruction quality</li> <li>Patient connectivity analysis</li> <li>Latent space interpolation</li> </ul>"},{"location":"api/prediction/#core-prediction-functions","title":"Core Prediction Functions","text":""},{"location":"api/prediction/#apply_vae","title":"apply_vae","text":"<p>Apply trained VAE model to encode gene expression data into latent space.</p> <p>Example Usage:</p> <pre><code>import torch\nimport pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE\nfrom renalprog.modeling.predict import apply_vae\n\n# Load model\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel.load_state_dict(torch.load(\"models/my_vae/best_model.pt\"))\n\n# Load test data\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\n\n# Apply VAE\nresults = apply_vae(\n    model=model,\n    data=test_expr.values,\n    device='cuda',\n    batch_size=32\n)\n\nlatent = results['latent']  # Latent representations\nreconstructed = results['reconstructed']  # Reconstructed expression\nprint(f\"Latent space shape: {latent.shape}\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.apply_vae","title":"apply_vae","text":"<pre><code>apply_vae(\n    model: Module, data: DataFrame, device: str = \"cpu\"\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <p>Apply VAE to data to get reconstruction and latent representation.</p> <p>Args:     model: Trained VAE model     data: Input data (samples x genes)     device: Device to run inference on</p> <p>Returns:     Tuple of (reconstruction, mu, logvar, z)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def apply_vae(\n    model: torch.nn.Module, data: pd.DataFrame, device: str = \"cpu\"\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply VAE to data to get reconstruction and latent representation.\n\n    Args:\n        model: Trained VAE model\n        data: Input data (samples x genes)\n        device: Device to run inference on\n\n    Returns:\n        Tuple of (reconstruction, mu, logvar, z)\n    \"\"\"\n    model.eval()\n    model = model.to(device)\n\n    # Convert to tensor\n    if isinstance(data, pd.DataFrame):\n        data_tensor = torch.tensor(data.values, dtype=torch.float32).to(device)\n    else:\n        data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n\n    with torch.no_grad():\n        reconstruction, mu, logvar, z = model(data_tensor)\n\n    # Convert back to numpy\n    reconstruction = reconstruction.cpu().numpy()\n    mu = mu.cpu().numpy()\n    logvar = logvar.cpu().numpy() if logvar is not None else None\n    z = z.cpu().numpy()\n\n    return reconstruction, mu, logvar, z\n</code></pre>"},{"location":"api/prediction/#interpolate_latent_linear","title":"interpolate_latent_linear","text":"<p>Linear interpolation between latent representations.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.interpolate_latent_linear","title":"interpolate_latent_linear","text":"<pre><code>interpolate_latent_linear(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Linear interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_linear(\n    z_source: np.ndarray, z_target: np.ndarray, n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Linear interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array(\n        [(1 - alpha) * z_source + alpha * z_target for alpha in alphas]\n    )\n    return interpolated\n</code></pre>"},{"location":"api/prediction/#interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<p>Spherical (SLERP) interpolation between latent representations.</p> <p>Example:</p> <pre><code>from renalprog.modeling.predict import interpolate_latent_spherical\nimport numpy as np\n\nz_start = np.random.randn(10, 128)  # 10 samples, 128 latent dims\nz_end = np.random.randn(10, 128)\n\n# Spherical interpolation (better for normalized spaces)\ntrajectory = interpolate_latent_spherical(z_start, z_end, n_steps=50)\n# Shape: (10, 50, 128)\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<pre><code>interpolate_latent_spherical(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Spherical (SLERP) interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_spherical(\n    z_source: np.ndarray, z_target: np.ndarray, n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Spherical (SLERP) interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    # Normalize vectors\n    z_source_norm = z_source / np.linalg.norm(z_source)\n    z_target_norm = z_target / np.linalg.norm(z_target)\n\n    # Calculate angle between vectors\n    omega = np.arccos(np.clip(np.dot(z_source_norm, z_target_norm), -1.0, 1.0))\n\n    if omega &lt; 1e-8:\n        # Vectors are nearly identical, use linear interpolation\n        return interpolate_latent_linear(z_source, z_target, n_steps)\n\n    # SLERP formula\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array(\n        [\n            (np.sin((1 - alpha) * omega) / np.sin(omega)) * z_source\n            + (np.sin(alpha * omega) / np.sin(omega)) * z_target\n            for alpha in alphas\n        ]\n    )\n\n    return interpolated\n</code></pre>"},{"location":"api/prediction/#reconstruction-evaluation","title":"Reconstruction Evaluation","text":""},{"location":"api/prediction/#evaluate_reconstruction","title":"evaluate_reconstruction","text":"<p>Comprehensive evaluation of VAE reconstruction quality.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import evaluate_reconstruction\n\n# Evaluate reconstruction quality\nmetrics = evaluate_reconstruction(\n    model=model,\n    original_data=test_expr.values,\n    device='cuda',\n    output_dir=Path(\"reports/reconstruction_eval\")\n)\n\nprint(f\"MSE: {metrics['mse']:.4f}\")\nprint(f\"Pearson R: {metrics['pearson_mean']:.4f}\")\nprint(f\"Cosine similarity: {metrics['cosine_mean']:.4f}\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.evaluate_reconstruction","title":"evaluate_reconstruction","text":"<pre><code>evaluate_reconstruction(\n    real_data: DataFrame,\n    synthetic_data: DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n    metadata_path: Path = None,\n) -&gt; Tuple[pd.Series, pd.Series]\n</code></pre> <p>Comprehensive evaluation of reconstruction quality using SDMetrics.</p> <p>This function orchestrates a complete quality assessment of synthetic/reconstructed data by computing both diagnostic and quality metrics. It's the main entry point for evaluating VAE reconstructions or VAE+RecNet outputs.</p> <p>The evaluation includes: 1. Boundary Adherence: Do synthetic values stay within real data bounds? 2. Distribution Similarity: Do synthetic distributions match real distributions? 3. Quality Report: Overall assessment of column shapes and correlations</p> <p>Args:     real_data: Real gene expression data (samples \u00d7 genes)     synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)     save_path_data: Directory path to save all metric results (CSV, PKL)     save_path_figures: Optional directory path to save visualization plots     metadata_path: Path to CSV file used to extract metadata structure                    (typically the test set CSV file)</p> <p>Returns:     Tuple of (boundary_adherence_series, ks_complement_series):     - boundary_adherence_series: Series with boundary adherence scores per gene     - ks_complement_series: Series with KS Complement scores per gene</p> <p>Workflow:     1. Extract metadata from test CSV file     2. Compute diagnostic metrics (boundary adherence)     3. Compute quality metrics (KS complement + quality report)     4. Save all results and visualizations</p> <p>Output Files:     In save_path_data/:     - boundary_adherence_per_gene.csv: Per-gene boundary scores     - ks_complement_per_gene.csv: Per-gene distribution similarity     - quality_report.pkl: Full SDMetrics quality report object</p> <pre><code>In save_path_figures/ (if provided):\n- boundary_adherence_per_gene.{html,png,pdf,svg}\n- ks_complement_per_gene.{html,png,pdf,svg}\n</code></pre> <p>Interpretation:     - Higher scores are better for both metrics (range: 0.0 to 1.0)     - Boundary Adherence: Checks if synthetic data stays in valid ranges     - KS Complement: Checks if distributions match (more stringent)     - Good reconstruction: BA &gt; 0.95, KS &gt; 0.85     - Excellent reconstruction: BA &gt; 0.99, KS &gt; 0.90</p> <p>Example:     &gt;&gt;&gt; ba_scores, ks_scores = evaluate_reconstruction(     ...     real_data=X_test,     ...     synthetic_data=vae_reconstruction,     ...     save_path_data=\"results/vae_eval/\",     ...     save_path_figures=\"figures/vae_eval/\",     ...     metadata_path=\"data/X_test.csv\"     ... )     &gt;&gt;&gt; print(f\"Mean BA: {ba_scores.mean():.4f}, Mean KS: {ks_scores.mean():.4f}\")     Mean BA: 0.9823, Mean KS: 0.8756</p> <p>Note:     - Ensure real_data and synthetic_data have identical column names and order     - The metadata_path CSV should have the same structure as real_data     - This function is used by scripts/pipeline_steps/3_check_reconstruction.py     - Both DataFrames should have samples as rows and genes as columns</p> <p>Raises:     ValueError: If data shapes don't match or columns don't align     FileNotFoundError: If metadata_path doesn't exist</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def evaluate_reconstruction(\n    real_data: pd.DataFrame,\n    synthetic_data: pd.DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n    metadata_path: Path = None,\n) -&gt; Tuple[pd.Series, pd.Series]:\n    \"\"\"\n    Comprehensive evaluation of reconstruction quality using SDMetrics.\n\n    This function orchestrates a complete quality assessment of synthetic/reconstructed\n    data by computing both diagnostic and quality metrics. It's the main entry point\n    for evaluating VAE reconstructions or VAE+RecNet outputs.\n\n    The evaluation includes:\n    1. Boundary Adherence: Do synthetic values stay within real data bounds?\n    2. Distribution Similarity: Do synthetic distributions match real distributions?\n    3. Quality Report: Overall assessment of column shapes and correlations\n\n    Args:\n        real_data: Real gene expression data (samples \u00d7 genes)\n        synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)\n        save_path_data: Directory path to save all metric results (CSV, PKL)\n        save_path_figures: Optional directory path to save visualization plots\n        metadata_path: Path to CSV file used to extract metadata structure\n                       (typically the test set CSV file)\n\n    Returns:\n        Tuple of (boundary_adherence_series, ks_complement_series):\n        - boundary_adherence_series: Series with boundary adherence scores per gene\n        - ks_complement_series: Series with KS Complement scores per gene\n\n    Workflow:\n        1. Extract metadata from test CSV file\n        2. Compute diagnostic metrics (boundary adherence)\n        3. Compute quality metrics (KS complement + quality report)\n        4. Save all results and visualizations\n\n    Output Files:\n        In save_path_data/:\n        - boundary_adherence_per_gene.csv: Per-gene boundary scores\n        - ks_complement_per_gene.csv: Per-gene distribution similarity\n        - quality_report.pkl: Full SDMetrics quality report object\n\n        In save_path_figures/ (if provided):\n        - boundary_adherence_per_gene.{html,png,pdf,svg}\n        - ks_complement_per_gene.{html,png,pdf,svg}\n\n    Interpretation:\n        - Higher scores are better for both metrics (range: 0.0 to 1.0)\n        - Boundary Adherence: Checks if synthetic data stays in valid ranges\n        - KS Complement: Checks if distributions match (more stringent)\n        - Good reconstruction: BA &gt; 0.95, KS &gt; 0.85\n        - Excellent reconstruction: BA &gt; 0.99, KS &gt; 0.90\n\n    Example:\n        &gt;&gt;&gt; ba_scores, ks_scores = evaluate_reconstruction(\n        ...     real_data=X_test,\n        ...     synthetic_data=vae_reconstruction,\n        ...     save_path_data=\"results/vae_eval/\",\n        ...     save_path_figures=\"figures/vae_eval/\",\n        ...     metadata_path=\"data/X_test.csv\"\n        ... )\n        &gt;&gt;&gt; print(f\"Mean BA: {ba_scores.mean():.4f}, Mean KS: {ks_scores.mean():.4f}\")\n        Mean BA: 0.9823, Mean KS: 0.8756\n\n    Note:\n        - Ensure real_data and synthetic_data have identical column names and order\n        - The metadata_path CSV should have the same structure as real_data\n        - This function is used by scripts/pipeline_steps/3_check_reconstruction.py\n        - Both DataFrames should have samples as rows and genes as columns\n\n    Raises:\n        ValueError: If data shapes don't match or columns don't align\n        FileNotFoundError: If metadata_path doesn't exist\n    \"\"\"\n    logger.info(\"=\" * 80)\n    logger.info(\"RECONSTRUCTION QUALITY EVALUATION\")\n    logger.info(\"=\" * 80)\n    logger.info(f\"Real data shape: {real_data.shape}\")\n    logger.info(f\"Synthetic data shape: {synthetic_data.shape}\")\n    logger.info(f\"Saving results to: {save_path_data}\")\n    if save_path_figures:\n        logger.info(f\"Saving figures to: {save_path_figures}\")\n\n    # Validate inputs\n    if real_data.shape != synthetic_data.shape:\n        raise ValueError(\n            f\"Data shape mismatch: real {real_data.shape} vs synthetic {synthetic_data.shape}\"\n        )\n\n    if not all(real_data.columns == synthetic_data.columns):\n        raise ValueError(\"Column names must match between real and synthetic data\")\n\n    # Step 1: Extract metadata in SDMetrics format\n    logger.info(\"\\n[1/3] Extracting metadata...\")\n    metadata_sd = get_metadata(metadata_path)\n    logger.info(f\"Metadata extracted for {len(metadata_sd['columns'])} genes\")\n\n    # Step 2: Compute diagnostic metrics\n    logger.info(\"\\n[2/3] Computing diagnostic metrics...\")\n    df_ba = diagnostic_metrics(\n        real_data=real_data,\n        synthetic_data=synthetic_data,\n        save_path_data=save_path_data,\n        save_path_figures=save_path_figures,\n    )\n\n    # Step 3: Compute quality metrics\n    logger.info(\"\\n[3/3] Computing quality metrics...\")\n    df_ks = quality_metrics(\n        real_data=real_data,\n        synthetic_data=synthetic_data,\n        metadata=metadata_sd,\n        save_path_data=save_path_data,\n        save_path_figures=save_path_figures,\n    )\n\n    # Final summary\n    logger.info(\"\\n\" + \"=\" * 80)\n    logger.info(\"EVALUATION COMPLETE - SUMMARY\")\n    logger.info(\"=\" * 80)\n    logger.info(\n        f\"Boundary Adherence - Mean: {df_ba.mean():.4f}, Median: {df_ba.median():.4f}\"\n    )\n    logger.info(\n        f\"KS Complement      - Mean: {df_ks.mean():.4f}, Median: {df_ks.median():.4f}\"\n    )\n\n    # Quality assessment\n    ba_quality = (\n        \"Excellent\"\n        if df_ba.mean() &gt; 0.99\n        else \"Good\"\n        if df_ba.mean() &gt; 0.95\n        else \"Fair\"\n        if df_ba.mean() &gt; 0.90\n        else \"Poor\"\n    )\n    ks_quality = (\n        \"Excellent\"\n        if df_ks.mean() &gt; 0.90\n        else \"Good\"\n        if df_ks.mean() &gt; 0.85\n        else \"Fair\"\n        if df_ks.mean() &gt; 0.75\n        else \"Poor\"\n    )\n\n    logger.info(\n        f\"Overall Assessment - Boundary: {ba_quality}, Distribution: {ks_quality}\"\n    )\n    logger.info(f\"Results saved to: {save_path_data}\")\n    logger.info(\"=\" * 80)\n\n    return df_ba, df_ks\n</code></pre>"},{"location":"api/prediction/#quality-metrics","title":"Quality Metrics","text":""},{"location":"api/prediction/#diagnostic_metrics","title":"diagnostic_metrics","text":"<p>Calculate diagnostic metrics for model evaluation.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.diagnostic_metrics","title":"diagnostic_metrics","text":"<pre><code>diagnostic_metrics(\n    real_data: DataFrame,\n    synthetic_data: DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n) -&gt; pd.Series\n</code></pre> <p>Calculate diagnostic metrics to assess synthetic data quality.</p> <p>This function computes the Boundary Adherence metric for each gene, which measures whether synthetic values respect the min/max boundaries of real data. This is a critical diagnostic to detect mode collapse or distribution shift.</p> <p>Args:     real_data: Real gene expression data (samples \u00d7 genes)     synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)     save_path_data: Directory path to save metric CSV results     save_path_figures: Optional directory path to save visualization plots</p> <p>Returns:     Series with boundary adherence scores per gene (index=gene, values=scores).     Scores range from 0.0 (worst) to 1.0 (best).</p> <p>Metric Details:     Boundary Adherence per Gene:     - 1.0 (best): All synthetic values are within [min, max] of real data     - 0.0 (worst): No synthetic values fall within real data boundaries     - Values between 0-1 indicate partial adherence</p> <p>Saves:     - {save_path_data}/boundary_adherence_per_gene.csv: Per-gene scores     - {save_path_figures}/boundary_adherence_per_gene.html: Interactive histogram     - {save_path_figures}/boundary_adherence_per_gene.{png,pdf,svg}: Static plots</p> <p>Example:     &gt;&gt;&gt; ba_scores = diagnostic_metrics(X_real, X_synthetic, \"results/\", \"figures/\")     &gt;&gt;&gt; print(f\"Mean adherence: {ba_scores.mean():.4f}\")     Mean adherence: 0.9823</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def diagnostic_metrics(\n    real_data: pd.DataFrame,\n    synthetic_data: pd.DataFrame,\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Calculate diagnostic metrics to assess synthetic data quality.\n\n    This function computes the Boundary Adherence metric for each gene, which\n    measures whether synthetic values respect the min/max boundaries of real data.\n    This is a critical diagnostic to detect mode collapse or distribution shift.\n\n    Args:\n        real_data: Real gene expression data (samples \u00d7 genes)\n        synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)\n        save_path_data: Directory path to save metric CSV results\n        save_path_figures: Optional directory path to save visualization plots\n\n    Returns:\n        Series with boundary adherence scores per gene (index=gene, values=scores).\n        Scores range from 0.0 (worst) to 1.0 (best).\n\n    Metric Details:\n        Boundary Adherence per Gene:\n        - 1.0 (best): All synthetic values are within [min, max] of real data\n        - 0.0 (worst): No synthetic values fall within real data boundaries\n        - Values between 0-1 indicate partial adherence\n\n    Saves:\n        - {save_path_data}/boundary_adherence_per_gene.csv: Per-gene scores\n        - {save_path_figures}/boundary_adherence_per_gene.html: Interactive histogram\n        - {save_path_figures}/boundary_adherence_per_gene.{png,pdf,svg}: Static plots\n\n    Example:\n        &gt;&gt;&gt; ba_scores = diagnostic_metrics(X_real, X_synthetic, \"results/\", \"figures/\")\n        &gt;&gt;&gt; print(f\"Mean adherence: {ba_scores.mean():.4f}\")\n        Mean adherence: 0.9823\n    \"\"\"\n\n    logger.info(\"=\" * 60)\n    logger.info(\"DIAGNOSTIC METRICS: Boundary Adherence\")\n    logger.info(\"=\" * 60)\n    logger.info(\n        f\"Evaluating {real_data.shape[1]} genes across {real_data.shape[0]} samples\"\n    )\n\n    # Calculate boundary adherence for each gene\n    # This measures what percentage of synthetic values fall within the\n    # [min, max] range observed in the real data\n    ba_dict = {}\n\n    for gene_i in tqdm(real_data.columns, desc=\"Computing Boundary Adherence\"):\n        # Compute metric: % of synthetic values within [min, max] of real values\n        ba_i = BoundaryAdherence.compute(\n            real_data=real_data[gene_i], synthetic_data=synthetic_data[gene_i]\n        )\n        ba_dict[gene_i] = ba_i\n\n    # Convert to Series for easy analysis\n    df_ba = pd.Series(ba_dict, name=\"boundary_adherence\")\n\n    # Save results\n    output_csv = os.path.join(save_path_data, \"boundary_adherence_per_gene.csv\")\n    df_ba.to_csv(output_csv)\n    logger.info(f\"Saved results to: {output_csv}\")\n\n    # Log summary statistics\n    logger.info(f\"Mean Boundary Adherence: {df_ba.mean():.4f}\")\n    logger.info(f\"Median Boundary Adherence: {df_ba.median():.4f}\")\n    logger.info(f\"Min Boundary Adherence: {df_ba.min():.4f}\")\n    logger.info(f\"Max Boundary Adherence: {df_ba.max():.4f}\")\n    logger.info(\n        f\"Genes with perfect adherence (1.0): {(df_ba == 1.0).sum()}/{len(df_ba)} ({100 * (df_ba == 1.0).sum() / len(df_ba):.1f}%)\"\n    )\n\n    # Generate visualizations if output directory provided\n    if save_path_figures is not None:\n        logger.info(\"Generating visualizations...\")\n\n        # Create interactive histogram\n        fig = px.histogram(\n            df_ba,\n            x=\"boundary_adherence\",\n            nbins=50,\n            title=\"Distribution of Boundary Adherence Scores per Gene\",\n            labels={\"boundary_adherence\": \"Boundary Adherence Score\"},\n            template=\"plotly_white\",\n        )\n        fig.update_layout(\n            xaxis_title=\"Boundary Adherence Score\",\n            yaxis_title=\"Number of Genes\",\n            showlegend=False,\n        )\n\n        # Save in multiple formats\n        html_path = os.path.join(save_path_figures, \"boundary_adherence_per_gene.html\")\n        fig.write_html(html_path)\n        logger.info(f\"  Saved interactive plot: {html_path}\")\n\n        for format_ext in [\"png\", \"pdf\", \"svg\"]:\n            img_path = os.path.join(\n                save_path_figures, f\"boundary_adherence_per_gene.{format_ext}\"\n            )\n            fig.write_image(img_path, scale=2)\n            logger.info(f\"  Saved {format_ext.upper()} plot: {img_path}\")\n\n    logger.info(\"Diagnostic metrics calculation complete\")\n    logger.info(\"=\" * 60)\n\n    return df_ba\n</code></pre>"},{"location":"api/prediction/#quality_metrics","title":"quality_metrics","text":"<p>Calculate quality metrics for generated trajectories.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.quality_metrics","title":"quality_metrics","text":"<pre><code>quality_metrics(\n    real_data: DataFrame,\n    synthetic_data: DataFrame,\n    metadata: Dict[str, Any],\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n) -&gt; pd.Series\n</code></pre> <p>Calculate quality metrics to assess synthetic data fidelity.</p> <p>This function computes two key metrics: 1. Quality Report: Overall assessment of column shapes and pair-wise trends 2. KS Complement: Per-gene similarity of marginal distributions</p> <p>These metrics evaluate how well the synthetic data captures the statistical properties of the real data, beyond just staying within boundaries.</p> <p>Args:     real_data: Real gene expression data (samples \u00d7 genes)     synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)     metadata: Metadata dictionary from get_metadata() for SDMetrics     save_path_data: Directory path to save metric results     save_path_figures: Optional directory path to save visualization plots</p> <p>Returns:     Series with KS Complement scores per gene (index=gene, values=scores).     Scores range from 0.0 (worst) to 1.0 (best).</p> <p>Metrics Details:     Column Shapes (in Quality Report):     - Measures overall distribution similarity per column     - Higher scores indicate better shape matching</p> <pre><code>Column Pair Trends (in Quality Report):\n- Measures correlation and relationship preservation\n- Higher scores indicate better trend matching\n\nKS Complement (per gene):\n- 1.0 (best): Real and synthetic distributions are identical\n- 0.0 (worst): Distributions are maximally different\n- Based on Kolmogorov-Smirnov test\n</code></pre> <p>Saves:     - {save_path_data}/quality_report.pkl: Full SDMetrics quality report     - {save_path_data}/ks_complement_per_gene.csv: Per-gene KS scores     - {save_path_figures}/ks_complement_per_gene.html: Interactive histogram     - {save_path_figures}/ks_complement_per_gene.{png,pdf,svg}: Static plots</p> <p>Example:     &gt;&gt;&gt; ks_scores = quality_metrics(X_real, X_synth, metadata, \"results/\", \"figs/\")     &gt;&gt;&gt; print(f\"Mean KS Complement: {ks_scores.mean():.4f}\")     Mean KS Complement: 0.8756</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def quality_metrics(\n    real_data: pd.DataFrame,\n    synthetic_data: pd.DataFrame,\n    metadata: Dict[str, Any],\n    save_path_data: Path,\n    save_path_figures: Optional[Path] = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Calculate quality metrics to assess synthetic data fidelity.\n\n    This function computes two key metrics:\n    1. Quality Report: Overall assessment of column shapes and pair-wise trends\n    2. KS Complement: Per-gene similarity of marginal distributions\n\n    These metrics evaluate how well the synthetic data captures the statistical\n    properties of the real data, beyond just staying within boundaries.\n\n    Args:\n        real_data: Real gene expression data (samples \u00d7 genes)\n        synthetic_data: Synthetic/reconstructed gene expression data (samples \u00d7 genes)\n        metadata: Metadata dictionary from get_metadata() for SDMetrics\n        save_path_data: Directory path to save metric results\n        save_path_figures: Optional directory path to save visualization plots\n\n    Returns:\n        Series with KS Complement scores per gene (index=gene, values=scores).\n        Scores range from 0.0 (worst) to 1.0 (best).\n\n    Metrics Details:\n        Column Shapes (in Quality Report):\n        - Measures overall distribution similarity per column\n        - Higher scores indicate better shape matching\n\n        Column Pair Trends (in Quality Report):\n        - Measures correlation and relationship preservation\n        - Higher scores indicate better trend matching\n\n        KS Complement (per gene):\n        - 1.0 (best): Real and synthetic distributions are identical\n        - 0.0 (worst): Distributions are maximally different\n        - Based on Kolmogorov-Smirnov test\n\n    Saves:\n        - {save_path_data}/quality_report.pkl: Full SDMetrics quality report\n        - {save_path_data}/ks_complement_per_gene.csv: Per-gene KS scores\n        - {save_path_figures}/ks_complement_per_gene.html: Interactive histogram\n        - {save_path_figures}/ks_complement_per_gene.{png,pdf,svg}: Static plots\n\n    Example:\n        &gt;&gt;&gt; ks_scores = quality_metrics(X_real, X_synth, metadata, \"results/\", \"figs/\")\n        &gt;&gt;&gt; print(f\"Mean KS Complement: {ks_scores.mean():.4f}\")\n        Mean KS Complement: 0.8756\n    \"\"\"\n    import os\n    from tqdm import tqdm\n    import plotly.express as px\n\n    logger.info(\"=\" * 60)\n    logger.info(\"QUALITY METRICS: Distribution Similarity\")\n    logger.info(\"=\" * 60)\n\n    # Generate comprehensive quality report\n    # This evaluates:\n    # 1. Column Shapes: How well distributions match per gene\n    # 2. Column Pair Trends: How well correlations are preserved\n    logger.info(\"Generating SDMetrics Quality Report...\")\n    q_report = QualityReport()\n    q_report.generate(real_data, synthetic_data, metadata)\n\n    # Save quality report object for later analysis\n    report_path = os.path.join(save_path_data, \"quality_report.pkl\")\n    q_report.save(report_path)\n    logger.info(f\"Saved quality report to: {report_path}\")\n\n    # Get overall quality score from the report\n    overall_score = q_report.get_score()\n    logger.info(f\"Overall Quality Score: {overall_score:.4f}\")\n\n    # Calculate KS Complement for each gene\n    # This measures similarity of marginal distributions (1D histograms)\n    # KS Complement = 1 - KS statistic, where KS statistic measures max difference between CDFs\n    logger.info(f\"Computing KS Complement for {real_data.shape[1]} genes...\")\n\n    ks_dict = {}\n    for gene_i in tqdm(real_data.columns, desc=\"Computing KS Complement\"):\n        # KS Complement measures how similar the empirical cumulative distribution functions are\n        # Higher values mean the distributions are more similar\n        ks_i = KSComplement.compute(\n            real_data=real_data[gene_i], synthetic_data=synthetic_data[gene_i]\n        )\n        ks_dict[gene_i] = ks_i\n\n    # Convert to Series for analysis\n    df_ks = pd.Series(ks_dict, name=\"ks_complement\")\n\n    # Save results\n    output_csv = os.path.join(save_path_data, \"ks_complement_per_gene.csv\")\n    df_ks.to_csv(output_csv)\n    logger.info(f\"Saved results to: {output_csv}\")\n\n    # Log summary statistics\n    logger.info(f\"Mean KS Complement: {df_ks.mean():.4f}\")\n    logger.info(f\"Median KS Complement: {df_ks.median():.4f}\")\n    logger.info(f\"Min KS Complement: {df_ks.min():.4f}\")\n    logger.info(f\"Max KS Complement: {df_ks.max():.4f}\")\n    logger.info(\n        f\"Genes with KS &gt; 0.9: {(df_ks &gt; 0.9).sum()}/{len(df_ks)} ({100 * (df_ks &gt; 0.9).sum() / len(df_ks):.1f}%)\"\n    )\n\n    # Generate visualizations if output directory provided\n    if save_path_figures is not None:\n        logger.info(\"Generating visualizations...\")\n\n        # Create interactive histogram\n        fig = px.histogram(\n            df_ks,\n            x=\"ks_complement\",\n            nbins=50,\n            title=\"Distribution of KS Complement Scores per Gene\",\n            labels={\"ks_complement\": \"KS Complement Score\"},\n            template=\"plotly_white\",\n        )\n        fig.update_layout(\n            xaxis_title=\"KS Complement Score (Distribution Similarity)\",\n            yaxis_title=\"Number of Genes\",\n            showlegend=False,\n        )\n\n        # Add reference line at 0.9 (high quality threshold)\n        fig.add_vline(\n            x=0.9,\n            line_dash=\"dash\",\n            line_color=\"red\",\n            annotation_text=\"High Quality (0.9)\",\n        )\n\n        # Save in multiple formats\n        html_path = os.path.join(save_path_figures, \"ks_complement_per_gene.html\")\n        fig.write_html(html_path)\n        logger.info(f\"  Saved interactive plot: {html_path}\")\n\n        for format_ext in [\"png\", \"pdf\", \"svg\"]:\n            img_path = os.path.join(\n                save_path_figures, f\"ks_complement_per_gene.{format_ext}\"\n            )\n            fig.write_image(img_path, scale=2)\n            logger.info(f\"  Saved {format_ext.upper()} plot: {img_path}\")\n\n    logger.info(\"Quality metrics calculation complete\")\n    logger.info(\"=\" * 60)\n\n    return df_ks\n</code></pre>"},{"location":"api/prediction/#trajectory-classification","title":"Trajectory Classification","text":""},{"location":"api/prediction/#classify_trajectories","title":"classify_trajectories","text":"<p>Classify disease progression trajectories as progressing vs. non-progressing.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import classify_trajectories\n\n# Train classifier on trajectories\nclassifier, metrics = classify_trajectories(\n    trajectories=trajectory_data,\n    labels=progression_labels,\n    output_dir=Path(\"models/trajectory_classifier\")\n)\n\nprint(f\"Classification accuracy: {metrics['accuracy']:.3f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.3f}\")\n</code></pre>"},{"location":"api/prediction/#renalprog.modeling.predict.classify_trajectories","title":"classify_trajectories","text":"<pre><code>classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, DataFrame],\n    gene_subset: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Apply stage classifier to synthetic trajectories.</p> <p>Args:     classifier: Trained classifier model     trajectory_data: Dictionary of patient pair to trajectory DataFrames     gene_subset: Optional subset of genes to use for classification</p> <p>Returns:     DataFrame with classification results for each trajectory point</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def classify_trajectories(\n    classifier,\n    trajectory_data: Dict[str, pd.DataFrame],\n    gene_subset: Optional[List[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply stage classifier to synthetic trajectories.\n\n    Args:\n        classifier: Trained classifier model\n        trajectory_data: Dictionary of patient pair to trajectory DataFrames\n        gene_subset: Optional subset of genes to use for classification\n\n    Returns:\n        DataFrame with classification results for each trajectory point\n    \"\"\"\n    logger.info(\"Classifying trajectory points\")\n\n    # TODO: Implement trajectory classification\n    # Migrate from notebooks/kirc_classification_trajectory.ipynb\n\n    raise NotImplementedError(\n        \"classify_trajectories() needs implementation from \"\n        \"notebooks/kirc_classification_trajectory.ipynb\"\n    )\n</code></pre>"},{"location":"api/prediction/#network-analysis","title":"Network Analysis","text":""},{"location":"api/prediction/#build_trajectory_network","title":"build_trajectory_network","text":"<p>Build network graph of patient trajectories.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.build_trajectory_network","title":"build_trajectory_network","text":"<pre><code>build_trajectory_network(\n    patient_links: DataFrame,\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]\n</code></pre> <p>Build trajectory network and find all complete disease progression paths.</p> <p>Constructs a directed graph from patient links and identifies all possible complete trajectories from root nodes (earliest stage patients not appearing as targets) to leaf nodes (latest stage patients not appearing as sources).</p> <p>Args:     patient_links: DataFrame with 'source' and 'target' columns from linking functions</p> <p>Returns:     Tuple of:     - network: Dict mapping each source patient to list of target patients     - trajectories: List of complete trajectories, where each trajectory is a                     list of patient IDs ordered from earliest to latest stage</p> <p>Network Structure:     - Adjacency list representation: {source: [target1, target2, ...]}     - Directed edges from earlier to later stages     - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)</p> <p>Trajectory Discovery:     - Uses depth-first search from root nodes     - Root nodes: Patients in 'source' but not in 'target' (stage I or early)     - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)     - Each trajectory represents a complete disease progression path</p> <p>Example:     &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)     &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")     &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")     &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")     Network has 500 nodes     Found 234 complete trajectories     Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']</p> <p>Trajectory Characteristics:     - Length varies based on how many stages the path spans     - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions     - Length 2 for early\u2192late progressions     - Patients can appear in multiple trajectories</p> <p>Note:     - Cycles are prevented during trajectory search     - All paths from root to leaf are enumerated     - Trajectories respect chronological disease progression</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def build_trajectory_network(\n    patient_links: pd.DataFrame,\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]:\n    \"\"\"\n    Build trajectory network and find all complete disease progression paths.\n\n    Constructs a directed graph from patient links and identifies all possible\n    complete trajectories from root nodes (earliest stage patients not appearing\n    as targets) to leaf nodes (latest stage patients not appearing as sources).\n\n    Args:\n        patient_links: DataFrame with 'source' and 'target' columns from linking functions\n\n    Returns:\n        Tuple of:\n        - network: Dict mapping each source patient to list of target patients\n        - trajectories: List of complete trajectories, where each trajectory is a\n                        list of patient IDs ordered from earliest to latest stage\n\n    Network Structure:\n        - Adjacency list representation: {source: [target1, target2, ...]}\n        - Directed edges from earlier to later stages\n        - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)\n\n    Trajectory Discovery:\n        - Uses depth-first search from root nodes\n        - Root nodes: Patients in 'source' but not in 'target' (stage I or early)\n        - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)\n        - Each trajectory represents a complete disease progression path\n\n    Example:\n        &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)\n        &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")\n        &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")\n        &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")\n        Network has 500 nodes\n        Found 234 complete trajectories\n        Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']\n\n    Trajectory Characteristics:\n        - Length varies based on how many stages the path spans\n        - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions\n        - Length 2 for early\u2192late progressions\n        - Patients can appear in multiple trajectories\n\n    Note:\n        - Cycles are prevented during trajectory search\n        - All paths from root to leaf are enumerated\n        - Trajectories respect chronological disease progression\n    \"\"\"\n    logger.info(\"Building trajectory network from patient links\")\n\n    sources = patient_links[\"source\"]\n    targets = patient_links[\"target\"]\n\n    # Build network adjacency list\n    network = {}\n    for source, target in zip(sources, targets):\n        if source not in network:\n            network[source] = []\n        network[source].append(target)\n\n    logger.info(f\"Network built: {len(network)} source nodes\")\n\n    # Find root nodes (patients who are sources but never targets)\n    unique_sources = set(sources) - set(targets)\n    logger.info(f\"Found {len(unique_sources)} root nodes (earliest stage patients)\")\n\n    # Recursively find all trajectories from each root\n    def find_trajectories(\n        start_node: str, visited: Optional[List[str]] = None\n    ) -&gt; List[List[str]]:\n        \"\"\"Depth-first search to find all paths from start_node to leaf nodes.\"\"\"\n        if visited is None:\n            visited = []\n\n        visited.append(start_node)\n\n        # If node has no outgoing edges, this is a leaf node - return path\n        if start_node not in network:\n            return [visited]\n\n        # Recursively explore all targets\n        trajectories = []\n        for target in network[start_node]:\n            if target not in visited:  # Avoid cycles\n                new_visited = visited.copy()\n                trajectories.extend(find_trajectories(target, new_visited))\n\n        return trajectories\n\n    # Find all trajectories starting from each root\n    all_trajectories = []\n\n    if len(unique_sources) == 0:\n        # No clear root nodes - this happens with early\u2192late transitions where\n        # patients can be both sources and targets. In this case, each source\u2192target\n        # pair is already a complete 2-patient trajectory.\n        logger.info(\"No root nodes found (typical for early\u2192late transitions).\")\n        logger.info(\"Using each source\u2192target pair as a complete trajectory.\")\n        for source, target in zip(sources, targets):\n            all_trajectories.append([source, target])\n    else:\n        # Standard case: multi-stage progressions (I\u2192II\u2192III\u2192IV)\n        for source in unique_sources:\n            all_trajectories.extend(find_trajectories(source))\n\n    logger.info(\n        f\"Discovered {len(all_trajectories)} complete disease progression trajectories\"\n    )\n\n    # Log trajectory length statistics only if we have trajectories\n    if len(all_trajectories) &gt; 0:\n        traj_lengths = [len(t) for t in all_trajectories]\n        logger.info(\n            f\"Trajectory lengths - Min: {min(traj_lengths)}, Max: {max(traj_lengths)}, \"\n            f\"Mean: {np.mean(traj_lengths):.1f}\"\n        )\n    else:\n        logger.warning(\"No trajectories found!\")\n\n    return network, all_trajectories\n</code></pre>"},{"location":"api/prediction/#link_patients_closest","title":"link_patients_closest","text":"<p>Link patients using closest neighbor strategy.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.link_patients_closest","title":"link_patients_closest","text":"<pre><code>link_patients_closest(\n    transitions_df: DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients by selecting closest (or farthest) matches across stages.</p> <p>For each patient at a source stage, this function identifies the closest (or farthest) patient at the target stage, considering metadata constraints (gender, race). This creates one-to-one patient linkages that form the basis for trajectory construction.</p> <p>Args:     transitions_df: DataFrame from calculate_all_possible_transitions()                     containing all possible patient pairs with distances     start_with_first_stage: If True, build forward trajectories (early\u2192late)                             If False, build backward trajectories (late\u2192early)     early_late: If True, uses early/late groupings. If False, uses I-IV stages     closest: If True, connect closest patients. If False, connect farthest patients</p> <p>Returns:     DataFrame with selected patient links, containing one row per source patient     with their optimal target patient match. Includes all columns from transitions_df.</p> <p>Selection Strategy:     - Forward (start_with_first_stage=True): For each source, find optimal target     - Backward (start_with_first_stage=False): For each target, find optimal source     - Closest (closest=True): Minimum distance match     - Farthest (closest=False): Maximum distance match</p> <p>Metadata Stratification:     Links are selected independently within each combination of:     - Gender (MALE, FEMALE)     - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)     This ensures demographic consistency in trajectories.</p> <p>Example:     &gt;&gt;&gt; links = link_patients_closest(     ...     transitions_df=all_transitions,     ...     start_with_first_stage=True,     ...     closest=True     ... )     &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")     Created 234 patient links</p> <p>Note:     - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV     - Processes in reverse for backward: IV\u2192III\u2192II\u2192I     - Each patient appears at most once as a source in the result</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_closest(\n    transitions_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients by selecting closest (or farthest) matches across stages.\n\n    For each patient at a source stage, this function identifies the closest\n    (or farthest) patient at the target stage, considering metadata constraints\n    (gender, race). This creates one-to-one patient linkages that form the basis\n    for trajectory construction.\n\n    Args:\n        transitions_df: DataFrame from calculate_all_possible_transitions()\n                        containing all possible patient pairs with distances\n        start_with_first_stage: If True, build forward trajectories (early\u2192late)\n                                If False, build backward trajectories (late\u2192early)\n        early_late: If True, uses early/late groupings. If False, uses I-IV stages\n        closest: If True, connect closest patients. If False, connect farthest patients\n\n    Returns:\n        DataFrame with selected patient links, containing one row per source patient\n        with their optimal target patient match. Includes all columns from transitions_df.\n\n    Selection Strategy:\n        - Forward (start_with_first_stage=True): For each source, find optimal target\n        - Backward (start_with_first_stage=False): For each target, find optimal source\n        - Closest (closest=True): Minimum distance match\n        - Farthest (closest=False): Maximum distance match\n\n    Metadata Stratification:\n        Links are selected independently within each combination of:\n        - Gender (MALE, FEMALE)\n        - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)\n        This ensures demographic consistency in trajectories.\n\n    Example:\n        &gt;&gt;&gt; links = link_patients_closest(\n        ...     transitions_df=all_transitions,\n        ...     start_with_first_stage=True,\n        ...     closest=True\n        ... )\n        &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")\n        Created 234 patient links\n\n    Note:\n        - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV\n        - Processes in reverse for backward: IV\u2192III\u2192II\u2192I\n        - Each patient appears at most once as a source in the result\n    \"\"\"\n    logger.info(\"Linking patients by closest/farthest matches\")\n    logger.info(f\"Direction: {'Forward' if start_with_first_stage else 'Backward'}\")\n    logger.info(f\"Strategy: {'Closest' if closest else 'Farthest'}\")\n\n    # Define transition order based on direction\n    if start_with_first_stage and not early_late:\n        transitions_possible = [\"1_to_2\", \"2_to_3\", \"3_to_4\"]\n    elif not start_with_first_stage and not early_late:\n        transitions_possible = [\"3_to_4\", \"2_to_3\", \"1_to_2\"]\n    elif early_late:\n        transitions_possible = [\"early_to_late\"]\n\n    # 0 for closest (smallest distance), -1 for farthest (largest distance)\n    idx = 0 if closest else -1\n\n    # Find closest/farthest patient for each source patient\n    closest_list = []\n    for transition_i in transitions_possible:\n        transition_df_i = transitions_df[transitions_df[\"transition\"] == transition_i]\n\n        logger.info(\n            f\"Processing transition {transition_i}: {len(transition_df_i)} pairs\"\n        )\n\n        # Iterate through all metadata combinations\n        for gender_i in [\"FEMALE\", \"MALE\"]:\n            df_gender_i = transition_df_i.query(f\"source_gender == '{gender_i}'\")\n\n            for race_i in [\"ASIAN\", \"BLACK OR AFRICAN AMERICAN\", \"WHITE\"]:\n                df_race_i = df_gender_i.query(f\"source_race == '{race_i}'\")\n\n                if df_race_i.empty:\n                    continue\n\n                # Get unique patients to link\n                unique_sources = df_race_i[\"source\"].unique()\n                unique_targets = df_race_i[\"target\"].unique()\n                use_uniques = (\n                    unique_sources if start_with_first_stage else unique_targets\n                )\n                use_column = \"source\" if start_with_first_stage else \"target\"\n\n                # Find closest/farthest match for each patient\n                for pat_i in use_uniques:\n                    pat_matches = df_race_i[df_race_i[use_column] == pat_i]\n                    if len(pat_matches) &gt; 0:\n                        # Sort by distance and select first (closest) or last (farthest)\n                        best_match = pat_matches.sort_values(\"distance\").iloc[idx]\n                        closest_list.append(best_match)\n\n    # Convert to DataFrame\n    closest_df = pd.DataFrame(closest_list)\n    closest_df.reset_index(drop=True, inplace=True)\n\n    logger.info(f\"Created {len(closest_df)} patient links\")\n\n    return closest_df\n</code></pre>"},{"location":"api/prediction/#link_patients_random","title":"link_patients_random","text":"<p>Link patients randomly (for control/comparison).</p>"},{"location":"api/prediction/#renalprog.modeling.predict.link_patients_random","title":"link_patients_random","text":"<pre><code>link_patients_random(\n    results_df: DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients to multiple random targets at the next stage.</p> <p>Instead of linking each patient to only their closest match, this function randomly samples multiple patients at the next stage to link to each source patient. This creates a one-to-many mapping useful for generating multiple trajectory samples.</p> <p>Parameters:</p> Name Type Description Default <code>results_df</code> <code>DataFrame</code> <p>DataFrame with possible sources and targets, their metadata, and distance.</p> required <code>start_with_first_stage</code> <code>bool</code> <p>If True, initiate trajectories with first stage as sources. If False, initiate trajectories with last stage as sources.</p> <code>True</code> <code>link_next</code> <code>int</code> <p>Number of patients at next stage to randomly link to each patient of current stage.</p> <code>5</code> <code>transitions_possible</code> <code>list</code> <p>List of transitions to process (e.g., ['1_to_2', '2_to_3']). If None, defaults to ['early_to_late'].</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with randomly sampled patient links for each transition. Contains multiple rows per source patient (up to link_next).</p> Notes <ul> <li>Random sampling is primarily performed for WHITE race patients due to sample size</li> <li>If fewer than link_next targets are available, all available targets are selected</li> <li>Patients from other races are included with all their possible connections</li> <li>Empty DataFrame is returned if no WHITE patients are found</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_random(\n    results_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients to multiple random targets at the next stage.\n\n    Instead of linking each patient to only their closest match, this function randomly\n    samples multiple patients at the next stage to link to each source patient. This\n    creates a one-to-many mapping useful for generating multiple trajectory samples.\n\n    Parameters\n    ----------\n    results_df : pd.DataFrame\n        DataFrame with possible sources and targets, their metadata, and distance.\n    start_with_first_stage : bool, default=True\n        If True, initiate trajectories with first stage as sources.\n        If False, initiate trajectories with last stage as sources.\n    link_next : int, default=5\n        Number of patients at next stage to randomly link to each patient of current stage.\n    transitions_possible : list, optional\n        List of transitions to process (e.g., ['1_to_2', '2_to_3']).\n        If None, defaults to ['early_to_late'].\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with randomly sampled patient links for each transition.\n        Contains multiple rows per source patient (up to link_next).\n\n    Notes\n    -----\n    - Random sampling is primarily performed for WHITE race patients due to sample size\n    - If fewer than link_next targets are available, all available targets are selected\n    - Patients from other races are included with all their possible connections\n    - Empty DataFrame is returned if no WHITE patients are found\n    \"\"\"\n    # Set default transitions if not provided\n    if transitions_possible is None:\n        transitions_possible = [\"early_to_late\"]\n\n    # Get unique genders and races\n    unique_genders = results_df[\"source_gender\"].unique().tolist()\n    # Get unique races\n    unique_races = results_df[\"source_race\"].unique().tolist()\n    if \"WHITE\" in unique_races:\n        unique_races.remove(\"WHITE\")\n    # transition:\n    samples = []\n    for transition_i in transitions_possible:\n        transition_df_i = results_df[results_df[\"transition\"] == transition_i]\n        for gender_i in unique_genders:\n            df_samples_i = transition_df_i.query(\n                f\"source_gender == '{gender_i}' &amp; source_race == 'WHITE'\"\n            )  # we can only do this for the whites since these are the only ones with enough samples\n            if df_samples_i.empty:\n                print(\n                    f\"Warning: No WHITE patients found for gender {gender_i} in transition {transition_i}\"\n                )\n                continue\n            unique_sources_i = np.unique(df_samples_i[\"source\"]).tolist()\n            unique_targets_i = np.unique(df_samples_i[\"target\"]).tolist()\n            use_uniques = (\n                unique_sources_i if start_with_first_stage else unique_targets_i\n            )\n            use_source_target = \"source\" if start_with_first_stage else \"target\"\n            for pat_i in use_uniques:\n                sample_i = df_samples_i.loc[df_samples_i[use_source_target] == pat_i]\n                if len(sample_i) &gt;= link_next:\n                    sample_i = sample_i.sample(\n                        link_next\n                    )  # Sample a number of patients at next stage to link to each patient of current stage\n                else:\n                    sample_i = sample_i.sample(\n                        len(sample_i)\n                    )  # Sample all available patients if less than link_next\n                samples.append(sample_i)\n\n    # Check if samples list is empty\n    if not samples:\n        print(\"Warning: No samples found for WHITE race. Returning empty DataFrame.\")\n        return pd.DataFrame(columns=results_df.columns)\n\n    # Turn samples into dataframe:\n    samples_df = pd.concat(samples)\n    # Add the rest of the races\n    if unique_races:\n        samples_df = pd.concat(\n            [samples_df, results_df[results_df[\"source_race\"].isin(unique_races)]]\n        )\n    samples_df.reset_index(drop=True, inplace=True)\n    return samples_df\n</code></pre>"},{"location":"api/prediction/#dynamic-analysis","title":"Dynamic Analysis","text":""},{"location":"api/prediction/#dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<p>Perform pathway enrichment along trajectory timepoints.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<pre><code>dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Perform dynamic enrichment analysis on synthetic trajectories.</p> <p>This orchestrates: 1. DESeq2 analysis on each trajectory point 2. GSEA on differential expression results 3. Aggregation of enrichment across trajectories</p> <p>Args:     trajectory_dir: Directory containing trajectory CSV files     pathways_file: Path to pathway GMT file     output_dir: Directory to save results     cancer_type: Cancer type identifier</p> <p>Returns:     DataFrame with aggregated enrichment results</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform dynamic enrichment analysis on synthetic trajectories.\n\n    This orchestrates:\n    1. DESeq2 analysis on each trajectory point\n    2. GSEA on differential expression results\n    3. Aggregation of enrichment across trajectories\n\n    Args:\n        trajectory_dir: Directory containing trajectory CSV files\n        pathways_file: Path to pathway GMT file\n        output_dir: Directory to save results\n        cancer_type: Cancer type identifier\n\n    Returns:\n        DataFrame with aggregated enrichment results\n    \"\"\"\n    logger.info(f\"Running dynamic enrichment analysis for {cancer_type}\")\n\n    # TODO: Implement orchestration\n    # Migrate from src_deseq_and_gsea_NCSR/pipeline.sh and related scripts\n\n    raise NotImplementedError(\n        \"dynamic_enrichment_analysis() needs implementation. \"\n        \"Migrate orchestration from src_deseq_and_gsea_NCSR/pipeline.sh, \"\n        \"py_deseq.py, and trajectory_formatting.py\"\n    )\n</code></pre>"},{"location":"api/prediction/#calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<p>Calculate all possible patient-to-patient transitions.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<pre><code>calculate_all_possible_transitions(\n    data: DataFrame,\n    metadata_selection: DataFrame,\n    distance: str = \"wasserstein\",\n    early_late: bool = False,\n    negative_trajectory: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Calculate all possible patient-to-patient transitions for KIRC cancer.</p> <p>This function computes pairwise distances between all patients at consecutive (or same) cancer stages, considering metadata constraints. Only patients with matching gender and race are considered as potential trajectory pairs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Gene expression data with patients as columns.</p> required <code>metadata_selection</code> <code>DataFrame</code> <p>Clinical metadata with columns: histological_type, race, gender, stage.</p> required <code>distance</code> <code>(wasserstein, euclidean)</code> <p>Distance metric to use for calculating patient-to-patient distances.</p> <code>'wasserstein'</code> <code>early_late</code> <code>bool</code> <p>If True, uses early/late stage groupings. If False, uses I-IV stages.</p> <code>False</code> <code>negative_trajectory</code> <code>bool</code> <p>If True, generates same-stage transitions (negative controls). If False, generates progression transitions (positive trajectories).</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing all possible transitions with columns: - source, target: Patient IDs - source_gender, target_gender: Gender - source_race, target_race: Race - transition: Stage transition label (e.g., '1_to_2', 'early_to_late') - distance: Calculated distance between patients</p> <p>Sorted by gender, race, transition, and distance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance metric is not 'wasserstein' or 'euclidean'.</p> Notes <ul> <li>For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late</li> <li>For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late</li> <li>Only patients with identical gender and race are paired</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def calculate_all_possible_transitions(\n    data: pd.DataFrame,\n    metadata_selection: pd.DataFrame,\n    distance: str = \"wasserstein\",\n    early_late: bool = False,\n    negative_trajectory: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate all possible patient-to-patient transitions for KIRC cancer.\n\n    This function computes pairwise distances between all patients at consecutive\n    (or same) cancer stages, considering metadata constraints. Only patients with\n    matching gender and race are considered as potential trajectory pairs.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Gene expression data with patients as columns.\n    metadata_selection : pd.DataFrame\n        Clinical metadata with columns: histological_type, race, gender, stage.\n    distance : {'wasserstein', 'euclidean'}, default='wasserstein'\n        Distance metric to use for calculating patient-to-patient distances.\n    early_late : bool, default=False\n        If True, uses early/late stage groupings. If False, uses I-IV stages.\n    negative_trajectory : bool, default=False\n        If True, generates same-stage transitions (negative controls).\n        If False, generates progression transitions (positive trajectories).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing all possible transitions with columns:\n        - source, target: Patient IDs\n        - source_gender, target_gender: Gender\n        - source_race, target_race: Race\n        - transition: Stage transition label (e.g., '1_to_2', 'early_to_late')\n        - distance: Calculated distance between patients\n\n        Sorted by gender, race, transition, and distance.\n\n    Raises\n    ------\n    ValueError\n        If distance metric is not 'wasserstein' or 'euclidean'.\n\n    Notes\n    -----\n    - For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late\n    - For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late\n    - Only patients with identical gender and race are paired\n    \"\"\"\n    # Select distance function\n    if distance == \"wasserstein\":\n        from scipy.stats import wasserstein_distance\n\n        distance_fun = wasserstein_distance\n    elif distance == \"euclidean\":\n        from scipy.spatial.distance import euclidean\n\n        distance_fun = euclidean\n    else:\n        raise ValueError(\n            'Distance function not implemented. Use either \"wasserstein\" or \"euclidean\".'\n        )\n\n    # Define stage transitions based on parameters\n    if early_late and not negative_trajectory:\n        possible_transitions = [\"early_to_late\"]\n        stage_pairs = [[\"early\", \"late\"]]\n    elif early_late and negative_trajectory:\n        possible_transitions = [\"early_to_early\", \"late_to_late\"]\n        stage_pairs = [[\"early\", \"early\"], [\"late\", \"late\"]]\n    elif not early_late and not negative_trajectory:\n        possible_transitions = [\"1_to_2\", \"2_to_3\", \"3_to_4\"]\n        stage_pairs = [[\"I\", \"II\"], [\"II\", \"III\"], [\"III\", \"IV\"]]\n    elif not early_late and negative_trajectory:\n        possible_transitions = [\"1_to_1\", \"2_to_2\", \"3_to_3\", \"4_to_4\"]\n        stage_pairs = [[\"I\", \"I\"], [\"II\", \"II\"], [\"III\", \"III\"], [\"IV\", \"IV\"]]\n\n    # Calculate all possible transitions\n    results = []\n    for i_tr, transition in enumerate(possible_transitions):\n        source_target_stage = stage_pairs[i_tr]\n\n        # Iterate through all patient pairs at specified stages\n        for pat_i in metadata_selection.index[\n            metadata_selection[\"stage\"] == source_target_stage[0]\n        ]:\n            for pat_ii in metadata_selection.index[\n                metadata_selection[\"stage\"] == source_target_stage[1]\n            ]:\n                # Extract metadata for both patients\n                source_gender = metadata_selection.at[pat_i, \"gender\"]\n                target_gender = metadata_selection.at[pat_ii, \"gender\"]\n                source_race = metadata_selection.at[pat_i, \"race\"]\n                target_race = metadata_selection.at[pat_ii, \"race\"]\n\n                # Skip if metadata doesn't match (gender and race must match)\n                if not (source_race == target_race and source_gender == target_gender):\n                    continue\n\n                # Store transition information\n                results_i = {\n                    \"source\": pat_i,\n                    \"target\": pat_ii,\n                    \"source_gender\": source_gender,\n                    \"target_gender\": target_gender,\n                    \"source_race\": source_race,\n                    \"target_race\": target_race,\n                    \"transition\": transition,\n                    \"distance\": distance_fun(data[pat_i], data[pat_ii]),\n                }\n                results.append(results_i)\n\n    # Convert to DataFrame and sort\n    results_df = pd.DataFrame(results)\n    results_df.sort_values(\n        [\n            \"source_gender\",\n            \"target_gender\",\n            \"source_race\",\n            \"target_race\",\n            \"transition\",\n            \"distance\",\n        ],\n        inplace=True,\n        ignore_index=True,\n    )\n    return results_df\n</code></pre>"},{"location":"api/prediction/#metadata","title":"Metadata","text":""},{"location":"api/prediction/#get_metadata","title":"get_metadata","text":"<p>Extract metadata from model directory.</p>"},{"location":"api/prediction/#renalprog.modeling.predict.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata(test_path: Path) -&gt; Dict[str, Any]\n</code></pre> <p>Extract metadata from test dataset in SDMetrics format.</p> <p>This function loads a CSV file and extracts its column metadata using SDMetrics' automatic detection. The metadata describes the structure and data types of the dataset, which is required for SDMetrics quality evaluation.</p> <p>Args:     test_path: Path to the CSV file containing the test dataset.                Can be a string or Path object.</p> <p>Returns:     Dictionary containing metadata with column names and data types.     Format: {'columns': {col_name: {'sdtype': type}}}</p> <p>Note:     - The CSV is loaded with index_col=0 to avoid treating the index as a feature     - Both real and synthetic data must share the same metadata structure     - This ensures SDMetrics can properly validate and compare the datasets</p> <p>Example:     &gt;&gt;&gt; metadata = get_metadata(\"data/X_test.csv\")     &gt;&gt;&gt; print(metadata['columns'].keys())     dict_keys(['gene1', 'gene2', ...])</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def get_metadata(test_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract metadata from test dataset in SDMetrics format.\n\n    This function loads a CSV file and extracts its column metadata using SDMetrics'\n    automatic detection. The metadata describes the structure and data types of\n    the dataset, which is required for SDMetrics quality evaluation.\n\n    Args:\n        test_path: Path to the CSV file containing the test dataset.\n                   Can be a string or Path object.\n\n    Returns:\n        Dictionary containing metadata with column names and data types.\n        Format: {'columns': {col_name: {'sdtype': type}}}\n\n    Note:\n        - The CSV is loaded with index_col=0 to avoid treating the index as a feature\n        - Both real and synthetic data must share the same metadata structure\n        - This ensures SDMetrics can properly validate and compare the datasets\n\n    Example:\n        &gt;&gt;&gt; metadata = get_metadata(\"data/X_test.csv\")\n        &gt;&gt;&gt; print(metadata['columns'].keys())\n        dict_keys(['gene1', 'gene2', ...])\n    \"\"\"\n    from pathlib import Path as pathlib_Path\n\n    logger.info(\"Extracting metadata for SDMetrics evaluation\")\n    logger.info(f\"Loading data from: {test_path}\")\n\n    # Convert to Path object for consistent handling across platforms\n    test_path = pathlib_Path(test_path)\n\n    # Load data using SDMetrics CSV handler\n    # CRITICAL: index_col=0 prevents the index from being treated as a feature column\n    # This would cause metadata mismatch errors if the index is included\n    connector = CSVHandler()\n    real_data = connector.read(\n        folder_name=str(test_path.parent),\n        file_names=[test_path.name],\n        read_csv_parameters={\n            \"index_col\": 0,  # Use first column as index, not as feature\n            \"parse_dates\": False,  # Don't parse dates (all numeric gene expression)\n            \"encoding\": \"latin-1\",  # Standard encoding for CSV files\n        },\n    )\n\n    # Auto-detect metadata from the loaded data\n    metadata = Metadata.detect_from_dataframes(data=real_data)\n\n    # Extract table-specific metadata (removes wrapper structure)\n    # The key 'X_test' matches the filename without extension\n    metadata_use = metadata.to_dict()[\"tables\"][\"X_test\"]\n\n    logger.info(f\"Extracted metadata for {len(metadata_use['columns'])} genes\")\n\n    return metadata_use\n</code></pre>"},{"location":"api/prediction/#complete-example","title":"Complete Example","text":"<pre><code>import torch\nimport pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE\nfrom renalprog.modeling.predict import (\n    apply_vae,\n    generate_trajectories,\n    evaluate_reconstruction\n)\nfrom renalprog.plots import  plot_trajectory\n\n# Load model and data\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel.load_state_dict(torch.load(\"models/my_vae/best_model.pt\"))\n\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\nclinical = pd.read_csv(\"data/interim/split/test_clinical.tsv\", sep=\"\\t\", index_col=0)\n\n# Encode data\ntrain_results = apply_vae(model, train_expr.values, device='cuda')\ntest_results = apply_vae(model, test_expr.values, device='cuda')\n\n# Create patient connections\nearly_mask = clinical['stage'] == 'early'\nlate_mask = clinical['stage'] == 'late'\n\n# Generate trajectories\ntrajectories = generate_trajectories(\n    model=model,\n    start_data=test_expr.values[early_mask],\n    end_data=test_expr.values[late_mask],\n    n_steps=50,\n    interpolation='spherical',\n    device='cuda'\n)\n\n# Evaluate reconstruction\nmetrics = evaluate_reconstruction(\n    model=model,\n    original_data=test_expr.values,\n    device='cuda',\n    output_dir=Path(\"reports/reconstruction\")\n)\n\nprint(f\"Generated {len(trajectories)} trajectories\")\nprint(f\"Reconstruction MSE: {metrics['mse']:.4f}\")\n</code></pre>"},{"location":"api/prediction/#see-also","title":"See Also","text":"<ul> <li>Training API - Train VAE models</li> <li>Trajectories API - Trajectory analysis</li> <li>Models API - VAE architectures</li> </ul>"},{"location":"api/training/","title":"Training API","text":"<p>Complete training pipeline for VAE models with checkpointing and monitoring.</p>"},{"location":"api/training/#overview","title":"Overview","text":"<p>The training module provides high-level functions for:</p> <ul> <li>Complete VAE training workflow</li> <li>Automatic checkpointing</li> <li>Training history visualization</li> <li>Early stopping</li> <li>Learning rate scheduling</li> </ul>"},{"location":"api/training/#main-training-function","title":"Main Training Function","text":""},{"location":"api/training/#train_vae","title":"train_vae","text":"<p>The main training function that orchestrates the entire VAE training process.</p>"},{"location":"api/training/#renalprog.modeling.train.train_vae","title":"train_vae","text":"<pre><code>train_vae(\n    X_train: ndarray,\n    X_test: ndarray,\n    y_train: Optional[ndarray] = None,\n    y_test: Optional[ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]\n</code></pre> <p>Train a VAE model with full checkpointing support.</p> <p>Args:     X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame     X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame     y_train: Optional training labels for CVAE     y_test: Optional test labels for CVAE     config: Training configuration     save_dir: Directory to save checkpoints     resume_from: Optional checkpoint path to resume training     force_cpu: Force CPU usage even if CUDA is available (for compatibility)</p> <p>Returns:     Tuple of (trained_model, training_history)</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def train_vae(\n    X_train: np.ndarray,\n    X_test: np.ndarray,\n    y_train: Optional[np.ndarray] = None,\n    y_test: Optional[np.ndarray] = None,\n    config: Optional[VAEConfig] = None,\n    save_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    force_cpu: bool = False,\n) -&gt; Tuple[nn.Module, Dict[str, list]]:\n    \"\"\"Train a VAE model with full checkpointing support.\n\n    Args:\n        X_train: Training data (samples \u00d7 features) - numpy array or pandas DataFrame\n        X_test: Test data (samples \u00d7 features) - numpy array or pandas DataFrame\n        y_train: Optional training labels for CVAE\n        y_test: Optional test labels for CVAE\n        config: Training configuration\n        save_dir: Directory to save checkpoints\n        resume_from: Optional checkpoint path to resume training\n        force_cpu: Force CPU usage even if CUDA is available (for compatibility)\n\n    Returns:\n        Tuple of (trained_model, training_history)\n    \"\"\"\n    # Convert DataFrames to numpy arrays if needed\n    if hasattr(X_train, \"values\"):  # Check if it's a DataFrame\n        X_train = X_train.values\n    if hasattr(X_test, \"values\"):  # Check if it's a DataFrame\n        X_test = X_test.values\n    if y_train is not None and hasattr(y_train, \"values\"):\n        y_train = y_train.values\n    if y_test is not None and hasattr(y_test, \"values\"):\n        y_test = y_test.values\n\n    if config is None:\n        config = VAEConfig()\n        config.INPUT_DIM = X_train.shape[1]\n\n    set_seed(config.SEED)\n\n    # Setup save directory\n    if save_dir is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d\")\n        save_dir = Path(f\"models/{timestamp}_VAE_KIRC\")\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save config\n    save_model_config(config, save_dir / \"config.json\")\n\n    # Setup device\n    device = get_device(force_cpu=force_cpu)\n    logger.info(f\"Using device: {device}\")\n\n    # Initialize model\n    model = VAE(\n        input_dim=config.INPUT_DIM,\n        mid_dim=config.MID_DIM,\n        features=config.LATENT_DIM,\n    ).to(device)\n\n    logger.info(\n        f\"Model: VAE(input_dim={config.INPUT_DIM}, mid_dim={config.MID_DIM}, latent_dim={config.LATENT_DIM})\"\n    )\n    logger.info(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    # Setup optimizer\n    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n\n    # Setup checkpointer\n    checkpointer = ModelCheckpointer(\n        save_dir=save_dir,\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_freq=config.CHECKPOINT_FREQ,\n        keep_last_n=3,\n    )\n\n    # Resume from checkpoint if provided\n    start_epoch = 0\n    if resume_from is not None:\n        checkpoint_info = checkpointer.load_checkpoint(\n            resume_from, model, optimizer, device=str(device)\n        )\n        start_epoch = checkpoint_info[\"epoch\"] + 1\n        logger.info(f\"Resuming training from epoch {start_epoch}\")\n\n    # Create dataloaders\n    train_loader = create_dataloader(X_train, y_train, config.BATCH_SIZE, shuffle=True)\n    test_loader = create_dataloader(X_test, y_test, config.BATCH_SIZE, shuffle=False)\n\n    # Training history\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"train_recon_loss\": [],\n        \"train_kl_loss\": [],\n        \"val_recon_loss\": [],\n        \"val_kl_loss\": [],\n        \"beta_schedule\": [],  # Track beta values\n    }\n\n    # Setup beta annealing schedule\n    if config.USE_BETA_ANNEALING:\n        beta_schedule = frange_cycle_linear(\n            start=config.BETA_START,\n            stop=config.BETA,\n            n_epoch=config.EPOCHS,\n            n_cycle=config.BETA_CYCLES,\n            ratio=config.BETA_RATIO,\n        )\n        logger.info(\n            f\"Using cyclical beta annealing: \"\n            f\"{config.BETA_START} -&gt; {config.BETA} over {config.BETA_CYCLES} cycles\"\n        )\n    else:\n        # Constant beta\n        beta_schedule = np.ones(config.EPOCHS) * config.BETA\n        logger.info(f\"Using constant beta: {config.BETA}\")\n\n    # Training loop\n    logger.info(f\"Starting training for {config.EPOCHS} epochs\")\n\n    # Add epoch progress bar\n    epoch_pbar = tqdm(range(start_epoch, config.EPOCHS), desc=\"Epochs\", position=0)\n    for epoch in epoch_pbar:\n        # Get beta for this epoch from schedule\n        current_beta = beta_schedule[epoch]\n\n        # Train\n        train_metrics = train_epoch(\n            model, train_loader, optimizer, device, config, beta=current_beta\n        )\n\n        # Validate\n        val_metrics = evaluate_model(\n            model, test_loader, device, config, beta=current_beta\n        )\n\n        # Update history\n        history[\"train_loss\"].append(train_metrics[\"loss\"])\n        history[\"val_loss\"].append(val_metrics[\"loss\"])\n        history[\"train_recon_loss\"].append(train_metrics[\"recon_loss\"])\n        history[\"train_kl_loss\"].append(train_metrics[\"kl_loss\"])\n        history[\"val_recon_loss\"].append(val_metrics[\"recon_loss\"])\n        history[\"val_kl_loss\"].append(val_metrics[\"kl_loss\"])\n        history[\"beta_schedule\"].append(float(current_beta))\n\n        # Update epoch progress bar\n        epoch_pbar.set_postfix(\n            {\n                \"train_loss\": f\"{train_metrics['loss']:.4f}\",\n                \"val_loss\": f\"{val_metrics['loss']:.4f}\",\n                \"beta\": f\"{current_beta:.3f}\",\n            }\n        )\n\n        # Log progress\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            logger.info(\n                f\"Epoch {epoch + 1}/{config.EPOCHS} - \"\n                f\"train_loss: {train_metrics['loss']:.4f}, \"\n                f\"val_loss: {val_metrics['loss']:.4f}\"\n            )\n\n        # Combine metrics for checkpointing\n        current_metrics = {\n            \"train_loss\": train_metrics[\"loss\"],\n            \"val_loss\": val_metrics[\"loss\"],\n            \"train_recon\": train_metrics[\"recon_loss\"],\n            \"train_kl\": train_metrics[\"kl_loss\"],\n            \"val_recon\": val_metrics[\"recon_loss\"],\n            \"val_kl\": val_metrics[\"kl_loss\"],\n        }\n\n        # # Save periodic checkpoint\n        # if checkpointer.should_save_checkpoint(epoch):\n        #     checkpointer.save_checkpoint(\n        #         epoch, model, optimizer, current_metrics, config\n        #     )\n\n    # Save final model\n    checkpointer.save_checkpoint(\n        config.EPOCHS - 1, model, optimizer, current_metrics, config, is_final=True\n    )\n\n    logger.info(\"Training complete!\")\n\n    return model, history\n</code></pre>"},{"location":"api/training/#key-features","title":"Key Features","text":""},{"location":"api/training/#automatic-checkpointing","title":"Automatic Checkpointing","text":"<p>The training function automatically saves:</p> <ul> <li>Model state dict</li> <li>Optimizer state</li> <li>Training history</li> <li>Configuration parameters</li> </ul> <p>Checkpoints are saved when: - Validation loss improves (best model) - At regular intervals (every <code>checkpoint_every</code> epochs) - After training completes (final model)</p>"},{"location":"api/training/#early-stopping","title":"Early Stopping","text":"<p>Training stops automatically if validation loss doesn't improve for <code>early_stopping_patience</code> epochs. This prevents overfitting and saves computation time.</p>"},{"location":"api/training/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>When <code>use_scheduler=True</code>, the learning rate is reduced when validation loss plateaus:</p> <pre><code>scheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=10,\n    verbose=True\n)\n</code></pre>"},{"location":"api/training/#cyclical-kl-annealing","title":"Cyclical KL Annealing","text":"<p>The KL divergence weight \u03b2 is gradually increased using cyclical annealing to prevent posterior collapse:</p>"},{"location":"api/training/#renalprog.modeling.train.frange_cycle_linear","title":"frange_cycle_linear","text":"<pre><code>frange_cycle_linear(\n    start: float,\n    stop: float,\n    n_epoch: int,\n    n_cycle: int = 4,\n    ratio: float = 0.5,\n) -&gt; np.ndarray\n</code></pre> <p>Generate a linear cyclical schedule for beta hyperparameter.</p> <p>This creates a cyclical annealing schedule where beta increases linearly from start to stop over a portion of each cycle (controlled by ratio), then stays constant at stop for the remainder of the cycle.</p> <p>Args:     start: Initial value of beta (typically 0.0)     stop: Final/maximum value of beta (typically 1.0)     n_epoch: Total number of epochs     n_cycle: Number of cycles (default: 4)     ratio: Ratio of cycle spent increasing beta (default: 0.5)            - 0.5 means half cycle increasing, half constant            - 1.0 means entire cycle increasing</p> <p>Returns:     Array of beta values for each epoch</p> <p>Example:     &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle     &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)     &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0     &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0     &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0     &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0</p> Source code in <code>renalprog/modeling/train.py</code> <pre><code>def frange_cycle_linear(\n    start: float, stop: float, n_epoch: int, n_cycle: int = 4, ratio: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a linear cyclical schedule for beta hyperparameter.\n\n    This creates a cyclical annealing schedule where beta increases linearly\n    from start to stop over a portion of each cycle (controlled by ratio),\n    then stays constant at stop for the remainder of the cycle.\n\n    Args:\n        start: Initial value of beta (typically 0.0)\n        stop: Final/maximum value of beta (typically 1.0)\n        n_epoch: Total number of epochs\n        n_cycle: Number of cycles (default: 4)\n        ratio: Ratio of cycle spent increasing beta (default: 0.5)\n               - 0.5 means half cycle increasing, half constant\n               - 1.0 means entire cycle increasing\n\n    Returns:\n        Array of beta values for each epoch\n\n    Example:\n        &gt;&gt;&gt; # 3 cycles over 300 epochs, beta increases from 0 to 1 over first half of each cycle\n        &gt;&gt;&gt; beta_schedule = frange_cycle_linear(0.0, 1.0, 300, n_cycle=3, ratio=0.5)\n        &gt;&gt;&gt; # Epoch 0-50: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 50-100: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 100-150: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 150-200: beta stays at 1.0\n        &gt;&gt;&gt; # Epoch 200-250: beta increases 0.0 -&gt; 1.0\n        &gt;&gt;&gt; # Epoch 250-300: beta stays at 1.0\n    \"\"\"\n    L = np.ones(n_epoch) * stop  # Initialize all to stop value\n    period = n_epoch / n_cycle\n    step = (stop - start) / (period * ratio)  # Linear schedule\n\n    for c in range(n_cycle):\n        v, i = start, 0\n        while v &lt;= stop and (int(i + c * period) &lt; n_epoch):\n            L[int(i + c * period)] = v\n            v += step\n            i += 1\n\n    return L\n</code></pre>"},{"location":"api/training/#training-history","title":"Training History","text":"<p>The training function returns a dictionary with:</p> Key Description <code>train_loss</code> Training loss per epoch <code>val_loss</code> Validation loss per epoch <code>train_recon</code> Training reconstruction loss per epoch <code>val_recon</code> Validation reconstruction loss per epoch <code>train_kl</code> Training KL divergence per epoch <code>val_kl</code> Validation KL divergence per epoch <code>learning_rates</code> Learning rate per epoch"},{"location":"api/training/#complete-example","title":"Complete Example","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\nfrom renalprog.modeling.train import train_vae\nfrom renalprog.plots import plot_training_history\nfrom renalprog.utils import set_seed, configure_logging\n\n# Configure\nconfigure_logging()\nset_seed(42)\n\n# Load data\ntrain_expr = pd.read_csv(\"data/interim/split/train_expression.tsv\", sep=\"\\t\", index_col=0)\ntest_expr = pd.read_csv(\"data/interim/split/test_expression.tsv\", sep=\"\\t\", index_col=0)\n\n# Train VAE\nhistory, best_model, checkpoints = train_vae(\n    train_data=train_expr.values,\n    val_data=test_expr.values,\n    input_dim=train_expr.shape[1],\n    mid_dim=1024,\n    features=128,\n    output_dir=Path(\"models/my_vae\"),\n    n_epochs=100,\n    batch_size=32,\n    learning_rate=1e-3,\n    use_scheduler=True,\n    use_checkpoint=True,\n    checkpoint_every=10,\n    early_stopping_patience=20,\n    device='cuda'\n)\n\n# Plot results\nplot_training_history(\n    history,\n    output_path=Path(\"reports/figures/training_history.png\")\n)\n\n# Load best model for inference\nbest_model.eval()\nimport torch\nwith torch.no_grad():\n    reconstruction, mu, log_var, z = best_model(\n        torch.FloatTensor(test_expr.values).to(device)\n    )\n\nprint(f\"Best validation loss: {min(history['val_loss']):.4f}\")\nprint(f\"Final learning rate: {history['learning_rates'][-1]:.6f}\")\n</code></pre>"},{"location":"api/training/#checkpointing-api","title":"Checkpointing API","text":"<p>For manual checkpoint management:</p>"},{"location":"api/training/#renalprog.modeling.checkpointing","title":"checkpointing","text":"<p>Model checkpointing utilities for saving and loading training state.</p>"},{"location":"api/training/#renalprog.modeling.checkpointing-classes","title":"Classes","text":""},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer","title":"ModelCheckpointer","text":"<pre><code>ModelCheckpointer(\n    save_dir: Path,\n    monitor: str = \"val_loss\",\n    mode: str = \"min\",\n    save_freq: int = 0,\n    keep_last_n: int = 3,\n)\n</code></pre> <p>Handles saving and loading model checkpoints during training.</p> <p>Features: - Save best model based on validation metric - Save checkpoints every N epochs - Save final model after training - Save training history and configuration - Resume training from checkpoint</p> <p>Attributes:     save_dir: Directory to save checkpoints     monitor: Metric to monitor ('loss', 'val_loss', etc.)     mode: 'min' for loss, 'max' for accuracy     save_freq: Save checkpoint every N epochs (0 = only best)     keep_last_n: Keep only last N checkpoints (0 = keep all)</p> <p>Initialize checkpointer.</p> <p>Args:     save_dir: Directory to save checkpoints     monitor: Metric name to monitor     mode: 'min' to minimize metric, 'max' to maximize     save_freq: Save every N epochs (0 = only save best)     keep_last_n: Keep only N most recent checkpoints (0 = all)</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def __init__(\n    self,\n    save_dir: Path,\n    monitor: str = \"val_loss\",\n    mode: str = \"min\",\n    save_freq: int = 0,\n    keep_last_n: int = 3,\n):\n    \"\"\"Initialize checkpointer.\n\n    Args:\n        save_dir: Directory to save checkpoints\n        monitor: Metric name to monitor\n        mode: 'min' to minimize metric, 'max' to maximize\n        save_freq: Save every N epochs (0 = only save best)\n        keep_last_n: Keep only N most recent checkpoints (0 = all)\n    \"\"\"\n    self.save_dir = Path(save_dir)\n    self.save_dir.mkdir(parents=True, exist_ok=True)\n\n    self.monitor = monitor\n    self.mode = mode\n    self.save_freq = save_freq\n    self.keep_last_n = keep_last_n\n\n    # Track best metric\n    self.best_metric = float(\"inf\") if mode == \"min\" else float(\"-inf\")\n    self.best_epoch = 0\n\n    # Track saved checkpoints for cleanup\n    self.checkpoint_history = []\n\n    logger.info(f\"ModelCheckpointer initialized: {save_dir}\")\n    logger.info(f\"Monitoring: {monitor} ({mode})\")\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer-functions","title":"Functions","text":""},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.get_best_checkpoint_path","title":"get_best_checkpoint_path","text":"<pre><code>get_best_checkpoint_path() -&gt; Optional[Path]\n</code></pre> <p>Get path to best model checkpoint.</p> <p>Returns:     Path to best model, or None if not saved yet</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def get_best_checkpoint_path(self) -&gt; Optional[Path]:\n    \"\"\"Get path to best model checkpoint.\n\n    Returns:\n        Path to best model, or None if not saved yet\n    \"\"\"\n    best_path = self.save_dir / \"best_model.pth\"\n    return best_path if best_path.exists() else None\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.get_final_checkpoint_path","title":"get_final_checkpoint_path","text":"<pre><code>get_final_checkpoint_path() -&gt; Optional[Path]\n</code></pre> <p>Get path to final model checkpoint.</p> <p>Returns:     Path to final model, or None if not saved yet</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def get_final_checkpoint_path(self) -&gt; Optional[Path]:\n    \"\"\"Get path to final model checkpoint.\n\n    Returns:\n        Path to final model, or None if not saved yet\n    \"\"\"\n    final_path = self.save_dir / \"final_model.pth\"\n    return final_path if final_path.exists() else None\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(\n    checkpoint_path: Path,\n    model: Module,\n    optimizer: Optional[Optimizer] = None,\n    device: str = \"cpu\",\n) -&gt; Dict[str, Any]\n</code></pre> <p>Load a checkpoint and restore model state.</p> <p>Args:     checkpoint_path: Path to checkpoint file     model: Model to load state into     optimizer: Optional optimizer to restore state     device: Device to map checkpoint to</p> <p>Returns:     Dictionary with checkpoint information (epoch, metrics, config)</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def load_checkpoint(\n    self,\n    checkpoint_path: Path,\n    model: nn.Module,\n    optimizer: Optional[Optimizer] = None,\n    device: str = \"cpu\",\n) -&gt; Dict[str, Any]:\n    \"\"\"Load a checkpoint and restore model state.\n\n    Args:\n        checkpoint_path: Path to checkpoint file\n        model: Model to load state into\n        optimizer: Optional optimizer to restore state\n        device: Device to map checkpoint to\n\n    Returns:\n        Dictionary with checkpoint information (epoch, metrics, config)\n    \"\"\"\n    if not checkpoint_path.exists():\n        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n\n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    # Restore model state\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    logger.info(f\"Loaded model state from epoch {checkpoint['epoch']}\")\n\n    # Restore optimizer state if provided\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        logger.info(\"Loaded optimizer state\")\n\n    # Return checkpoint info\n    return {\n        \"epoch\": checkpoint[\"epoch\"],\n        \"metrics\": checkpoint.get(\"metrics\", {}),\n        \"config\": checkpoint.get(\"config\", {}),\n        \"best_metric\": checkpoint.get(\"best_metric\", self.best_metric),\n    }\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    epoch: int,\n    model: Module,\n    optimizer: Optimizer,\n    metrics: Dict[str, float],\n    config: Any,\n    is_best: bool = False,\n    is_final: bool = False,\n) -&gt; None\n</code></pre> <p>Save a training checkpoint.</p> <p>Args:     epoch: Current epoch number     model: PyTorch model to save     optimizer: Optimizer state to save     metrics: Dictionary of current metrics     config: Training configuration object     is_best: Whether this is the best model so far     is_final: Whether this is the final model</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def save_checkpoint(\n    self,\n    epoch: int,\n    model: nn.Module,\n    optimizer: Optimizer,\n    metrics: Dict[str, float],\n    config: Any,\n    is_best: bool = False,\n    is_final: bool = False,\n) -&gt; None:\n    \"\"\"Save a training checkpoint.\n\n    Args:\n        epoch: Current epoch number\n        model: PyTorch model to save\n        optimizer: Optimizer state to save\n        metrics: Dictionary of current metrics\n        config: Training configuration object\n        is_best: Whether this is the best model so far\n        is_final: Whether this is the final model\n    \"\"\"\n    # Create checkpoint dictionary\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"metrics\": metrics,\n        \"config\": self._config_to_dict(config),\n        \"best_metric\": self.best_metric,\n        \"monitor\": self.monitor,\n    }\n\n    # Determine checkpoint filename\n    if is_final:\n        filename = \"final_model.pth\"\n    elif is_best:\n        filename = \"best_model.pth\"\n    else:\n        filename = f\"checkpoint_epoch_{epoch:04d}.pth\"\n\n    checkpoint_path = self.save_dir / filename\n\n    # Save checkpoint\n    torch.save(checkpoint, checkpoint_path)\n    logger.info(f\"Saved checkpoint: {checkpoint_path}\")\n\n    # Track for cleanup\n    if not is_best and not is_final:\n        self.checkpoint_history.append(checkpoint_path)\n        self._cleanup_old_checkpoints()\n\n    # Save training history as JSON\n    if is_best or is_final:\n        self._save_history(metrics, epoch)\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.should_save_checkpoint","title":"should_save_checkpoint","text":"<pre><code>should_save_checkpoint(epoch: int) -&gt; bool\n</code></pre> <p>Determine if checkpoint should be saved this epoch.</p> <p>Args:     epoch: Current epoch number</p> <p>Returns:     True if checkpoint should be saved</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def should_save_checkpoint(self, epoch: int) -&gt; bool:\n    \"\"\"Determine if checkpoint should be saved this epoch.\n\n    Args:\n        epoch: Current epoch number\n\n    Returns:\n        True if checkpoint should be saved\n    \"\"\"\n    if self.save_freq == 0:\n        return False\n    return epoch % self.save_freq == 0\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.ModelCheckpointer.update_best","title":"update_best","text":"<pre><code>update_best(epoch: int, metric_value: float) -&gt; bool\n</code></pre> <p>Check if current metric is the best and update if so.</p> <p>Args:     epoch: Current epoch number     metric_value: Current metric value</p> <p>Returns:     True if this is a new best, False otherwise</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def update_best(self, epoch: int, metric_value: float) -&gt; bool:\n    \"\"\"Check if current metric is the best and update if so.\n\n    Args:\n        epoch: Current epoch number\n        metric_value: Current metric value\n\n    Returns:\n        True if this is a new best, False otherwise\n    \"\"\"\n    is_better = (self.mode == \"min\" and metric_value &lt; self.best_metric) or (\n        self.mode == \"max\" and metric_value &gt; self.best_metric\n    )\n\n    if is_better:\n        self.best_metric = metric_value\n        self.best_epoch = epoch\n        logger.info(f\"New best {self.monitor}: {metric_value:.6f} at epoch {epoch}\")\n        return True\n    return False\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing-functions","title":"Functions","text":""},{"location":"api/training/#renalprog.modeling.checkpointing.load_model_config","title":"load_model_config","text":"<pre><code>load_model_config(config_path: Path) -&gt; Dict[str, Any]\n</code></pre> <p>Load model configuration from JSON file.</p> <p>Args:     config_path: Path to JSON config file</p> <p>Returns:     Dictionary with configuration</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def load_model_config(config_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"Load model configuration from JSON file.\n\n    Args:\n        config_path: Path to JSON config file\n\n    Returns:\n        Dictionary with configuration\n    \"\"\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config not found: {config_path}\")\n\n    with open(config_path, \"r\") as f:\n        config = json.load(f)\n\n    logger.info(f\"Loaded config: {config_path}\")\n    return config\n</code></pre>"},{"location":"api/training/#renalprog.modeling.checkpointing.save_model_config","title":"save_model_config","text":"<pre><code>save_model_config(config: Any, save_path: Path) -&gt; None\n</code></pre> <p>Save model configuration to JSON file.</p> <p>Args:     config: Configuration object     save_path: Path to save JSON file</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def save_model_config(config: Any, save_path: Path) -&gt; None:\n    \"\"\"Save model configuration to JSON file.\n\n    Args:\n        config: Configuration object\n        save_path: Path to save JSON file\n    \"\"\"\n    config_dict = {}\n    if hasattr(config, \"__dict__\"):\n        config_dict = {\n            k: v\n            for k, v in config.__dict__.items()\n            if not k.startswith(\"_\") and not callable(v)\n        }\n\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(save_path, \"w\") as f:\n        json.dump(config_dict, f, indent=2)\n\n    logger.info(f\"Saved config: {save_path}\")\n</code></pre>"},{"location":"api/training/#save_checkpoint","title":"save_checkpoint","text":"<p>Save model checkpoint with metadata.</p>"},{"location":"api/training/#renalprog.modeling.checkpointing.save_model_config","title":"save_model_config","text":"<pre><code>save_model_config(config: Any, save_path: Path) -&gt; None\n</code></pre> <p>Save model configuration to JSON file.</p> <p>Args:     config: Configuration object     save_path: Path to save JSON file</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def save_model_config(config: Any, save_path: Path) -&gt; None:\n    \"\"\"Save model configuration to JSON file.\n\n    Args:\n        config: Configuration object\n        save_path: Path to save JSON file\n    \"\"\"\n    config_dict = {}\n    if hasattr(config, \"__dict__\"):\n        config_dict = {\n            k: v\n            for k, v in config.__dict__.items()\n            if not k.startswith(\"_\") and not callable(v)\n        }\n\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(save_path, \"w\") as f:\n        json.dump(config_dict, f, indent=2)\n\n    logger.info(f\"Saved config: {save_path}\")\n</code></pre>"},{"location":"api/training/#load_checkpoint","title":"load_checkpoint","text":"<p>Load model from checkpoint.</p>"},{"location":"api/training/#renalprog.modeling.checkpointing.load_model_config","title":"load_model_config","text":"<pre><code>load_model_config(config_path: Path) -&gt; Dict[str, Any]\n</code></pre> <p>Load model configuration from JSON file.</p> <p>Args:     config_path: Path to JSON config file</p> <p>Returns:     Dictionary with configuration</p> Source code in <code>renalprog/modeling/checkpointing.py</code> <pre><code>def load_model_config(config_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"Load model configuration from JSON file.\n\n    Args:\n        config_path: Path to JSON config file\n\n    Returns:\n        Dictionary with configuration\n    \"\"\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config not found: {config_path}\")\n\n    with open(config_path, \"r\") as f:\n        config = json.load(f)\n\n    logger.info(f\"Loaded config: {config_path}\")\n    return config\n</code></pre>"},{"location":"api/training/#see-also","title":"See Also","text":"<ul> <li>Models API - VAE architectures</li> <li>Prediction API - Using trained models</li> <li>Configuration - Training hyperparameters</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/trajectories/","title":"Trajectories API","text":"<p>Functions for analyzing disease progression trajectories.</p>"},{"location":"api/trajectories/#overview","title":"Overview","text":"<p>The trajectories module provides analysis tools for:</p> <ul> <li>Trajectory network construction</li> <li>Patient connectivity analysis</li> <li>Temporal pathway enrichment</li> <li>Transition probability calculation</li> <li>Trajectory visualization</li> </ul>"},{"location":"api/trajectories/#network-construction","title":"Network Construction","text":""},{"location":"api/trajectories/#build_trajectory_network","title":"build_trajectory_network","text":"<p>Build directed graph of patient transitions.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import build_trajectory_network\nimport pandas as pd\nfrom pathlib import Path\n\n# Load patient connections\nconnections = pd.read_csv(\"data/processed/patient_connections.csv\")\n\n# Build network\nnetwork = build_trajectory_network(\n    connections=connections,\n    output_path=Path(\"data/processed/trajectory_network.graphml\")\n)\n\nprint(f\"Network has {network.number_of_nodes()} nodes\")\nprint(f\"Network has {network.number_of_edges()} edges\")\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.build_trajectory_network","title":"build_trajectory_network","text":"<pre><code>build_trajectory_network(\n    patient_links: DataFrame,\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]\n</code></pre> <p>Build trajectory network and find all complete disease progression paths.</p> <p>Constructs a directed graph from patient links and identifies all possible complete trajectories from root nodes (earliest stage patients not appearing as targets) to leaf nodes (latest stage patients not appearing as sources).</p> <p>Args:     patient_links: DataFrame with 'source' and 'target' columns from linking functions</p> <p>Returns:     Tuple of:     - network: Dict mapping each source patient to list of target patients     - trajectories: List of complete trajectories, where each trajectory is a                     list of patient IDs ordered from earliest to latest stage</p> <p>Network Structure:     - Adjacency list representation: {source: [target1, target2, ...]}     - Directed edges from earlier to later stages     - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)</p> <p>Trajectory Discovery:     - Uses depth-first search from root nodes     - Root nodes: Patients in 'source' but not in 'target' (stage I or early)     - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)     - Each trajectory represents a complete disease progression path</p> <p>Example:     &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)     &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")     &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")     &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")     Network has 500 nodes     Found 234 complete trajectories     Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']</p> <p>Trajectory Characteristics:     - Length varies based on how many stages the path spans     - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions     - Length 2 for early\u2192late progressions     - Patients can appear in multiple trajectories</p> <p>Note:     - Cycles are prevented during trajectory search     - All paths from root to leaf are enumerated     - Trajectories respect chronological disease progression</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def build_trajectory_network(\n    patient_links: pd.DataFrame,\n) -&gt; Tuple[Dict[str, List[str]], List[List[str]]]:\n    \"\"\"\n    Build trajectory network and find all complete disease progression paths.\n\n    Constructs a directed graph from patient links and identifies all possible\n    complete trajectories from root nodes (earliest stage patients not appearing\n    as targets) to leaf nodes (latest stage patients not appearing as sources).\n\n    Args:\n        patient_links: DataFrame with 'source' and 'target' columns from linking functions\n\n    Returns:\n        Tuple of:\n        - network: Dict mapping each source patient to list of target patients\n        - trajectories: List of complete trajectories, where each trajectory is a\n                        list of patient IDs ordered from earliest to latest stage\n\n    Network Structure:\n        - Adjacency list representation: {source: [target1, target2, ...]}\n        - Directed edges from earlier to later stages\n        - Allows multiple outgoing edges (one patient \u2192 multiple next-stage patients)\n\n    Trajectory Discovery:\n        - Uses depth-first search from root nodes\n        - Root nodes: Patients in 'source' but not in 'target' (stage I or early)\n        - Leaf nodes: Patients in 'target' but not in 'source' (stage IV or late)\n        - Each trajectory represents a complete disease progression path\n\n    Example:\n        &gt;&gt;&gt; network, trajectories = build_trajectory_network(patient_links)\n        &gt;&gt;&gt; print(f\"Network has {len(network)} nodes\")\n        &gt;&gt;&gt; print(f\"Found {len(trajectories)} complete trajectories\")\n        &gt;&gt;&gt; print(f\"Example trajectory: {trajectories[0]}\")\n        Network has 500 nodes\n        Found 234 complete trajectories\n        Example trajectory: ['PAT001', 'PAT045', 'PAT123', 'PAT289']\n\n    Trajectory Characteristics:\n        - Length varies based on how many stages the path spans\n        - Typical lengths: 2-4 patients for I\u2192II\u2192III\u2192IV progressions\n        - Length 2 for early\u2192late progressions\n        - Patients can appear in multiple trajectories\n\n    Note:\n        - Cycles are prevented during trajectory search\n        - All paths from root to leaf are enumerated\n        - Trajectories respect chronological disease progression\n    \"\"\"\n    logger.info(\"Building trajectory network from patient links\")\n\n    sources = patient_links[\"source\"]\n    targets = patient_links[\"target\"]\n\n    # Build network adjacency list\n    network = {}\n    for source, target in zip(sources, targets):\n        if source not in network:\n            network[source] = []\n        network[source].append(target)\n\n    logger.info(f\"Network built: {len(network)} source nodes\")\n\n    # Find root nodes (patients who are sources but never targets)\n    unique_sources = set(sources) - set(targets)\n    logger.info(f\"Found {len(unique_sources)} root nodes (earliest stage patients)\")\n\n    # Recursively find all trajectories from each root\n    def find_trajectories(\n        start_node: str, visited: Optional[List[str]] = None\n    ) -&gt; List[List[str]]:\n        \"\"\"Depth-first search to find all paths from start_node to leaf nodes.\"\"\"\n        if visited is None:\n            visited = []\n\n        visited.append(start_node)\n\n        # If node has no outgoing edges, this is a leaf node - return path\n        if start_node not in network:\n            return [visited]\n\n        # Recursively explore all targets\n        trajectories = []\n        for target in network[start_node]:\n            if target not in visited:  # Avoid cycles\n                new_visited = visited.copy()\n                trajectories.extend(find_trajectories(target, new_visited))\n\n        return trajectories\n\n    # Find all trajectories starting from each root\n    all_trajectories = []\n\n    if len(unique_sources) == 0:\n        # No clear root nodes - this happens with early\u2192late transitions where\n        # patients can be both sources and targets. In this case, each source\u2192target\n        # pair is already a complete 2-patient trajectory.\n        logger.info(\"No root nodes found (typical for early\u2192late transitions).\")\n        logger.info(\"Using each source\u2192target pair as a complete trajectory.\")\n        for source, target in zip(sources, targets):\n            all_trajectories.append([source, target])\n    else:\n        # Standard case: multi-stage progressions (I\u2192II\u2192III\u2192IV)\n        for source in unique_sources:\n            all_trajectories.extend(find_trajectories(source))\n\n    logger.info(\n        f\"Discovered {len(all_trajectories)} complete disease progression trajectories\"\n    )\n\n    # Log trajectory length statistics only if we have trajectories\n    if len(all_trajectories) &gt; 0:\n        traj_lengths = [len(t) for t in all_trajectories]\n        logger.info(\n            f\"Trajectory lengths - Min: {min(traj_lengths)}, Max: {max(traj_lengths)}, \"\n            f\"Mean: {np.mean(traj_lengths):.1f}\"\n        )\n    else:\n        logger.warning(\"No trajectories found!\")\n\n    return network, all_trajectories\n</code></pre>"},{"location":"api/trajectories/#generate_trajectory_data","title":"generate_trajectory_data","text":"<p>Generate complete trajectory dataset with metadata.</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.generate_trajectory_data","title":"generate_trajectory_data","text":"<pre><code>generate_trajectory_data(\n    vae_model: Module,\n    recnet_model: Optional[Module],\n    trajectory: List[str],\n    gene_data: DataFrame,\n    n_timepoints: int = 50,\n    interpolation_method: str = \"linear\",\n    device: str = \"cpu\",\n    save_path: Optional[Path] = None,\n    scaler: Optional[MinMaxScaler] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Generate synthetic gene expression data along a patient trajectory.</p> <p>Creates N interpolated time points between consecutive patients in a trajectory by performing interpolation in the VAE latent space, then decoding back to gene expression space. Optionally applies reconstruction network for refinement.</p> <p>Args:     vae_model: Trained VAE model for encoding/decoding     recnet_model: Optional reconstruction network for refining VAE output     trajectory: List of patient IDs in chronological progression order     gene_data: Gene expression DataFrame (genes \u00d7 patients)     n_timepoints: Number of interpolation points between each patient pair     interpolation_method: 'linear' or 'spherical' interpolation in latent space     device: Torch device for computation     save_path: Optional path to save trajectory CSV file     scaler: Pre-fitted MinMaxScaler from VAE training. If None, will fit on gene_data.</p> <p>Returns:     DataFrame with synthetic gene expression profiles for all time points.     Shape: (n_timepoints * (len(trajectory)-1), n_genes)     Index contains time point identifiers</p> <p>Workflow:     1. Extract gene expression for each patient in trajectory     2. Normalize using the SAME scaler used during VAE training     3. Encode each patient to VAE latent space     4. For each consecutive pair:        a. Interpolate in latent space (linear or spherical)        b. Decode interpolated points back to gene space        c. Optionally apply reconstruction network     5. Concatenate all segments into complete trajectory</p> <p>Interpolation Methods:     linear: Straight-line interpolation in latent space             z(t) = (1-t)*z_source + t*z_target</p> <pre><code>spherical: Spherical linear interpolation (SLERP)\n           Preserves magnitude, interpolates on hypersphere\n           Recommended for normalized latent spaces\n</code></pre> <p>Note:     CRITICAL: The scaler must be the same one used during VAE training.     Using a different scaler will produce incorrect latent representations.     If scaler=None, will fit on all gene_data (all patients), which approximates     the training distribution.</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def generate_trajectory_data(\n    vae_model: torch.nn.Module,\n    recnet_model: Optional[torch.nn.Module],\n    trajectory: List[str],\n    gene_data: pd.DataFrame,\n    n_timepoints: int = 50,\n    interpolation_method: str = \"linear\",\n    device: str = \"cpu\",\n    save_path: Optional[Path] = None,\n    scaler: Optional[MinMaxScaler] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate synthetic gene expression data along a patient trajectory.\n\n    Creates N interpolated time points between consecutive patients in a trajectory\n    by performing interpolation in the VAE latent space, then decoding back to\n    gene expression space. Optionally applies reconstruction network for refinement.\n\n    Args:\n        vae_model: Trained VAE model for encoding/decoding\n        recnet_model: Optional reconstruction network for refining VAE output\n        trajectory: List of patient IDs in chronological progression order\n        gene_data: Gene expression DataFrame (genes \u00d7 patients)\n        n_timepoints: Number of interpolation points between each patient pair\n        interpolation_method: 'linear' or 'spherical' interpolation in latent space\n        device: Torch device for computation\n        save_path: Optional path to save trajectory CSV file\n        scaler: Pre-fitted MinMaxScaler from VAE training. If None, will fit on gene_data.\n\n    Returns:\n        DataFrame with synthetic gene expression profiles for all time points.\n        Shape: (n_timepoints * (len(trajectory)-1), n_genes)\n        Index contains time point identifiers\n\n    Workflow:\n        1. Extract gene expression for each patient in trajectory\n        2. Normalize using the SAME scaler used during VAE training\n        3. Encode each patient to VAE latent space\n        4. For each consecutive pair:\n           a. Interpolate in latent space (linear or spherical)\n           b. Decode interpolated points back to gene space\n           c. Optionally apply reconstruction network\n        5. Concatenate all segments into complete trajectory\n\n    Interpolation Methods:\n        linear: Straight-line interpolation in latent space\n                z(t) = (1-t)*z_source + t*z_target\n\n        spherical: Spherical linear interpolation (SLERP)\n                   Preserves magnitude, interpolates on hypersphere\n                   Recommended for normalized latent spaces\n\n    Note:\n        CRITICAL: The scaler must be the same one used during VAE training.\n        Using a different scaler will produce incorrect latent representations.\n        If scaler=None, will fit on all gene_data (all patients), which approximates\n        the training distribution.\n    \"\"\"\n\n    logger.info(f\"Generating trajectory data for {len(trajectory)} patients\")\n    logger.info(\n        f\"Interpolation: {n_timepoints} points \u00d7 {len(trajectory) - 1} segments\"\n    )\n    logger.info(f\"Method: {interpolation_method}\")\n\n    # Set models to evaluation mode\n    vae_model.eval()\n    if recnet_model is not None:\n        recnet_model.eval()\n\n    vae_model = vae_model.to(device)\n    if recnet_model is not None:\n        recnet_model = recnet_model.to(device)\n\n    # Use provided scaler or fit new one on all gene data\n    if scaler is None:\n        logger.warning(\"No scaler provided - fitting new scaler on all gene data\")\n        logger.warning(\"This may not match VAE training normalization!\")\n        scaler = MinMaxScaler()\n        # gene_data is (genes \u00d7 patients), need (patients \u00d7 genes) for scaler\n        scaler.fit(gene_data.T.values)\n        logger.info(f\"Fitted scaler on {gene_data.shape[1]} patients\")\n    else:\n        logger.info(\"Using provided scaler from VAE training\")\n\n    # Select interpolation function\n    if interpolation_method == \"linear\":\n        interp_func = interpolate_latent_linear\n    elif interpolation_method == \"spherical\":\n        interp_func = interpolate_latent_spherical\n    else:\n        raise ValueError(f\"Unknown interpolation method: {interpolation_method}\")\n\n    # Generate synthetic data for each segment of the trajectory\n    all_segments = []\n\n    with torch.no_grad():\n        for i in range(len(trajectory) - 1):\n            source_patient = trajectory[i]\n            target_patient = trajectory[i + 1]\n\n            logger.info(\n                f\"Segment {i + 1}/{len(trajectory) - 1}: {source_patient} \u2192 {target_patient}\"\n            )\n\n            # Get gene expression for source and target\n            # gene_data is (genes \u00d7 patients), so gene_data[patient] is a Series of gene values\n            source_expr = gene_data[source_patient].values.reshape(1, -1)  # (1, genes)\n            target_expr = gene_data[target_patient].values.reshape(1, -1)  # (1, genes)\n\n            # Normalize data using the provided scaler\n            # Scaler expects (n_samples, n_features) = (1, genes)\n            source_norm = scaler.transform(source_expr)  # (1, genes)\n            target_norm = scaler.transform(target_expr)  # (1, genes)\n\n            # Encode to latent space\n            source_tensor = torch.tensor(source_norm, dtype=torch.float32).to(device)\n            target_tensor = torch.tensor(target_norm, dtype=torch.float32).to(device)\n\n            _, _, _, z_source = vae_model(source_tensor)\n            _, _, _, z_target = vae_model(target_tensor)\n\n            # Interpolate in latent space\n            z_source_np = z_source.cpu().numpy().flatten()\n            z_target_np = z_target.cpu().numpy().flatten()\n\n            interpolated_z = interp_func(z_source_np, z_target_np, n_timepoints)\n\n            # Decode interpolated latent vectors\n            interpolated_z_tensor = torch.tensor(\n                interpolated_z, dtype=torch.float32\n            ).to(device)\n            decoded = vae_model.decoder(interpolated_z_tensor)\n\n            # Denormalize using the same scaler\n            # decoded is (n_timepoints, genes), scaler expects (n_samples, n_features)\n            decoded_np = decoded.cpu().numpy()  # (n_timepoints, genes)\n            segment_data = scaler.inverse_transform(\n                decoded_np\n            )  # (n_timepoints, genes) - REAL SPACE\n\n            # Apply reconstruction network if provided\n            # CRITICAL: RecNet works on REAL SPACE data, not normalized!\n            if recnet_model is not None:\n                # Convert to tensor and apply RecNet directly to real space data\n                segment_tensor = torch.tensor(segment_data, dtype=torch.float32).to(\n                    device\n                )\n                refined = recnet_model(segment_tensor)\n                segment_data = (\n                    refined.cpu().numpy()\n                )  # (n_timepoints, genes) - REAL SPACE\n\n            all_segments.append(segment_data)\n\n    # Concatenate all segments\n    trajectory_data = np.vstack(all_segments)\n\n    # Create DataFrame\n    trajectory_df = pd.DataFrame(trajectory_data, columns=gene_data.index)\n\n    # Create informative index\n    time_indices = []\n    for i in range(len(trajectory) - 1):\n        for t in range(n_timepoints):\n            time_indices.append(f\"{trajectory[i]}_to_{trajectory[i + 1]}_t{t:03d}\")\n    trajectory_df.index = time_indices\n\n    logger.info(f\"Generated trajectory data: {trajectory_df.shape}\")\n\n    # Save if path provided\n    if save_path is not None:\n        save_path = Path(save_path)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        trajectory_df.to_csv(save_path)\n        logger.info(f\"Saved trajectory to: {save_path}\")\n\n    return trajectory_df\n</code></pre>"},{"location":"api/trajectories/#patient-connectivity","title":"Patient Connectivity","text":""},{"location":"api/trajectories/#link_patients_closest","title":"link_patients_closest","text":"<p>Link patients using closest latent space neighbors.</p> <p>Example:</p> <pre><code>from renalprog.modeling.predict import link_patients_closest\nimport numpy as np\n\nearly_latent = np.random.randn(100, 128)\nlate_latent = np.random.randn(80, 128)\n\nconnections = link_patients_closest(\n    latent_early=early_latent,\n    latent_late=late_latent,\n    patient_ids_early=['E001', 'E002', ...],\n    patient_ids_late=['L001', 'L002', ...]\n)\n\n# Returns DataFrame with columns: early_patient, late_patient, distance\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.link_patients_closest","title":"link_patients_closest","text":"<pre><code>link_patients_closest(\n    transitions_df: DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients by selecting closest (or farthest) matches across stages.</p> <p>For each patient at a source stage, this function identifies the closest (or farthest) patient at the target stage, considering metadata constraints (gender, race). This creates one-to-one patient linkages that form the basis for trajectory construction.</p> <p>Args:     transitions_df: DataFrame from calculate_all_possible_transitions()                     containing all possible patient pairs with distances     start_with_first_stage: If True, build forward trajectories (early\u2192late)                             If False, build backward trajectories (late\u2192early)     early_late: If True, uses early/late groupings. If False, uses I-IV stages     closest: If True, connect closest patients. If False, connect farthest patients</p> <p>Returns:     DataFrame with selected patient links, containing one row per source patient     with their optimal target patient match. Includes all columns from transitions_df.</p> <p>Selection Strategy:     - Forward (start_with_first_stage=True): For each source, find optimal target     - Backward (start_with_first_stage=False): For each target, find optimal source     - Closest (closest=True): Minimum distance match     - Farthest (closest=False): Maximum distance match</p> <p>Metadata Stratification:     Links are selected independently within each combination of:     - Gender (MALE, FEMALE)     - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)     This ensures demographic consistency in trajectories.</p> <p>Example:     &gt;&gt;&gt; links = link_patients_closest(     ...     transitions_df=all_transitions,     ...     start_with_first_stage=True,     ...     closest=True     ... )     &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")     Created 234 patient links</p> <p>Note:     - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV     - Processes in reverse for backward: IV\u2192III\u2192II\u2192I     - Each patient appears at most once as a source in the result</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_closest(\n    transitions_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    early_late: bool = False,\n    closest: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients by selecting closest (or farthest) matches across stages.\n\n    For each patient at a source stage, this function identifies the closest\n    (or farthest) patient at the target stage, considering metadata constraints\n    (gender, race). This creates one-to-one patient linkages that form the basis\n    for trajectory construction.\n\n    Args:\n        transitions_df: DataFrame from calculate_all_possible_transitions()\n                        containing all possible patient pairs with distances\n        start_with_first_stage: If True, build forward trajectories (early\u2192late)\n                                If False, build backward trajectories (late\u2192early)\n        early_late: If True, uses early/late groupings. If False, uses I-IV stages\n        closest: If True, connect closest patients. If False, connect farthest patients\n\n    Returns:\n        DataFrame with selected patient links, containing one row per source patient\n        with their optimal target patient match. Includes all columns from transitions_df.\n\n    Selection Strategy:\n        - Forward (start_with_first_stage=True): For each source, find optimal target\n        - Backward (start_with_first_stage=False): For each target, find optimal source\n        - Closest (closest=True): Minimum distance match\n        - Farthest (closest=False): Maximum distance match\n\n    Metadata Stratification:\n        Links are selected independently within each combination of:\n        - Gender (MALE, FEMALE)\n        - Race (ASIAN, BLACK OR AFRICAN AMERICAN, WHITE)\n        This ensures demographic consistency in trajectories.\n\n    Example:\n        &gt;&gt;&gt; links = link_patients_closest(\n        ...     transitions_df=all_transitions,\n        ...     start_with_first_stage=True,\n        ...     closest=True\n        ... )\n        &gt;&gt;&gt; print(f\"Created {len(links)} patient links\")\n        Created 234 patient links\n\n    Note:\n        - Processes transitions in order for forward: I\u2192II\u2192III\u2192IV\n        - Processes in reverse for backward: IV\u2192III\u2192II\u2192I\n        - Each patient appears at most once as a source in the result\n    \"\"\"\n    logger.info(\"Linking patients by closest/farthest matches\")\n    logger.info(f\"Direction: {'Forward' if start_with_first_stage else 'Backward'}\")\n    logger.info(f\"Strategy: {'Closest' if closest else 'Farthest'}\")\n\n    # Define transition order based on direction\n    if start_with_first_stage and not early_late:\n        transitions_possible = [\"1_to_2\", \"2_to_3\", \"3_to_4\"]\n    elif not start_with_first_stage and not early_late:\n        transitions_possible = [\"3_to_4\", \"2_to_3\", \"1_to_2\"]\n    elif early_late:\n        transitions_possible = [\"early_to_late\"]\n\n    # 0 for closest (smallest distance), -1 for farthest (largest distance)\n    idx = 0 if closest else -1\n\n    # Find closest/farthest patient for each source patient\n    closest_list = []\n    for transition_i in transitions_possible:\n        transition_df_i = transitions_df[transitions_df[\"transition\"] == transition_i]\n\n        logger.info(\n            f\"Processing transition {transition_i}: {len(transition_df_i)} pairs\"\n        )\n\n        # Iterate through all metadata combinations\n        for gender_i in [\"FEMALE\", \"MALE\"]:\n            df_gender_i = transition_df_i.query(f\"source_gender == '{gender_i}'\")\n\n            for race_i in [\"ASIAN\", \"BLACK OR AFRICAN AMERICAN\", \"WHITE\"]:\n                df_race_i = df_gender_i.query(f\"source_race == '{race_i}'\")\n\n                if df_race_i.empty:\n                    continue\n\n                # Get unique patients to link\n                unique_sources = df_race_i[\"source\"].unique()\n                unique_targets = df_race_i[\"target\"].unique()\n                use_uniques = (\n                    unique_sources if start_with_first_stage else unique_targets\n                )\n                use_column = \"source\" if start_with_first_stage else \"target\"\n\n                # Find closest/farthest match for each patient\n                for pat_i in use_uniques:\n                    pat_matches = df_race_i[df_race_i[use_column] == pat_i]\n                    if len(pat_matches) &gt; 0:\n                        # Sort by distance and select first (closest) or last (farthest)\n                        best_match = pat_matches.sort_values(\"distance\").iloc[idx]\n                        closest_list.append(best_match)\n\n    # Convert to DataFrame\n    closest_df = pd.DataFrame(closest_list)\n    closest_df.reset_index(drop=True, inplace=True)\n\n    logger.info(f\"Created {len(closest_df)} patient links\")\n\n    return closest_df\n</code></pre>"},{"location":"api/trajectories/#link_patients_random","title":"link_patients_random","text":"<p>Link patients randomly (control method).</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.link_patients_random","title":"link_patients_random","text":"<pre><code>link_patients_random(\n    results_df: DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Link patients to multiple random targets at the next stage.</p> <p>Instead of linking each patient to only their closest match, this function randomly samples multiple patients at the next stage to link to each source patient. This creates a one-to-many mapping useful for generating multiple trajectory samples.</p> <p>Parameters:</p> Name Type Description Default <code>results_df</code> <code>DataFrame</code> <p>DataFrame with possible sources and targets, their metadata, and distance.</p> required <code>start_with_first_stage</code> <code>bool</code> <p>If True, initiate trajectories with first stage as sources. If False, initiate trajectories with last stage as sources.</p> <code>True</code> <code>link_next</code> <code>int</code> <p>Number of patients at next stage to randomly link to each patient of current stage.</p> <code>5</code> <code>transitions_possible</code> <code>list</code> <p>List of transitions to process (e.g., ['1_to_2', '2_to_3']). If None, defaults to ['early_to_late'].</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with randomly sampled patient links for each transition. Contains multiple rows per source patient (up to link_next).</p> Notes <ul> <li>Random sampling is primarily performed for WHITE race patients due to sample size</li> <li>If fewer than link_next targets are available, all available targets are selected</li> <li>Patients from other races are included with all their possible connections</li> <li>Empty DataFrame is returned if no WHITE patients are found</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def link_patients_random(\n    results_df: pd.DataFrame,\n    start_with_first_stage: bool = True,\n    link_next: int = 5,\n    transitions_possible: Optional[List[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Link patients to multiple random targets at the next stage.\n\n    Instead of linking each patient to only their closest match, this function randomly\n    samples multiple patients at the next stage to link to each source patient. This\n    creates a one-to-many mapping useful for generating multiple trajectory samples.\n\n    Parameters\n    ----------\n    results_df : pd.DataFrame\n        DataFrame with possible sources and targets, their metadata, and distance.\n    start_with_first_stage : bool, default=True\n        If True, initiate trajectories with first stage as sources.\n        If False, initiate trajectories with last stage as sources.\n    link_next : int, default=5\n        Number of patients at next stage to randomly link to each patient of current stage.\n    transitions_possible : list, optional\n        List of transitions to process (e.g., ['1_to_2', '2_to_3']).\n        If None, defaults to ['early_to_late'].\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with randomly sampled patient links for each transition.\n        Contains multiple rows per source patient (up to link_next).\n\n    Notes\n    -----\n    - Random sampling is primarily performed for WHITE race patients due to sample size\n    - If fewer than link_next targets are available, all available targets are selected\n    - Patients from other races are included with all their possible connections\n    - Empty DataFrame is returned if no WHITE patients are found\n    \"\"\"\n    # Set default transitions if not provided\n    if transitions_possible is None:\n        transitions_possible = [\"early_to_late\"]\n\n    # Get unique genders and races\n    unique_genders = results_df[\"source_gender\"].unique().tolist()\n    # Get unique races\n    unique_races = results_df[\"source_race\"].unique().tolist()\n    if \"WHITE\" in unique_races:\n        unique_races.remove(\"WHITE\")\n    # transition:\n    samples = []\n    for transition_i in transitions_possible:\n        transition_df_i = results_df[results_df[\"transition\"] == transition_i]\n        for gender_i in unique_genders:\n            df_samples_i = transition_df_i.query(\n                f\"source_gender == '{gender_i}' &amp; source_race == 'WHITE'\"\n            )  # we can only do this for the whites since these are the only ones with enough samples\n            if df_samples_i.empty:\n                print(\n                    f\"Warning: No WHITE patients found for gender {gender_i} in transition {transition_i}\"\n                )\n                continue\n            unique_sources_i = np.unique(df_samples_i[\"source\"]).tolist()\n            unique_targets_i = np.unique(df_samples_i[\"target\"]).tolist()\n            use_uniques = (\n                unique_sources_i if start_with_first_stage else unique_targets_i\n            )\n            use_source_target = \"source\" if start_with_first_stage else \"target\"\n            for pat_i in use_uniques:\n                sample_i = df_samples_i.loc[df_samples_i[use_source_target] == pat_i]\n                if len(sample_i) &gt;= link_next:\n                    sample_i = sample_i.sample(\n                        link_next\n                    )  # Sample a number of patients at next stage to link to each patient of current stage\n                else:\n                    sample_i = sample_i.sample(\n                        len(sample_i)\n                    )  # Sample all available patients if less than link_next\n                samples.append(sample_i)\n\n    # Check if samples list is empty\n    if not samples:\n        print(\"Warning: No samples found for WHITE race. Returning empty DataFrame.\")\n        return pd.DataFrame(columns=results_df.columns)\n\n    # Turn samples into dataframe:\n    samples_df = pd.concat(samples)\n    # Add the rest of the races\n    if unique_races:\n        samples_df = pd.concat(\n            [samples_df, results_df[results_df[\"source_race\"].isin(unique_races)]]\n        )\n    samples_df.reset_index(drop=True, inplace=True)\n    return samples_df\n</code></pre>"},{"location":"api/trajectories/#transition-analysis","title":"Transition Analysis","text":""},{"location":"api/trajectories/#calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<p>Calculate metrics for all possible patient transitions.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import calculate_all_possible_transitions\n\n# Calculate all transitions\ntransitions = calculate_all_possible_transitions(\n    latent_early=early_latent,\n    latent_late=late_latent,\n    patient_ids_early=early_ids,\n    patient_ids_late=late_ids,\n    output_dir=Path(\"data/processed/transitions\")\n)\n\n# Analyze transition patterns\nprint(transitions.describe())\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.calculate_all_possible_transitions","title":"calculate_all_possible_transitions","text":"<pre><code>calculate_all_possible_transitions(\n    data: DataFrame,\n    metadata_selection: DataFrame,\n    distance: str = \"wasserstein\",\n    early_late: bool = False,\n    negative_trajectory: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Calculate all possible patient-to-patient transitions for KIRC cancer.</p> <p>This function computes pairwise distances between all patients at consecutive (or same) cancer stages, considering metadata constraints. Only patients with matching gender and race are considered as potential trajectory pairs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Gene expression data with patients as columns.</p> required <code>metadata_selection</code> <code>DataFrame</code> <p>Clinical metadata with columns: histological_type, race, gender, stage.</p> required <code>distance</code> <code>(wasserstein, euclidean)</code> <p>Distance metric to use for calculating patient-to-patient distances.</p> <code>'wasserstein'</code> <code>early_late</code> <code>bool</code> <p>If True, uses early/late stage groupings. If False, uses I-IV stages.</p> <code>False</code> <code>negative_trajectory</code> <code>bool</code> <p>If True, generates same-stage transitions (negative controls). If False, generates progression transitions (positive trajectories).</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing all possible transitions with columns: - source, target: Patient IDs - source_gender, target_gender: Gender - source_race, target_race: Race - transition: Stage transition label (e.g., '1_to_2', 'early_to_late') - distance: Calculated distance between patients</p> <p>Sorted by gender, race, transition, and distance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance metric is not 'wasserstein' or 'euclidean'.</p> Notes <ul> <li>For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late</li> <li>For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late</li> <li>Only patients with identical gender and race are paired</li> </ul> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def calculate_all_possible_transitions(\n    data: pd.DataFrame,\n    metadata_selection: pd.DataFrame,\n    distance: str = \"wasserstein\",\n    early_late: bool = False,\n    negative_trajectory: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate all possible patient-to-patient transitions for KIRC cancer.\n\n    This function computes pairwise distances between all patients at consecutive\n    (or same) cancer stages, considering metadata constraints. Only patients with\n    matching gender and race are considered as potential trajectory pairs.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Gene expression data with patients as columns.\n    metadata_selection : pd.DataFrame\n        Clinical metadata with columns: histological_type, race, gender, stage.\n    distance : {'wasserstein', 'euclidean'}, default='wasserstein'\n        Distance metric to use for calculating patient-to-patient distances.\n    early_late : bool, default=False\n        If True, uses early/late stage groupings. If False, uses I-IV stages.\n    negative_trajectory : bool, default=False\n        If True, generates same-stage transitions (negative controls).\n        If False, generates progression transitions (positive trajectories).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing all possible transitions with columns:\n        - source, target: Patient IDs\n        - source_gender, target_gender: Gender\n        - source_race, target_race: Race\n        - transition: Stage transition label (e.g., '1_to_2', 'early_to_late')\n        - distance: Calculated distance between patients\n\n        Sorted by gender, race, transition, and distance.\n\n    Raises\n    ------\n    ValueError\n        If distance metric is not 'wasserstein' or 'euclidean'.\n\n    Notes\n    -----\n    - For positive trajectories: links I\u2192II, II\u2192III, III\u2192IV or early\u2192late\n    - For negative trajectories: links I\u2192I, II\u2192II, III\u2192III, IV\u2192IV or early\u2192early, late\u2192late\n    - Only patients with identical gender and race are paired\n    \"\"\"\n    # Select distance function\n    if distance == \"wasserstein\":\n        from scipy.stats import wasserstein_distance\n\n        distance_fun = wasserstein_distance\n    elif distance == \"euclidean\":\n        from scipy.spatial.distance import euclidean\n\n        distance_fun = euclidean\n    else:\n        raise ValueError(\n            'Distance function not implemented. Use either \"wasserstein\" or \"euclidean\".'\n        )\n\n    # Define stage transitions based on parameters\n    if early_late and not negative_trajectory:\n        possible_transitions = [\"early_to_late\"]\n        stage_pairs = [[\"early\", \"late\"]]\n    elif early_late and negative_trajectory:\n        possible_transitions = [\"early_to_early\", \"late_to_late\"]\n        stage_pairs = [[\"early\", \"early\"], [\"late\", \"late\"]]\n    elif not early_late and not negative_trajectory:\n        possible_transitions = [\"1_to_2\", \"2_to_3\", \"3_to_4\"]\n        stage_pairs = [[\"I\", \"II\"], [\"II\", \"III\"], [\"III\", \"IV\"]]\n    elif not early_late and negative_trajectory:\n        possible_transitions = [\"1_to_1\", \"2_to_2\", \"3_to_3\", \"4_to_4\"]\n        stage_pairs = [[\"I\", \"I\"], [\"II\", \"II\"], [\"III\", \"III\"], [\"IV\", \"IV\"]]\n\n    # Calculate all possible transitions\n    results = []\n    for i_tr, transition in enumerate(possible_transitions):\n        source_target_stage = stage_pairs[i_tr]\n\n        # Iterate through all patient pairs at specified stages\n        for pat_i in metadata_selection.index[\n            metadata_selection[\"stage\"] == source_target_stage[0]\n        ]:\n            for pat_ii in metadata_selection.index[\n                metadata_selection[\"stage\"] == source_target_stage[1]\n            ]:\n                # Extract metadata for both patients\n                source_gender = metadata_selection.at[pat_i, \"gender\"]\n                target_gender = metadata_selection.at[pat_ii, \"gender\"]\n                source_race = metadata_selection.at[pat_i, \"race\"]\n                target_race = metadata_selection.at[pat_ii, \"race\"]\n\n                # Skip if metadata doesn't match (gender and race must match)\n                if not (source_race == target_race and source_gender == target_gender):\n                    continue\n\n                # Store transition information\n                results_i = {\n                    \"source\": pat_i,\n                    \"target\": pat_ii,\n                    \"source_gender\": source_gender,\n                    \"target_gender\": target_gender,\n                    \"source_race\": source_race,\n                    \"target_race\": target_race,\n                    \"transition\": transition,\n                    \"distance\": distance_fun(data[pat_i], data[pat_ii]),\n                }\n                results.append(results_i)\n\n    # Convert to DataFrame and sort\n    results_df = pd.DataFrame(results)\n    results_df.sort_values(\n        [\n            \"source_gender\",\n            \"target_gender\",\n            \"source_race\",\n            \"target_race\",\n            \"transition\",\n            \"distance\",\n        ],\n        inplace=True,\n        ignore_index=True,\n    )\n    return results_df\n</code></pre>"},{"location":"api/trajectories/#dynamic-enrichment","title":"Dynamic Enrichment","text":""},{"location":"api/trajectories/#dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<p>Perform pathway enrichment at each trajectory timepoint.</p> <p>Example Usage:</p> <pre><code>from renalprog.modeling.predict import dynamic_enrichment_analysis\nfrom pathlib import Path\n\n# Analyze pathway dynamics along trajectories\nenrichment_results = dynamic_enrichment_analysis(\n    trajectories=trajectory_gene_expression,  # Shape: (n_traj, n_steps, n_genes)\n    gene_names=gene_list,\n    pathway_file=Path(\"data/external/ReactomePathways.gmt\"),\n    output_dir=Path(\"reports/dynamic_enrichment\")\n)\n\n# Results contain enrichment at each timepoint\nfor timepoint, results in enrichment_results.items():\n    print(f\"Timepoint {timepoint}: {len(results)} enriched pathways\")\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.dynamic_enrichment_analysis","title":"dynamic_enrichment_analysis","text":"<pre><code>dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Perform dynamic enrichment analysis on synthetic trajectories.</p> <p>This orchestrates: 1. DESeq2 analysis on each trajectory point 2. GSEA on differential expression results 3. Aggregation of enrichment across trajectories</p> <p>Args:     trajectory_dir: Directory containing trajectory CSV files     pathways_file: Path to pathway GMT file     output_dir: Directory to save results     cancer_type: Cancer type identifier</p> <p>Returns:     DataFrame with aggregated enrichment results</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def dynamic_enrichment_analysis(\n    trajectory_dir: Path,\n    pathways_file: Path,\n    output_dir: Path,\n    cancer_type: str = \"kirc\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform dynamic enrichment analysis on synthetic trajectories.\n\n    This orchestrates:\n    1. DESeq2 analysis on each trajectory point\n    2. GSEA on differential expression results\n    3. Aggregation of enrichment across trajectories\n\n    Args:\n        trajectory_dir: Directory containing trajectory CSV files\n        pathways_file: Path to pathway GMT file\n        output_dir: Directory to save results\n        cancer_type: Cancer type identifier\n\n    Returns:\n        DataFrame with aggregated enrichment results\n    \"\"\"\n    logger.info(f\"Running dynamic enrichment analysis for {cancer_type}\")\n\n    # TODO: Implement orchestration\n    # Migrate from src_deseq_and_gsea_NCSR/pipeline.sh and related scripts\n\n    raise NotImplementedError(\n        \"dynamic_enrichment_analysis() needs implementation. \"\n        \"Migrate orchestration from src_deseq_and_gsea_NCSR/pipeline.sh, \"\n        \"py_deseq.py, and trajectory_formatting.py\"\n    )\n</code></pre>"},{"location":"api/trajectories/#interpolation-methods","title":"Interpolation Methods","text":""},{"location":"api/trajectories/#interpolate_latent_linear","title":"interpolate_latent_linear","text":"<p>Linear interpolation between points.</p>"},{"location":"api/trajectories/#renalprog.modeling.predict.interpolate_latent_linear","title":"interpolate_latent_linear","text":"<pre><code>interpolate_latent_linear(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Linear interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_linear(\n    z_source: np.ndarray, z_target: np.ndarray, n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Linear interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array(\n        [(1 - alpha) * z_source + alpha * z_target for alpha in alphas]\n    )\n    return interpolated\n</code></pre>"},{"location":"api/trajectories/#interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<p>Spherical interpolation (SLERP) for normalized spaces.</p> <p>Comparison:</p> <pre><code>from renalprog.modeling.predict import (\n    interpolate_latent_linear,\n    interpolate_latent_spherical\n)\nimport numpy as np\n\nz_start = np.random.randn(1, 128)\nz_end = np.random.randn(1, 128)\n\n# Linear interpolation\ntraj_linear = interpolate_latent_linear(z_start, z_end, n_steps=50)\n\n# Spherical interpolation (preserves norm better)\ntraj_spherical = interpolate_latent_spherical(z_start, z_end, n_steps=50)\n\n# Spherical is preferred for normalized latent spaces\n</code></pre>"},{"location":"api/trajectories/#renalprog.modeling.predict.interpolate_latent_spherical","title":"interpolate_latent_spherical","text":"<pre><code>interpolate_latent_spherical(\n    z_source: ndarray, z_target: ndarray, n_steps: int = 50\n) -&gt; np.ndarray\n</code></pre> <p>Spherical (SLERP) interpolation in latent space.</p> <p>Args:     z_source: Source latent vector     z_target: Target latent vector     n_steps: Number of interpolation steps</p> <p>Returns:     Array of interpolated latent vectors (n_steps x latent_dim)</p> Source code in <code>renalprog/modeling/predict.py</code> <pre><code>def interpolate_latent_spherical(\n    z_source: np.ndarray, z_target: np.ndarray, n_steps: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Spherical (SLERP) interpolation in latent space.\n\n    Args:\n        z_source: Source latent vector\n        z_target: Target latent vector\n        n_steps: Number of interpolation steps\n\n    Returns:\n        Array of interpolated latent vectors (n_steps x latent_dim)\n    \"\"\"\n    # Normalize vectors\n    z_source_norm = z_source / np.linalg.norm(z_source)\n    z_target_norm = z_target / np.linalg.norm(z_target)\n\n    # Calculate angle between vectors\n    omega = np.arccos(np.clip(np.dot(z_source_norm, z_target_norm), -1.0, 1.0))\n\n    if omega &lt; 1e-8:\n        # Vectors are nearly identical, use linear interpolation\n        return interpolate_latent_linear(z_source, z_target, n_steps)\n\n    # SLERP formula\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated = np.array(\n        [\n            (np.sin((1 - alpha) * omega) / np.sin(omega)) * z_source\n            + (np.sin(alpha * omega) / np.sin(omega)) * z_target\n            for alpha in alphas\n        ]\n    )\n\n    return interpolated\n</code></pre>"},{"location":"api/trajectories/#visualization","title":"Visualization","text":""},{"location":"api/trajectories/#plot_trajectory","title":"plot_trajectory","text":"<p>Visualize individual trajectory.</p> <p>Example:</p> <pre><code>from renalprog.plots import plot_trajectory\nfrom pathlib import Path\n\n# Plot single trajectory\nplot_trajectory(\n    trajectory=trajectory_data[0],  # Shape: (n_steps, n_features)\n    feature_names=selected_genes,\n    output_path=Path(\"reports/figures/trajectory_example.png\"),\n    title=\"Disease Progression Trajectory\"\n)\n</code></pre>"},{"location":"api/trajectories/#renalprog.plots.plot_trajectory","title":"plot_trajectory","text":"<pre><code>plot_trajectory(\n    trajectory: ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure\n</code></pre> <p>Plot gene expression changes along a trajectory.</p> <p>Args:     trajectory: Array of shape (n_timepoints, n_genes)     gene_names: Optional list of gene names     save_path: Optional path to save figure     title: Plot title     n_genes_to_show: Number of top varying genes to display</p> <p>Returns:     Plotly Figure object</p> Source code in <code>renalprog/plots.py</code> <pre><code>def plot_trajectory(\n    trajectory: np.ndarray,\n    gene_names: Optional[List[str]] = None,\n    save_path: Optional[Path] = None,\n    title: str = \"Gene Expression Trajectory\",\n    n_genes_to_show: int = 20,\n) -&gt; go.Figure:\n    \"\"\"\n    Plot gene expression changes along a trajectory.\n\n    Args:\n        trajectory: Array of shape (n_timepoints, n_genes)\n        gene_names: Optional list of gene names\n        save_path: Optional path to save figure\n        title: Plot title\n        n_genes_to_show: Number of top varying genes to display\n\n    Returns:\n        Plotly Figure object\n    \"\"\"\n    n_timepoints, n_genes = trajectory.shape\n\n    # Calculate variance for each gene\n    gene_variance = np.var(trajectory, axis=0)\n    top_genes_idx = np.argsort(gene_variance)[-n_genes_to_show:]\n\n    if gene_names is None:\n        gene_names = [f\"Gene_{i}\" for i in range(n_genes)]\n\n    fig = go.Figure()\n\n    timepoints = list(range(n_timepoints))\n\n    for idx in top_genes_idx:\n        fig.add_trace(\n            go.Scatter(\n                x=timepoints,\n                y=trajectory[:, idx],\n                mode=\"lines\",\n                name=gene_names[idx],\n                line=dict(width=1.5),\n            )\n        )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Timepoint\",\n        yaxis_title=\"Expression Level\",\n        template=DEFAULT_TEMPLATE,\n        width=DEFAULT_WIDTH,\n        height=DEFAULT_HEIGHT,\n        hovermode=\"x unified\",\n    )\n\n    if save_path:\n        save_plot(fig, save_path)\n\n    return fig\n</code></pre>"},{"location":"api/trajectories/#see-also","title":"See Also","text":"<ul> <li>Prediction API - Apply trained models</li> <li>Plots API - Visualization functions</li> <li>Complete Pipeline Tutorial</li> </ul>"},{"location":"api/utils/","title":"Utils API","text":"<p>Utility functions used across the RenalProg package.</p>"},{"location":"api/utils/#overview","title":"Overview","text":"<p>The utils module provides:</p> <ul> <li>Random seed setting for reproducibility</li> <li>Logging configuration</li> <li>Device selection (CPU/GPU)</li> <li>Data preprocessing utilities</li> <li>Helper functions</li> </ul>"},{"location":"api/utils/#core-utilities","title":"Core Utilities","text":""},{"location":"api/utils/#set_seed","title":"set_seed","text":"<p>Set random seed for reproducibility across all libraries.</p> <p>Example Usage:</p> <pre><code>from renalprog.utils import set_seed\n\n# Set seed at the start of your script\nset_seed(42)\n\n# All random operations will be reproducible\nimport numpy as np\nimport torch\n\nprint(np.random.rand(5))  # Same output every time\nprint(torch.randn(5))     # Same output every time\n</code></pre> <p>Libraries Affected:</p> <ul> <li><code>random</code> (Python standard library)</li> <li><code>numpy</code></li> <li><code>torch</code> (CPU)</li> <li><code>torch.cuda</code> (GPU)</li> <li>Sets <code>torch.backends.cudnn.deterministic = True</code></li> </ul>"},{"location":"api/utils/#renalprog.utils.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int = 2023)\n</code></pre> <p>Set random seed for reproducibility across numpy, random, and torch.</p> <p>Args:     seed: Random seed value (default: 2023)</p> Source code in <code>renalprog/utils/__init__.py</code> <pre><code>def set_seed(seed: int = 2023):\n    \"\"\"\n    Set random seed for reproducibility across numpy, random, and torch.\n\n    Args:\n        seed: Random seed value (default: 2023)\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/utils/#configure_logging","title":"configure_logging","text":"<p>Configure logging with scientific output formatting.</p> <p>Example Usage:</p> <pre><code>from renalprog.utils import configure_logging\nimport logging\n\n# Basic configuration (recommended)\nconfigure_logging()\n\n# Now logging works throughout the package\nimport renalprog.dataset as dataset\ndataset.download_data()  # Will show progress logs\n\n# Debug mode with timestamps\nconfigure_logging(\n    level=logging.DEBUG,\n    format_string=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n)\n\n# Custom file logging\nfile_handler = logging.FileHandler(\"pipeline.log\")\nconfigure_logging(handlers=[file_handler])\n</code></pre> <p>Logging Levels:</p> <pre><code>import logging\n\nconfigure_logging(level=logging.DEBUG)    # Show everything\nconfigure_logging(level=logging.INFO)     # Default - show info and above\nconfigure_logging(level=logging.WARNING)  # Show only warnings and errors\nconfigure_logging(level=logging.ERROR)    # Show only errors\n</code></pre>"},{"location":"api/utils/#renalprog.utils.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(\n    level: int = logging.INFO,\n    format_string: str = None,\n    datefmt: str = \"%Y-%m-%d %H:%M:%S\",\n    handlers: list = None,\n) -&gt; None\n</code></pre> <p>Configure logging for the renalprog package with scientific output formatting.</p> <p>This function sets up logging with appropriate formatting for both console output and optional file logging. It should be called at the start of scripts that use the renalprog package to ensure log messages are visible.</p> <p>Args:     level: Logging level (e.g., logging.INFO, logging.DEBUG).         Default: logging.INFO     format_string: Custom format string for log messages.         Default: \"[%(levelname)s] %(message)s\" for clean scientific output     datefmt: Date format for timestamps if included in format_string.         Default: \"%Y-%m-%d %H:%M:%S\"     handlers: List of custom logging handlers. If None, configures stdout handler.         Default: None (uses console output)</p> <p>Examples:     &gt;&gt;&gt; # Basic configuration (recommended for most scripts)     &gt;&gt;&gt; from renalprog.utils import configure_logging     &gt;&gt;&gt; configure_logging()</p> <pre><code>&gt;&gt;&gt; # Debug mode with timestamps\n&gt;&gt;&gt; configure_logging(\n...     level=logging.DEBUG,\n...     format_string=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n... )\n\n&gt;&gt;&gt; # Custom handlers (e.g., file logging)\n&gt;&gt;&gt; file_handler = logging.FileHandler(\"output.log\")\n&gt;&gt;&gt; configure_logging(handlers=[file_handler])\n</code></pre> <p>Notes:     - This function configures the root logger, which affects all package loggers     - Call this ONCE at the beginning of your script, not in library code     - For publication-quality output, use the default clean format</p> Source code in <code>renalprog/utils/__init__.py</code> <pre><code>def configure_logging(\n    level: int = logging.INFO,\n    format_string: str = None,\n    datefmt: str = \"%Y-%m-%d %H:%M:%S\",\n    handlers: list = None,\n) -&gt; None:\n    \"\"\"\n    Configure logging for the renalprog package with scientific output formatting.\n\n    This function sets up logging with appropriate formatting for both console output\n    and optional file logging. It should be called at the start of scripts that use\n    the renalprog package to ensure log messages are visible.\n\n    Args:\n        level: Logging level (e.g., logging.INFO, logging.DEBUG).\n            Default: logging.INFO\n        format_string: Custom format string for log messages.\n            Default: \"[%(levelname)s] %(message)s\" for clean scientific output\n        datefmt: Date format for timestamps if included in format_string.\n            Default: \"%Y-%m-%d %H:%M:%S\"\n        handlers: List of custom logging handlers. If None, configures stdout handler.\n            Default: None (uses console output)\n\n    Examples:\n        &gt;&gt;&gt; # Basic configuration (recommended for most scripts)\n        &gt;&gt;&gt; from renalprog.utils import configure_logging\n        &gt;&gt;&gt; configure_logging()\n\n        &gt;&gt;&gt; # Debug mode with timestamps\n        &gt;&gt;&gt; configure_logging(\n        ...     level=logging.DEBUG,\n        ...     format_string=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n        ... )\n\n        &gt;&gt;&gt; # Custom handlers (e.g., file logging)\n        &gt;&gt;&gt; file_handler = logging.FileHandler(\"output.log\")\n        &gt;&gt;&gt; configure_logging(handlers=[file_handler])\n\n    Notes:\n        - This function configures the root logger, which affects all package loggers\n        - Call this ONCE at the beginning of your script, not in library code\n        - For publication-quality output, use the default clean format\n    \"\"\"\n    if format_string is None:\n        # Clean format for scientific output (no timestamps cluttering the output)\n        format_string = \"[%(levelname)s] %(message)s\"\n\n    if handlers is None:\n        # Configure console output to stdout\n        handlers = [logging.StreamHandler(sys.stdout)]\n\n    # Configure root logger\n    logging.basicConfig(\n        level=level,\n        format=format_string,\n        datefmt=datefmt,\n        handlers=handlers,\n        force=True,  # Override any existing configuration\n    )\n\n    # Set level for all renalprog loggers\n    for logger_name in [\n        \"renalprog\",\n        \"renalprog.dataset\",\n        \"renalprog.features\",\n        \"renalprog.modeling\",\n        \"renalprog.trajectories\",\n        \"renalprog.plots\",\n    ]:\n        logger = logging.getLogger(logger_name)\n        logger.setLevel(level)\n</code></pre>"},{"location":"api/utils/#get_device","title":"get_device","text":"<p>Get appropriate device for PyTorch computation.</p> <p>Example Usage:</p> <pre><code>from renalprog.utils import get_device\nimport torch\n\n# Automatically select CUDA if available\ndevice = get_device()\nprint(f\"Using device: {device}\")  # cuda:0 or cpu\n\n# Force CPU usage\ndevice = get_device(force_cpu=True)\nprint(f\"Using device: {device}\")  # cpu\n\n# Use device in model\nmodel = VAE(input_dim=20000, mid_dim=1024, features=128)\nmodel = model.to(device)\n\ndata = torch.randn(32, 20000).to(device)\noutput = model(data)\n</code></pre>"},{"location":"api/utils/#renalprog.utils.get_device","title":"get_device","text":"<pre><code>get_device(force_cpu: bool = False)\n</code></pre> <p>Get the appropriate device for computation (cuda if available, else cpu).</p> <p>Args:     force_cpu: If True, force CPU usage even if CUDA is available</p> <p>Returns:     torch.device: Device to use for tensor operations</p> Source code in <code>renalprog/utils/__init__.py</code> <pre><code>def get_device(force_cpu: bool = False):\n    \"\"\"\n    Get the appropriate device for computation (cuda if available, else cpu).\n\n    Args:\n        force_cpu: If True, force CPU usage even if CUDA is available\n\n    Returns:\n        torch.device: Device to use for tensor operations\n    \"\"\"\n    if force_cpu:\n        return torch.device(\"cpu\")\n\n    if torch.cuda.is_available():\n        try:\n            # Test CUDA by creating a small tensor\n            test_tensor = torch.zeros(1).cuda()\n            del test_tensor\n            return torch.device(\"cuda\")\n        except Exception as e:\n            print(f\"Warning: CUDA is available but not functional: {e}\")\n            print(\"Falling back to CPU\")\n            return torch.device(\"cpu\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/utils/#complete-script-template","title":"Complete Script Template","text":"<p>Here's a template for a complete analysis script using all utilities:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete analysis pipeline for RenalProg.\n\nThis script demonstrates proper usage of utility functions for\nreproducibility and logging.\n\"\"\"\n\nimport logging\nfrom pathlib import Path\nimport pandas as pd\nimport torch\n\nfrom renalprog.utils import set_seed, configure_logging, get_device\nfrom renalprog.dataset import download_data, process_downloaded_data, create_train_test_split\nfrom renalprog.features import preprocess_rnaseq\nfrom renalprog.modeling.train import train_vae\nfrom renalprog.modeling.predict import apply_vae, generate_trajectories\nfrom renalprog.plots import plot_training_history\n\n# ============================================================================\n# Configuration\n# ============================================================================\n\n# Reproducibility\nSEED = 42\nset_seed(SEED)\n\n# Logging\nconfigure_logging(\n    level=logging.INFO,\n    format_string=\"%(asctime)s [%(levelname)s] %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n# Device\ndevice = get_device()\nlogger.info(f\"Using device: {device}\")\n\n# Paths\nBASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"data\"\nMODEL_DIR = BASE_DIR / \"models\" / \"my_experiment\"\nOUTPUT_DIR = BASE_DIR / \"reports\"\n\n# Ensure directories exist\nfor dir_path in [DATA_DIR, MODEL_DIR, OUTPUT_DIR]:\n    dir_path.mkdir(parents=True, exist_ok=True)\n\n# ============================================================================\n# Main Pipeline\n# ============================================================================\n\ndef main():\n    \"\"\"Run complete analysis pipeline.\"\"\"\n\n    logger.info(\"Starting RenalProg analysis pipeline\")\n\n    # Step 1: Download data\n    logger.info(\"Step 1: Downloading TCGA data\")\n    rnaseq_path, clinical_path, phenotype_path = download_data(\n        destination=DATA_DIR / \"raw\"\n    )\n\n    # Step 2: Process for KIRC\n    logger.info(\"Step 2: Processing KIRC data\")\n    rnaseq, clinical, phenotype = process_downloaded_data(\n        rnaseq_path=rnaseq_path,\n        clinical_path=clinical_path,\n        phenotype_path=phenotype_path,\n        cancer_type=\"KIRC\",\n        output_dir=DATA_DIR / \"raw\"\n    )\n\n    # Step 3: Preprocess\n    logger.info(\"Step 3: Preprocessing gene expression\")\n    rnaseq_preprocessed = preprocess_rnaseq(\n        rnaseq=rnaseq,\n        output_dir=DATA_DIR / \"interim\" / \"preprocessed\"\n    )\n\n    # Step 4: Train/test split\n    logger.info(\"Step 4: Creating train/test split\")\n    create_train_test_split(\n        rnaseq=rnaseq_preprocessed,\n        clinical=clinical,\n        test_size=0.2,\n        random_state=SEED,\n        output_dir=DATA_DIR / \"interim\" / \"split\"\n    )\n\n    # Step 5: Load split data\n    logger.info(\"Step 5: Loading split data\")\n    train_expr = pd.read_csv(\n        DATA_DIR / \"interim\" / \"split\" / \"train_expression.tsv\",\n        sep=\"\\t\", index_col=0\n    )\n    test_expr = pd.read_csv(\n        DATA_DIR / \"interim\" / \"split\" / \"test_expression.tsv\",\n        sep=\"\\t\", index_col=0\n    )\n\n    # Step 6: Train VAE\n    logger.info(\"Step 6: Training VAE\")\n    history, model, checkpoints = train_vae(\n        train_data=train_expr.values,\n        val_data=test_expr.values,\n        input_dim=train_expr.shape[1],\n        mid_dim=1024,\n        features=128,\n        output_dir=MODEL_DIR,\n        n_epochs=100,\n        batch_size=32,\n        learning_rate=1e-3,\n        device=device,\n        use_scheduler=True,\n        early_stopping_patience=20\n    )\n\n    # Step 7: Plot training history\n    logger.info(\"Step 7: Visualizing training\")\n    plot_training_history(\n        history=history,\n        output_path=OUTPUT_DIR / \"figures\" / \"training_history.png\"\n    )\n\n    # Step 8: Generate latent representations\n    logger.info(\"Step 8: Generating latent representations\")\n    results = apply_vae(\n        model=model,\n        data=test_expr.values,\n        device=device\n    )\n\n    # Step 9: Visualize latent space\n    logger.info(\"Step 9: Visualizing latent space\")\n    clinical_test = pd.read_csv(\n        DATA_DIR / \"interim\" / \"split\" / \"test_clinical.tsv\",\n        sep=\"\\t\", index_col=0\n    )\n\n\n    logger.info(\"Pipeline completed successfully!\")\n    logger.info(f\"Results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logger.error(f\"Pipeline failed: {e}\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api/utils/#best-practices","title":"Best Practices","text":""},{"location":"api/utils/#1-always-set-seed-first","title":"1. Always Set Seed First","text":"<pre><code>from renalprog.utils import set_seed\n\n# First line of your script\nset_seed(42)\n</code></pre>"},{"location":"api/utils/#2-configure-logging-early","title":"2. Configure Logging Early","text":"<pre><code>from renalprog.utils import configure_logging\n\n# Second line of your script\nconfigure_logging()\n</code></pre>"},{"location":"api/utils/#3-check-device-availability","title":"3. Check Device Availability","text":"<pre><code>from renalprog.utils import get_device\n\ndevice = get_device()\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"Running on CPU\")\n</code></pre>"},{"location":"api/utils/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<pre><code>import logging\nfrom renalprog.utils import configure_logging\n\nconfigure_logging()\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Your code here\n    results = some_function()\nexcept Exception as e:\n    logger.error(f\"Analysis failed: {e}\", exc_info=True)\n    raise\n</code></pre>"},{"location":"api/utils/#5-use-context-managers","title":"5. Use Context Managers","text":"<pre><code>import torch\nfrom renalprog.utils import get_device\n\ndevice = get_device()\n\n# Inference mode (faster, uses less memory)\nmodel.eval()\nwith torch.no_grad():\n    output = model(data.to(device))\n</code></pre>"},{"location":"api/utils/#environment-variables","title":"Environment Variables","text":"<p>You can control behavior via environment variables:</p> <pre><code># Force CPU usage\nexport CUDA_VISIBLE_DEVICES=\"\"\npython my_script.py\n\n# Use specific GPU\nexport CUDA_VISIBLE_DEVICES=1\npython my_script.py\n\n# Limit threads\nexport OMP_NUM_THREADS=4\npython my_script.py\n</code></pre>"},{"location":"api/utils/#reproducibility-checklist","title":"Reproducibility Checklist","text":"<p>For fully reproducible results:</p> <ul> <li>\u2705 Set random seed with <code>set_seed()</code></li> <li>\u2705 Use fixed <code>random_state</code> parameters</li> <li>\u2705 Set <code>torch.backends.cudnn.deterministic = True</code></li> <li>\u2705 Document package versions</li> <li>\u2705 Save configuration parameters</li> <li>\u2705 Version control code</li> <li>\u2705 Track data provenance</li> </ul> <pre><code>from renalprog.utils import set_seed\nimport torch\nimport numpy as np\nimport pandas as pd\nimport json\n\n# Reproducibility\nset_seed(42)\n\n# Save configuration\nconfig = {\n    'seed': 42,\n    'torch_version': torch.__version__,\n    'numpy_version': np.__version__,\n    'pandas_version': pd.__version__,\n    'cuda_available': torch.cuda.is_available(),\n    'cudnn_deterministic': torch.backends.cudnn.deterministic,\n    'cudnn_benchmark': torch.backends.cudnn.benchmark\n}\n\nwith open('config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n</code></pre>"},{"location":"api/utils/#see-also","title":"See Also","text":"<ul> <li>Configuration API - Project configuration</li> <li>Complete Pipeline Tutorial</li> <li>Contributing Guide</li> </ul>"},{"location":"contributing/guidelines/","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to renalprog! This document provides essential guidelines for contributing to the project.</p> <p>Respect</p> <p>Be respectful. Any issue or PR that contains disrespectful, unprofessional language, personal attacks, etc. will be disregarded and may be reported.</p>"},{"location":"contributing/guidelines/#quick-start","title":"Quick Start","text":"<ol> <li>Fork the repository</li> <li>Clone your fork: <code>git clone https://github.com/YOUR_USERNAME/renalprog.git</code></li> <li>Create a branch: <code>git checkout -b feature/your-feature-name</code></li> <li>Make changes following our guidelines</li> <li>Test your changes</li> <li>Commit: <code>git commit -m \"Description of changes\"</code></li> <li>Push: <code>git push origin feature/your-feature-name</code></li> <li>Open a Pull Request</li> </ol>"},{"location":"contributing/guidelines/#code-standards","title":"Code Standards","text":""},{"location":"contributing/guidelines/#python-code","title":"Python Code","text":"<ul> <li>Style: Follow PEP 8</li> <li>Formatting: Use <code>ruff</code> for code formatting</li> <li>Type hints: Add type hints to function signatures</li> <li>Docstrings: Use Google-style docstrings.</li> </ul> <p>Example: <pre><code>def process_data(\n    data: pd.DataFrame,\n    threshold: float = 0.05\n) -&gt; Tuple[pd.DataFrame, Dict[str, float]]:\n    \"\"\"\n    Process input data and return results.\n\n    Args:\n        data: Input dataframe with gene expression\n        threshold: Significance threshold (default: 0.05)\n\n    Returns:\n        Tuple of (processed_data, statistics_dict)\n\n    Examples:\n        &gt;&gt;&gt; result, stats = process_data(df, threshold=0.01)\n    \"\"\"\n    # Implementation\n    pass\n</code></pre></p>"},{"location":"contributing/guidelines/#r-code","title":"R Code","text":"<ul> <li>Style: Follow tidyverse style guide</li> <li>Documentation: Use roxygen2-style comments</li> <li>Naming: Use snake_case for functions and variables</li> </ul> <p>Example: <pre><code>#' Process Gene Expression Data\n#'\n#' @param data A data.frame with gene expression values\n#' @param alpha Significance threshold (default: 0.05)\n#' @return A list with processed data and statistics\n#' @export\nprocess_gene_data &lt;- function(data, alpha = 0.05) {\n  # Implementation\n}\n</code></pre></p>"},{"location":"contributing/guidelines/#documentation","title":"Documentation","text":""},{"location":"contributing/guidelines/#code-documentation","title":"Code Documentation","text":"<ul> <li>All public functions must have docstrings</li> <li>Include examples when helpful</li> <li>Document parameters and return values</li> <li>Note exceptions that may be raised</li> </ul>"},{"location":"contributing/guidelines/#user-documentation","title":"User Documentation","text":"<p>When adding new features, update:</p> <ul> <li><code>README.md</code> - If changing installation or quick start</li> <li><code>docs/docs/api/</code> - API reference for new modules/functions</li> <li><code>docs/docs/tutorials/</code> - Tutorial if introducing new workflow</li> <li><code>CHANGELOG.md</code> - Document changes</li> </ul>"},{"location":"contributing/guidelines/#building-documentation","title":"Building Documentation","text":"<pre><code>cd docs\nmkdocs serve\nmkdocs build\n</code></pre>"},{"location":"contributing/guidelines/#commit-messages","title":"Commit Messages","text":"<p>Use clear, descriptive commit messages following this format:</p> <pre><code>&lt;type&gt;: &lt;short summary&gt;\n\n&lt;optional detailed description&gt;\n</code></pre> <p>Types: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation changes - <code>style:</code> - Code style changes (formatting, no logic change) - <code>refactor:</code> - Code refactoring - <code>test:</code> - Adding or updating tests - <code>chore:</code> - Maintenance tasks</p> <p>Example: <pre><code>fix: correct control file naming in 1_data_processing.py\n\nChanged from plural (rnaseq_controls.csv) to singular \n(rnaseq_control.csv) to match actual file names in repository.\n</code></pre></p>"},{"location":"contributing/guidelines/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/guidelines/#before-submitting","title":"Before Submitting","text":"<ul> <li> Code follows style guidelines</li> <li> Tests pass locally (<code>pytest</code>)</li> <li> New tests added for new features</li> <li> Documentation updated</li> <li> Commit messages are clear</li> <li> Branch is up to date with <code>main</code></li> </ul>"},{"location":"contributing/guidelines/#pr-description-template","title":"PR Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Documentation update\n- [ ] Refactoring\n\n## Testing\nDescribe testing performed\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Tests added/updated\n- [ ] Documentation updated\n- [ ] All tests pass\n</code></pre>"},{"location":"contributing/guidelines/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass (tests, linting)</li> <li>Code review by maintainer</li> <li>Changes requested may need addressing</li> <li>Approval and merge by maintainer</li> </ol>"},{"location":"contributing/guidelines/#development-setup","title":"Development Setup","text":""},{"location":"contributing/guidelines/#environment-setup","title":"Environment Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\n\n# Create conda environment\nmamba env create -f environment.yml\nmamba activate renalprog\n\n# Install in editable mode\npip install -e .\n\n# Install development dependencies\npip install pytest ruff flake8 mypy\n</code></pre>"},{"location":"contributing/guidelines/#code-review-guidelines","title":"Code Review Guidelines","text":""},{"location":"contributing/guidelines/#for-contributors","title":"For Contributors","text":"<ul> <li>Keep PRs focused - One feature/fix per PR</li> <li>Write clear descriptions - Explain what and why</li> <li>Respond to feedback - Address review comments</li> <li>Be patient </li> </ul>"},{"location":"contributing/guidelines/#for-reviewers","title":"For Reviewers","text":"<ul> <li>Be constructive - Suggest improvements clearly</li> <li>Explain reasoning - Help contributors learn</li> <li>Focus on substance - Not just style</li> <li>Acknowledge good work - Positive feedback matters</li> </ul>"},{"location":"contributing/guidelines/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/guidelines/#bug-reports","title":"Bug Reports","text":"<p>Include: - Description of the bug - Steps to reproduce - Expected behavior - Actual behavior - Environment (OS, Python version, package versions) - Error messages (full traceback)</p> <p>Example: <pre><code>**Description:** VAE training fails with GPU\n\n**Steps to reproduce:**\n1. Set `force_cpu=False`\n2. Run `train_vae_with_postprocessing(...)`\n\n**Expected:** Training completes successfully\n\n**Actual:** RuntimeError: CUDA out of memory\n\n**Environment:**\n- OS: Ubuntu 20.04\n- Python: 3.9.7\n- PyTorch: 1.10.0\n- GPU: NVIDIA GTX 1080 (8GB)\n\n**Error:**\n\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB...\n</code></pre></p>"},{"location":"contributing/guidelines/#feature-requests","title":"Feature Requests","text":"<p>Include: - Use case - Why is this needed? - Proposed solution - How should it work? - Alternatives - Other approaches considered? - Additional context - Examples, references</p>"},{"location":"contributing/guidelines/#project-structure","title":"Project Structure","text":"<p>Understanding the project layout:</p> <pre><code>renalprog/\n\u251c\u2500\u2500 renalprog/          # Main package\n\u2502   \u251c\u2500\u2500 config.py       # Configuration\n\u2502   \u251c\u2500\u2500 dataset.py      # Data loading\n\u2502   \u251c\u2500\u2500 features.py     # Feature engineering\n\u2502   \u251c\u2500\u2500 enrichment.py   # Enrichment analysis\n\u2502   \u251c\u2500\u2500 plots.py        # Visualization\n\u2502   \u251c\u2500\u2500 modeling/       # Models (VAE, classification)\n\u2502   \u2514\u2500\u2500 utils/          # Utilities\n\u251c\u2500\u2500 scripts/            # Analysis scripts\n\u2502   \u251c\u2500\u2500 pipeline_steps/ # Main pipeline\n\u2502   \u2514\u2500\u2500 r_analysis/     # R scripts\n\u251c\u2500\u2500 tests/              # Test suite\n\u251c\u2500\u2500 docs/               # Documentation\n\u251c\u2500\u2500 data/               # Data directories\n\u2514\u2500\u2500 notebooks/          # Jupyter notebooks\n</code></pre>"},{"location":"contributing/guidelines/#questions","title":"Questions?","text":"<ul> <li>Documentation: Check docs/</li> <li>Issues: Search existing issues</li> <li>Discussions: Start a discussion</li> <li>Email: Contact maintainers</li> </ul>"},{"location":"contributing/guidelines/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"contributing/guidelines/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful - Treat everyone with respect</li> <li>Be collaborative - Work together constructively</li> <li>Be professional - Maintain professional conduct</li> <li>Be inclusive - Welcome diverse perspectives</li> </ul>"},{"location":"contributing/guidelines/#unacceptable-behavior","title":"Unacceptable Behavior","text":"<ul> <li>Harassment or discrimination</li> <li>Trolling or insulting comments</li> <li>Personal or political attacks</li> <li>Publishing others' private information</li> </ul>"},{"location":"contributing/guidelines/#enforcement","title":"Enforcement","text":"<p>Inappropriate behavior may be reported.</p>"},{"location":"contributing/guidelines/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"contributing/guidelines/#recognition","title":"Recognition","text":"<p>Contributors will be acknowledged in: - Acknowledgments - Release notes for significant contributions - Documentation where appropriate</p> <p>Thank you for contributing to renalprog! \ud83c\udf89</p>"},{"location":"tutorials/","title":"Tutorials Overview","text":"<p>This section provides comprehensive tutorials for using <code>renalprog</code> to analyze kidney cancer progression. Each tutorial is designed to be self-contained yet builds upon previous steps.</p>"},{"location":"tutorials/#tutorial-structure","title":"Tutorial Structure","text":""},{"location":"tutorials/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>New to <code>renalprog</code>? Start here!</p> <ul> <li>Quick Start: 10-minute introduction to core functionality</li> <li>Using Pretrained Models: Fastest way to reproduce paper results</li> <li>Data Requirements: Understanding input data formats</li> </ul>"},{"location":"tutorials/#complete-pipeline","title":"\ud83d\udccb Complete Pipeline","text":"<p>Step-by-step walkthrough of the entire analysis pipeline:</p> <ol> <li>Data Processing: Download, filter, and preprocess TCGA data</li> <li>VAE Training: Train variational autoencoders</li> <li>Reconstruction Validation: Assess model quality</li> <li>Trajectory Generation: Create synthetic progression paths</li> <li>Classification: Stage prediction and biomarker discovery</li> <li>Enrichment Analysis: Pathway analysis with GSEA</li> </ol>"},{"location":"tutorials/#visualization","title":"\ud83c\udfa8 Visualization","text":"<ul> <li>Visualization Guide: Create publication-quality figures</li> </ul>"},{"location":"tutorials/#for-reproducing-published-results","title":"For Reproducing Published Results","text":"<p>If you want to reproduce the paper:</p> <p>Option 1: Using Pretrained Models (Recommended)</p> <ol> <li>Follow Using Pretrained Models tutorial</li> <li>This is the fastest and most accurate way to reproduce results</li> <li>Uses the exact models from the paper</li> </ol> <p>Option 2: Training from Scratch</p>"},{"location":"tutorials/#tutorial-conventions","title":"Tutorial Conventions","text":""},{"location":"tutorials/#code-blocks","title":"Code Blocks","text":"<p>Python code to execute: <pre><code>from renalprog import dataset\ndata = dataset.load_data('path/to/data.csv')\n</code></pre></p> <p>Shell commands: <pre><code>python scripts/pipeline_steps/1_data_processing.py\n</code></pre></p>"},{"location":"tutorials/#callouts","title":"Callouts","text":"<p>Note</p> <p>Informational notes provide additional context.</p> <p>Tip</p> <p>Tips offer helpful suggestions and best practices.</p> <p>Warning</p> <p>Warnings highlight potential issues or common pitfalls.</p> <p>Danger</p> <p>Critical warnings about data loss or major errors.</p> <p>Example</p> <p>Example outputs or usage patterns.</p>"},{"location":"tutorials/#file-paths","title":"File Paths","text":"<p>All file paths are relative to the repository root unless otherwise specified:</p> <pre><code>renalprog/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/           # Downloaded TCGA data\n\u2502   \u251c\u2500\u2500 interim/       # Intermediate processing outputs\n\u2502   \u2514\u2500\u2500 processed/     # Final processed data\n\u251c\u2500\u2500 models/            # Trained models\n\u251c\u2500\u2500 reports/           # Analysis results\n\u2514\u2500\u2500 scripts/           # Pipeline scripts\n</code></pre>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before starting these tutorials, ensure you have:</p> <ol> <li>Installed <code>renalprog</code> (Installation Guide)</li> <li>Python 3.9+ and R 4.0+ available</li> <li>(Optional) CUDA-capable GPU for faster training</li> </ol>"},{"location":"tutorials/#getting-help","title":"Getting Help","text":"<p>If you encounter issues while following these tutorials:</p> <ol> <li>Review the API Reference for function details</li> <li>Search GitHub Issues</li> <li>Ask in GitHub Discussions</li> </ol>"},{"location":"tutorials/#next-steps","title":"Next Steps","text":"<p>Ready to begin? Start with the Quick Start Tutorial!</p>"},{"location":"tutorials/complete-pipeline/","title":"Complete Pipeline Tutorial","text":"<p>This tutorial walks through the complete <code>renalprog</code> analysis pipeline from raw TCGA data to pathway enrichment results. This represents the full workflow used in the publication.</p>"},{"location":"tutorials/complete-pipeline/#overview","title":"Overview","text":"<p>The pipeline consists of six main steps:</p> <ol> <li>Data Processing: Download and preprocess TCGA data</li> <li>VAE Training: Train deep generative models  </li> <li>Reconstruction Validation: Assess model quality</li> <li>Trajectory Generation: Create synthetic progression paths</li> <li>Classification: Stage prediction and biomarker discovery</li> <li>Enrichment Analysis: Pathway-level interpretation</li> </ol> <p>Note</p> <p>We recommend using HPC for steps 2 and 6 due to computational demands. Step 2 will benefit from GPU acceleration, while step 6 can be parallelized across multiple CPU cores.</p>"},{"location":"tutorials/complete-pipeline/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have completed the installation: see Installation Guide.</p>"},{"location":"tutorials/complete-pipeline/#pipeline-execution","title":"Pipeline Execution","text":""},{"location":"tutorials/complete-pipeline/#option-1-run-all-steps-sequentially","title":"Option 1: Run All Steps Sequentially","text":"<pre><code># Navigate to scripts directory\ncd scripts/pipeline_steps\n\n# Step 1: Data Processing (30 minutes)\npython 1_data_processing.py\n\n# Step 2: VAE Training (2-4 hours with GPU)\npython 2_models.py\n\n# Step 3: Reconstruction Check (10 minutes)\npython 3_check_reconstruction.py\n\n# Step 4: Trajectory Generation (30 minutes)\npython 4_trajectories.py\n\n# Step 5: Classification (1 hour)\npython 5_classification.py\n</code></pre>"},{"location":"tutorials/complete-pipeline/#option-2-run-with-makefile","title":"Option 2: Run with Makefile","text":"<pre><code># Run entire pipeline\nmake pipeline\n\n# Or run individual steps\nmake data\nmake models\nmake trajectories\nmake classification\n</code></pre>"},{"location":"tutorials/complete-pipeline/#detailed-step-by-step-guide","title":"Detailed Step-by-Step Guide","text":""},{"location":"tutorials/complete-pipeline/#step-1-data-processing","title":"Step 1: Data Processing","text":"<p>Script: <code>scripts/pipeline_steps/1_data_processing.py</code></p> <p>What it does: - Downloads TCGA KIRC RNA-seq and clinical data - Filters low-expression genes - Removes outliers using Mahalanobis distance - Creates train/test splits</p> <p>Expected outputs: <pre><code>data/interim/preprocessed_KIRC_data/\n\u251c\u2500\u2500 preprocessed_rnaseq.csv         # Filtered gene expression (498 samples \u00d7 ~5000 genes)\n\u251c\u2500\u2500 preprocessing_info.json         # Filter statistics\n\u2514\u2500\u2500 stages.csv                      # Cancer stage labels\n</code></pre></p> <p>Validation: <pre><code>import pandas as pd\n\nrnaseq = pd.read_csv('data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv', index_col=0)\nprint(f\"Samples: {rnaseq.shape[0]}, Genes: {rnaseq.shape[1]}\")\n# Expected: Samples: 498, Genes: ~5000\n\n# Check for missing values\nassert rnaseq.isnull().sum().sum() == 0, \"Missing values detected!\"\nprint(\"\u2713 Data quality check passed\")\n</code></pre></p>"},{"location":"tutorials/complete-pipeline/#step-2-vae-training","title":"Step 2: VAE Training","text":"<p>Script: <code>scripts/pipeline_steps/2_models.py</code></p> <p>What it does: - Trains Variational Autoencoder on gene expression - Learns low-dimensional latent representation - Trains reconstruction network for improved decoding - Saves models and training histories</p> <p>Configuration (in script): <pre><code>vae_config.INPUT_DIM = X_train.shape[1]  # Number of genes\nvae_config.MID_DIM = 512                  # Hidden layer size\nvae_config.LATENT_DIM = 256               # Latent space dimensionality\nvae_config.EPOCHS = 600                   # Total epochs (3 cycles \u00d7 200)\nvae_config.BATCH_SIZE = 8\n</code></pre></p> <p>Expected outputs: <pre><code>models/20251217_models_KIRC/\n\u251c\u2500\u2500 vae/\n\u2502   \u251c\u2500\u2500 vae_model.pt                 # Trained VAE\n\u2502   \u251c\u2500\u2500 vae_config.json              # Model configuration\n\u2502   \u2514\u2500\u2500 vae_training_history.png     # Loss curves\n\u251c\u2500\u2500 reconstruction/\n\u2502   \u251c\u2500\u2500 reconstruction_network.pt    # Post-processing network\n\u2502   \u2514\u2500\u2500 reconstruction_network_history.png\n\u2514\u2500\u2500 training_data/\n    \u251c\u2500\u2500 X_train.csv\n    \u2514\u2500\u2500 X_test.csv\n</code></pre></p> <p>Validation: <pre><code>import torch\nfrom renalprog.modeling.models import VAE\n\n# Load model\nvae = VAE.load('models/20251217_models_KIRC/vae/vae_model.pt')\n\n# Check reconstruction\nX_test = pd.read_csv('models/20251217_models_KIRC/training_data/X_test.csv', index_col=0)\nX_tensor = torch.FloatTensor(X_test.values)\n\nwith torch.no_grad():\n    X_recon = vae(X_tensor)[0]\n    mse = ((X_tensor - X_recon) ** 2).mean()\n    print(f\"Reconstruction MSE: {mse:.4f}\")\n    # Expected: MSE &lt; 1.0 for well-trained model\n</code></pre></p>"},{"location":"tutorials/complete-pipeline/#step-3-reconstruction-validation","title":"Step 3: Reconstruction Validation","text":"<p>Script: <code>scripts/pipeline_steps/3_check_reconstruction.py</code></p> <p>What it does: - Visualizes latent space with UMAP/t-SNE - Compares original vs. reconstructed gene expression - Generates quality control plots</p> <p>Expected outputs: <pre><code>reports/figures/reconstruction/\n\u251c\u2500\u2500 latent_space_umap.png           # UMAP colored by stage\n\u251c\u2500\u2500 latent_space_tsne.png           # t-SNE colored by stage\n\u251c\u2500\u2500 reconstruction_scatter.png      # Original vs. reconstructed\n\u2514\u2500\u2500 gene_correlation.png            # Per-gene reconstruction quality\n</code></pre></p> <p>Validation: <pre><code># Check that plots exist\nimport os\n\nplot_dir = 'reports/figures/reconstruction/'\nrequired_plots = [\n    'latent_space_umap.png',\n    'reconstruction_scatter.png'\n]\n\nfor plot in required_plots:\n    assert os.path.exists(os.path.join(plot_dir, plot)), f\"Missing: {plot}\"\nprint(\"\u2713 All validation plots generated\")\n</code></pre></p>"},{"location":"tutorials/complete-pipeline/#step-4-trajectory-generation","title":"Step 4: Trajectory Generation","text":"<p>Script: <code>scripts/pipeline_steps/4_trajectories.py</code></p> <p>What it does: - Generates synthetic patient trajectories - Interpolates between early and late stage samples - Decodes trajectories to gene expression space - Creates multiple trajectory types</p> <p>Configuration: <pre><code>n_trajectories = 500                # Number of trajectories to generate\nn_timepoints = 20                   # Timepoints per trajectory\ntrajectory_types = [\n    'early_to_late',                # Early stage \u2192 Late stage\n    'stage1_to_stage4',             # Stage I \u2192 Stage IV\n    'matched_pairs'                 # Patient-specific progressions\n]\n</code></pre></p> <p>Expected outputs: <pre><code>data/interim/20251217_synthetic_data/kirc/\n\u251c\u2500\u2500 early_to_late/\n\u2502   \u251c\u2500\u2500 TCGA-XXX-YYY_to_TCGA-AAA-BBB.csv\n\u2502   \u251c\u2500\u2500 TCGA-XXX-ZZZ_to_TCGA-CCC-DDD.csv\n\u2502   \u2514\u2500\u2500 ...                         # 500 trajectory files\n\u251c\u2500\u2500 trajectory_metadata.csv         # Start/end points, stages\n\u2514\u2500\u2500 generation_params.json          # Generation parameters\n</code></pre></p> <p>Validation: <pre><code>import pandas as pd\nimport glob\n\n# Count trajectories\ntrajectory_files = glob.glob('data/interim/*/kirc/early_to_late/*.csv')\nprint(f\"Generated {len(trajectory_files)} trajectories\")\n# Expected: 500 trajectories\n\n# Check trajectory format\ntraj = pd.read_csv(trajectory_files[0], index_col=0)\nprint(f\"Trajectory shape: {traj.shape}\")\n# Expected: (20 timepoints, ~5000 genes)\n\nassert traj.shape[0] == 20, \"Wrong number of timepoints\"\nassert traj.isnull().sum().sum() == 0, \"Missing values in trajectory\"\nprint(\"\u2713 Trajectory validation passed\")\n</code></pre></p>"},{"location":"tutorials/complete-pipeline/#step-5-classification","title":"Step 5: Classification","text":"<p>Script: <code>scripts/pipeline_steps/5_classification.py</code></p> <p>What it does: - Trains XGBoost classifier for stage prediction - Calculates SHAP values for interpretability - Identifies important gene signatures - Evaluates performance with cross-validation</p> <p>Expected outputs: <pre><code>models/20251217_classification_kirc/\n\u251c\u2500\u2500 classifier.pkl                  # Trained XGBoost model\n\u251c\u2500\u2500 classification_metrics.json     # Accuracy, ROC AUC, etc.\n\u251c\u2500\u2500 shap_values.npy                 # SHAP importance values\n\u251c\u2500\u2500 important_genes.csv             # Top biomarker genes\n\u2514\u2500\u2500 figures/\n    \u251c\u2500\u2500 confusion_matrix.png\n    \u251c\u2500\u2500 roc_curve.png\n    \u251c\u2500\u2500 shap_summary.png\n    \u2514\u2500\u2500 shap_waterfall.png\n</code></pre></p> <p>Validation: <pre><code>import json\n\n# Check performance metrics\nwith open('models/20251217_classification_kirc/classification_metrics.json') as f:\n    metrics = json.load(f)\n\nprint(f\"Test Accuracy: {metrics['test_accuracy']:.3f}\")\nprint(f\"ROC AUC: {metrics['test_roc_auc']:.3f}\")\n\n# Expected performance for KIRC dataset\nassert metrics['test_accuracy'] &gt; 0.85, \"Low accuracy!\"\nassert metrics['test_roc_auc'] &gt; 0.90, \"Low ROC AUC!\"\nprint(\"\u2713 Classification performance acceptable\")\n</code></pre></p>"},{"location":"tutorials/complete-pipeline/#step-6-enrichment-analysis","title":"Step 6: Enrichment Analysis","text":"<p>For the full tutorial, see: Enrichment Tutorial</p> <p>Script: <code>scripts/enrichment/pipeline.sh</code></p> <p>What it does: - Calculates differential expression (DESeq-style) for each timepoint - Runs GSEA on ranked gene lists - Identifies enriched pathways along trajectories - Combines results into single dataset</p> <p>Prerequisites: - GSEA CLI tool installed (installation guide) - ReactomePathways.gmt in <code>data/external/</code></p> <p>Expected outputs: <pre><code>output_dir/\n\u251c\u2500\u2500 test_to_test/                                          # Synthetic trajectories\n\u2502   \u251c\u2500\u2500 early_to_late/                                     # Transition type\n\u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient2/                          # Patient trajectory\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient_0.rnk                  # Ranked gene list for timepoint 0\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient_1.rnk\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 reports/                                   # GSEA output for all patients in directory\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 patient1_to_patient_0.GseaPreranked.*  # GSEA output files\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 gsea_report_for_na_pos_*.tsv           # Positive enrichment report\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 gsea_report_for_na_neg_*.tsv           # Negative enrichment report\n\u2502   \u2502   \u251c\u2500\u2500patient3_to_patient4/\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 gsea_commands_*.cmd                                # GSEA command files\n\u251c\u2500\u2500 full_gsea_reports_kirc.csv                             # Final combined results\n\u2514\u2500\u2500 heatmap_kirc_significantNES.csv                        # Significant pathways heatmap data\n</code></pre></p> <p>Final result format: <pre><code>Patient,Idx,Transition,NAME,ES,NES,FDR q-val\nTCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01,0,early_to_late,Cell Cycle,0.65,2.13,0.001\nTCGA-3Z-A93Z-01_to_TCGA-A3-A8OW-01,0,early_to_late,DNA Repair,0.52,1.87,0.012\n...\n</code></pre></p> <p>The Idx column represents the timepoint along the trajectory.</p> <p>Time: approximate times for KIRC, after generating 50 timepoints of 279 trajectories: - DESeq analysis: 8 hours, running in 10 nodes with 112 CPUs each (no multithreading), and assigning 21 CPUs per task, with 2GB per CPU. - GSEA: 30 minutes, running in 5 nodes with 64 CPUs each (no multithreading), and assigning 1 CPUs per task, with 2GB per CPU.</p>"},{"location":"tutorials/complete-pipeline/#citation","title":"Citation","text":"<p>If you use this pipeline in your research, please cite this work. For details, see the citation page file.</p>"},{"location":"tutorials/data-requirements/","title":"Data Requirements","text":"<p>Understanding the input data formats and requirements for <code>renalprog</code>.</p> <p>Note</p> <p>Even though the package has been used with bulk RNA-seq data and taking cancer stages as labels, the VAE-based pipeline is data and label agnostic. You can use this pipeline with other types of tabular data. For supervised tasks, you can use a CVAE instead of the VAE. For this, please check the API reference.</p>"},{"location":"tutorials/data-requirements/#overview","title":"Overview","text":"<p><code>renalprog</code> is designed to work with gene expression data and clinical annotations. The primary use case is TCGA (The Cancer Genome Atlas) data, but the package can work with any properly formatted RNA-seq or microarray data.</p>"},{"location":"tutorials/data-requirements/#input-data-formats","title":"Input Data Formats","text":""},{"location":"tutorials/data-requirements/#1-gene-expression-data","title":"1. Gene Expression Data","text":"<p>Format: CSV or TSV file with genes as columns and samples as rows (or vice versa).</p> <p>Example (<code>rnaseq.csv</code>): <pre><code>,Gene1,Gene2,Gene3,...,GeneN\nSample1,12.5,8.3,0.1,...,5.7\nSample2,13.1,7.9,0.0,...,6.2\nSample3,11.8,9.2,0.3,...,5.1\n...\n</code></pre></p> <p>Warning</p> <p>The datasets used by default in this package are in log2-transformed RSEM normalized counts. Before training, the pipeline automatically re-scales the data with the MinMaxScaler from scikit-learn to the [0, 1] range. You should not train the models with raw counts.</p>"},{"location":"tutorials/data-requirements/#2-clinical-data","title":"2. Clinical Data","text":"<p>Format: CSV or TSV file with samples as rows and clinical variables as columns.</p> <p>Example (<code>clinical.csv</code>): <pre><code>sample_id,ajcc_pathologic_tumor_stage,age,gender,survival_days\nSample1,Stage I,65,M,1825\nSample2,Stage IIIA,72,F,730\nSample3,Stage II,58,M,2190\n...\n</code></pre></p> <p>Required Columns:</p> <ul> <li><code>sample_id</code>: Must match gene expression sample IDs</li> <li><code>ajcc_pathologic_tumor_stage</code>: Cancer stage (or similar staging variable)</li> <li>Examples: \"Stage I\", \"Stage II\", \"Stage IIIA\", \"Stage IV\"</li> </ul> <p>You can specify a different column name for staging when loading data, but you need to pass the name to the functions. By default, <code>dataset.load_clinical_data()()</code> looks for <code>ajcc_pathologic_tumor_stage</code>, but can be changed with the <code>stage_column</code>parameter. See the API reference for details.</p> <p>Optional Columns:</p> <ul> <li><code>age</code>: Patient age at diagnosis</li> <li><code>gender</code> or <code>sex</code>: M/F or Male/Female</li> </ul> <p>These columns may be used for synthetic trajectory generation. See the tutorial on synthetic trajectory generation for details.</p> <p>The pipeline automatically groups stages:</p> <ul> <li>Early: Stage I, II</li> <li>Late: Stage III, IV</li> </ul>"},{"location":"tutorials/data-requirements/#tcga-data","title":"TCGA Data","text":""},{"location":"tutorials/data-requirements/#downloading-tcga-data","title":"Downloading TCGA Data","text":"<p>The package includes utilities to download TCGA data from UCSC Xena Browser:</p> <pre><code>from renalprog import dataset\n\n# Download KIRC data\nrnaseq, clinical, pheno = dataset.download_data(\n    destination='data/raw',\n    cancer_type='KIRC',\n    remove_gz=True\n)\n</code></pre>"},{"location":"tutorials/data-requirements/#tcga-data-structure","title":"TCGA Data Structure","text":"<p>TCGA data includes:</p> <ol> <li>Gene Expression (20,531 genes initially)</li> <li>Platform: Illumina HiSeq</li> <li>Normalization: log2(RSEM+1) </li> <li> <p>File: <code>EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena</code></p> </li> <li> <p>Clinical Data</p> </li> <li>Curated clinical annotations, including survival data.</li> <li> <p>File: <code>Survival_SupplementalTable_S1_20171025_xena_sp</code></p> </li> <li> <p>Phenotype (sample metadata)</p> </li> <li>Sample types (primary tumor, normal, metastatic)</li> <li>File: <code>TCGA_phenotype_denseDataOnlyDownload.tsv</code></li> </ol>"},{"location":"tutorials/data-requirements/#custom-non-tcga-data","title":"Custom (Non-TCGA) Data","text":""},{"location":"tutorials/data-requirements/#preparing-your-own-data","title":"Preparing Your Own Data","text":"<p>If you have your own gene expression data:</p>"},{"location":"tutorials/data-requirements/#step-1-format-gene-expression","title":"Step 1: Format Gene Expression","text":"<pre><code>import pandas as pd\n\n# Load your data\n# Assume you have a samples \u00d7 genes matrix\nyour_data = pd.read_csv('your_rnaseq_data.csv', index_col=0)\n\n# Ensure proper format\nassert your_data.shape[0] &lt; your_data.shape[1], \"Transpose if needed\"\n# Should have more genes than samples\n\n# Check for missing values\nassert not your_data.isnull().any().any(), \"Remove missing values\"\n\n# Save in standard format\nyour_data.to_csv('data/raw/your_data_formatted.csv')\n</code></pre>"},{"location":"tutorials/data-requirements/#step-2-format-clinical-data","title":"Step 2: Format Clinical Data","text":"<pre><code># Create clinical file\nclinical = pd.DataFrame({\n    'sample_id': your_data.index,\n    'ajcc_pathologic_tumor_stage': your_stages,  # e.g., [\"Stage I\", \"Stage III\", ...]\n    'race': your_races,\n    'gender': your_gender\n})\n\n# Map to early/late if needed\nstage_map = {\n    'Stage I': 'early',\n    'Stage II': 'early',\n    'Stage III': 'late',\n    'Stage IV': 'late'\n}\nclinical['stage_binary'] = clinical['ajcc_pathologic_tumor_stage'].map(stage_map)\n\nclinical.to_csv('data/raw/your_clinical_formatted.csv', index=False)\n</code></pre>"},{"location":"tutorials/data-requirements/#step-3-validate","title":"Step 3: Validate","text":"<pre><code>from renalprog import dataset\n\n# Try loading\nrnaseq = dataset.load_data('data/raw/your_data_formatted.csv')\nclinical = dataset.load_data('data/raw/your_clinical_formatted.csv')\n\n# Check alignment\nassert set(rnaseq.index) == set(clinical['sample_id']), \"Sample IDs must match!\"\n\nprint(f\"Samples: {rnaseq.shape[0]}\")\nprint(f\"Genes: {rnaseq.shape[1]}\")\nprint(f\"Stages: {clinical['ajcc_pathologic_tumor_stage'].value_counts()}\")\n</code></pre>"},{"location":"tutorials/data-requirements/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"tutorials/data-requirements/#automatic-preprocessing","title":"Automatic Preprocessing","text":"<p>The pipeline automatically:</p> <ol> <li>Filters low-expression genes</li> <li>Removes genes with low mean expression</li> <li>Removes genes with low variance</li> <li> <p>Keeps genes expressed in \u226520% of samples</p> </li> <li> <p>Detects outliers</p> </li> <li>Uses Mahalanobis distance</li> <li>Removes samples &gt;3 SD from mean</li> <li> <p>Configurable significance level</p> </li> <li> <p>Normalizes (optional)</p> </li> <li>Log transformation</li> <li>Z-score normalization</li> <li>Quantile normalization</li> </ol>"},{"location":"tutorials/data-requirements/#example-datasets","title":"Example Datasets","text":""},{"location":"tutorials/data-requirements/#kirc-kidney-renal-clear-cell-carcinoma","title":"KIRC (Kidney Renal Clear Cell Carcinoma)","text":"<p>The main dataset used in publication:</p> <ul> <li>Samples: 533 (530 with stage)</li> <li>Genes: 20,531 (8,516 after preprocessing for feature filtering)</li> <li>Source: TCGA via UCSC Xena</li> </ul>"},{"location":"tutorials/data-requirements/#other-tcga-cancers","title":"Other TCGA Cancers","text":"<p>The pipeline works with other TCGA cancers:</p> <pre><code># Download different cancer type\nrnaseq, clinical, pheno = dataset.download_data(\n    destination='data/raw',\n    cancer_type='BRCA'  # or LUAD, COAD, etc.\n)\n</code></pre> <p>For example: BRCA, LUAD, LUSC, COAD, STAD, LIHC, PRAD, etc.</p>"},{"location":"tutorials/data-requirements/#data-looks-weird","title":"\"Data looks weird\"","text":"<pre><code># Check if data needs transformation\nprint(\"Min:\", rnaseq.min().min())\nprint(\"Max:\", rnaseq.max().max())\n\n# If values are large (&gt;100), likely not log-transformed\nif rnaseq.max().max() &gt; 100:\n    rnaseq_log = np.log2(rnaseq + 1)\n    rnaseq = rnaseq_log\n</code></pre>"},{"location":"tutorials/data-requirements/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Tutorial</li> <li>Data Processing Pipeline</li> <li>API Reference</li> </ul>"},{"location":"tutorials/installation/","title":"Installation","text":"<p><code>renalprog</code> is a heavy package. Installation requires about 4.5 GB of disk space and a stable internet connection.</p>"},{"location":"tutorials/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>R 4.0+ (for enrichment analysis) - Can be installed via conda/mamba (recommended)</li> <li>Conda or Mamba (recommended for environment management)</li> <li>CUDA-capable GPU (optional, for faster VAE training)</li> </ul>"},{"location":"tutorials/installation/#recommended-using-mambaconda-uv","title":"Recommended: Using Mamba/Conda + uv","text":""},{"location":"tutorials/installation/#quick-setup-with-environmentyml-easiest","title":"Quick Setup with environment.yml (Easiest)","text":"<pre><code># Clone repository\ngit clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\n\n# Create environment from file (includes Python, R, and all dependencies)\nmamba env create -f environment.yml\nmamba activate renalprog\n\n# Install the package in editable mode\npip install -e .\n</code></pre>"},{"location":"tutorials/installation/#manual-setup","title":"Manual Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\n\n# Create environment with Python 3.9 AND R\nmamba create -n renalprog \"python==3.9\" \"r-base&gt;=4.0\"\nmamba activate renalprog\n\n# Install R packages via conda (recommended for reproducibility)\nmamba install -c conda-forge r-gprofiler2 r-ggplot2 r-optparse\n\n# Install uv for faster Python package management\npip install uv\n\n# Install Python package\nuv pip install -e .\n\n# Install testing dependencies\nuv pip install pytest pytest-cov\n</code></pre> <p>Note: Installing R via conda/mamba ensures all dependencies are managed in the same environment, improving reproducibility.</p>"},{"location":"tutorials/installation/#alternative-using-pip-venv","title":"Alternative: Using pip + venv","text":"<pre><code>git clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -e .\n</code></pre>"},{"location":"tutorials/installation/#with-development-dependencies","title":"With Development Dependencies","text":"<pre><code># Using uv\nuv pip install -e \".[dev]\"\n\n# Or using pip\npip install -e \".[dev]\"\n</code></pre>"},{"location":"tutorials/installation/#r-dependencies-for-enrichment-analysis","title":"R Dependencies for Enrichment Analysis","text":"<p>The package includes R scripts for gene enrichment analysis. You can install R dependencies in several ways:</p>"},{"location":"tutorials/installation/#option-1-via-condamamba-recommended","title":"Option 1: Via Conda/Mamba (Recommended)","text":"<p>If using conda/mamba environment:</p> <pre><code># Activate your environment\nmamba activate renalprog\n\n# Install R and packages (if not done during initial setup)\nmamba install -c conda-forge r-base r-gprofiler2 r-ggplot2 r-optparse\n</code></pre>"},{"location":"tutorials/installation/#option-2-via-rs-installpackages","title":"Option 2: Via R's install.packages()","text":"<p>If you already have R installed system-wide or prefer CRAN:</p> <pre><code># Install R packages using the provided script\nRscript scripts/r_analysis/install_r_packages.R\n</code></pre> <p>On Windows PowerShell: <pre><code>Rscript scripts\\r_analysis\\install_r_packages.R\n</code></pre></p> <p>Required R packages: - <code>r-gprofiler2</code> / <code>gprofiler2</code> - Gene enrichment via g:Profiler API - <code>r-ggplot2</code> / <code>ggplot2</code> - Visualization - <code>r-optparse</code> / <code>optparse</code> - Command-line parsing</p>"},{"location":"tutorials/installation/#gsea-installation-required-for-step-6","title":"GSEA Installation (Required for Step 6)","text":"<p>For dynamic enrichment analysis, you need to install GSEA:</p> <ol> <li>Download from: https://www.gsea-msigdb.org/gsea/index.jsp</li> <li>Extract to project root (creates <code>GSEA_4.3.2/</code> directory)</li> <li>See GSEA Installation Guide for detailed instructions</li> </ol>"},{"location":"tutorials/pretrained-models/","title":"Using Pretrained Models","text":"<p>This guide shows you how to use the pretrained VAE models from Hugging Face to reproduce the results from the paper without training models from scratch. The pretrained models were trained on the preprocessed TCGA data available in this repository (see Preprocessing Tutorial for details).</p>"},{"location":"tutorials/pretrained-models/#overview","title":"Overview","text":"<p>Pre-trained models are available on Hugging Face Hub and include:</p> <ul> <li>VAE models: Variational Autoencoders trained on TCGA data</li> <li>Reconstruction Networks: Post-processing networks that refine VAE outputs</li> <li>Configuration files: Model architectures and hyperparameters</li> </ul> <p>Available Cancer Types</p> <p>Pretrained models are available for:</p> <ul> <li>KIRC (Kidney Renal Clear Cell Carcinoma)</li> <li>BRCA (Breast Invasive Carcinoma)</li> </ul>"},{"location":"tutorials/pretrained-models/#quick-start","title":"Quick Start","text":""},{"location":"tutorials/pretrained-models/#using-the-pipeline-script","title":"Using the Pipeline Script","text":"<p>The easiest way to use pretrained models is with the <code>3_check_reconstruction.py</code> script:</p> <pre><code># Download and use KIRC pretrained models\npython scripts/pipeline_steps/3_check_reconstruction.py \\\n    --hf_models \\\n    --cancer_type KIRC\n\n# Or for BRCA\npython scripts/pipeline_steps/3_check_reconstruction.py \\\n    --hf_models \\\n    --cancer_type BRCA\n</code></pre> <p>This will: 1. Download the VAE model and configuration from Hugging Face 2. Download the Reconstruction Network model 3. Load your preprocessed data 4. Generate reconstructions 5. Create UMAP visualizations comparing original vs reconstructed data</p>"},{"location":"tutorials/pretrained-models/#manual-usage-in-python","title":"Manual Usage in Python","text":""},{"location":"tutorials/pretrained-models/#step-1-install-hugging-face-hub","title":"Step 1: Install Hugging Face Hub","text":"<p>Check the official documentation to install the <code>huggingface-hub</code> library. The simplest way is via pip:</p> <pre><code>pip install huggingface-hub\n</code></pre>"},{"location":"tutorials/pretrained-models/#step-2-download-and-load-models","title":"Step 2: Download and Load Models","text":"<pre><code>import huggingface_hub as hf\nimport torch\nimport json\nfrom pathlib import Path\nfrom renalprog.modeling.train import VAE, NetworkReconstruction\nfrom renalprog.config import MODELS_DIR\n\n# Set cancer type\ncancer_type = 'KIRC'  # or 'BRCA'\n\n# Create local directory for pretrained models\nmodel_dir = MODELS_DIR / \"pretrained\" / cancer_type\nmodel_dir.mkdir(parents=True, exist_ok=True)\n\n# ============================================================================\n# Download VAE Configuration\n# ============================================================================\nprint(f\"Downloading VAE config for {cancer_type}...\")\nvae_config_path = hf.hf_hub_download(\n    repo_id=\"gprolcastelo/evenflow_models\",\n    filename=f\"{cancer_type}/config.json\",\n    local_dir=model_dir.parent\n)\n\n# Load configuration\nwith open(vae_config_path, 'r') as f:\n    vae_config = json.load(f)\n\nprint(f\"VAE Configuration: {vae_config}\")\n\n# ============================================================================\n# Download and Load VAE Model\n# ============================================================================\n# Model filenames for each cancer type\nvae_models = {\n    'KIRC': \"KIRC/20250321_VAE_idim8516_md512_feat256mse_relu.pth\",\n    'BRCA': \"BRCA/20251209_VAE_idim8954_md1024_feat512mse_relu.pth\"\n}\n\nprint(f\"Downloading VAE model for {cancer_type}...\")\nvae_model_path = hf.hf_hub_download(\n    repo_id=\"gprolcastelo/evenflow_models\",\n    filename=vae_models[cancer_type],\n    local_dir=model_dir.parent\n)\n\n# Initialize VAE\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_vae = VAE(\n    input_dim=vae_config['INPUT_DIM'],\n    mid_dim=vae_config['MID_DIM'],\n    features=vae_config['LATENT_DIM']\n).to(device)\n\n# Load weights\ncheckpoint = torch.load(vae_model_path, map_location=device, weights_only=False)\nmodel_vae.load_state_dict(checkpoint)\nmodel_vae.eval()\n\nprint(f\"\u2713 VAE model loaded successfully!\")\n\n# ============================================================================\n# Download and Load Reconstruction Network\n# ============================================================================\nprint(f\"Downloading Reconstruction Network for {cancer_type}...\")\n\n# Download network dimensions\nnetwork_dims_path = hf.hf_hub_download(\n    repo_id=\"gprolcastelo/evenflow_models\",\n    filename=f\"{cancer_type}/network_dims.csv\",\n    local_dir=model_dir.parent\n)\n\n# Load dimensions\nimport pandas as pd\nnetwork_dims = pd.read_csv(network_dims_path).values.tolist()[0]\nprint(f\"Network dimensions: {network_dims}\")\n\n# Download model\nrecnet_model_path = hf.hf_hub_download(\n    repo_id=\"gprolcastelo/evenflow_models\",\n    filename=f\"{cancer_type}/network_reconstruction.pth\",\n    local_dir=model_dir.parent\n)\n\n# Initialize Reconstruction Network\nmodel_recnet = NetworkReconstruction(layer_dims=network_dims).to(device)\n\n# Load weights\ncheckpoint_recnet = torch.load(recnet_model_path, map_location=device, weights_only=False)\nmodel_recnet.load_state_dict(checkpoint_recnet)\nmodel_recnet.eval()\n\nprint(f\"\u2713 Reconstruction Network loaded successfully!\")\n</code></pre>"},{"location":"tutorials/pretrained-models/#step-3-use-models-for-inference","title":"Step 3: Use Models for Inference","text":"<pre><code>from renalprog.utils import apply_VAE\nimport pandas as pd\n\n# Load your preprocessed data\ndata = pd.read_csv('data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv', index_col=0)\n\n# If data is genes \u00d7 samples, transpose it to samples \u00d7 genes\nif data.shape[0] &gt; data.shape[1]:\n    data = data.T\n\nprint(f\"Data shape: {data.shape}\")\n\n# ============================================================================\n# Apply VAE\n# ============================================================================\ndata_tensor = torch.tensor(data.values, dtype=torch.float32)\n\nreconstruction_vae, _, _, latent, scaler = apply_VAE(\n    data_tensor, \n    model_vae, \n    y=None\n)\n\nprint(f\"VAE reconstruction shape: {reconstruction_vae.shape}\")\nprint(f\"Latent representation shape: {latent.shape}\")\n\n# Convert to DataFrame\ndf_reconstruction_vae = pd.DataFrame(\n    reconstruction_vae, \n    index=data.index, \n    columns=data.columns\n)\n\ndf_latent = pd.DataFrame(\n    latent, \n    index=data.index\n)\n\n# ============================================================================\n# Apply Reconstruction Network (Post-processing)\n# ============================================================================\nrec_tensor = torch.tensor(reconstruction_vae, dtype=torch.float32).to(device)\n\nwith torch.no_grad():\n    reconstruction_final = model_recnet(rec_tensor)\n\n# Convert to DataFrame\ndf_reconstruction_final = pd.DataFrame(\n    reconstruction_final.cpu().numpy(),\n    index=data.index,\n    columns=data.columns\n)\n\nprint(f\"Final reconstruction shape: {df_reconstruction_final.shape}\")\n\n# Save results\ndf_reconstruction_final.to_csv('reconstructed_data.csv')\ndf_latent.to_csv('latent_representation.csv')\n\nprint(\"\u2713 Reconstruction complete!\")\n</code></pre>"},{"location":"tutorials/pretrained-models/#vae-model-architecture-details","title":"VAE Model Architecture Details","text":"Model Input Dim Mid Dim Latent Dim File KIRC 8,516 512 256 <code>KIRC/20250321_VAE_idim8516_md512_feat256mse_relu.pth</code> BRCA 8,954 1,024 512 <code>BRCA/20251209_VAE_idim8954_md1024_feat512mse_relu.pth</code>"},{"location":"tutorials/pretrained-models/#hugging-face-repository","title":"Hugging Face Repository","text":"<p>All pretrained models are hosted at:</p> <p>\ud83e\udd17 gprolcastelo/evenflow_models</p>"},{"location":"tutorials/pretrained-models/#repository-structure","title":"Repository Structure","text":"<pre><code>evenflow_models/\n\u251c\u2500\u2500 KIRC/\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u251c\u2500\u2500 network_dims.csv\n\u2502   \u251c\u2500\u2500 network_reconstruction.pth\n\u2502   \u2514\u2500\u2500 20250321_VAE_idim8516_md512_feat256mse_relu.pth\n\u2514\u2500\u2500 BRCA/\n    \u251c\u2500\u2500 config.json\n    \u251c\u2500\u2500 network_dims.csv\n    \u251c\u2500\u2500 network_reconstruction.pth\n    \u2514\u2500\u2500 20251209_VAE_idim8954_md1024_feat512mse_relu.pth\n</code></pre>"},{"location":"tutorials/pretrained-models/#complete-example-reconstruction-validation","title":"Complete Example: Reconstruction Validation","text":"<p>For a detailed example of using the pretrained models to validate reconstructions, refer to the reconstruction tutorial. As a summary, when using <code>3_check_reconstruction.py</code>, you have several options:</p> <pre><code># Basic usage with pretrained models\npython scripts/pipeline_steps/3_check_reconstruction.py --hf_models --cancer_type KIRC\n\n# Include SDMetrics evaluation (takes longer)\npython scripts/pipeline_steps/3_check_reconstruction.py --hf_models --cancer_type KIRC --sdmetrics\n\n# Use locally trained models instead\npython scripts/pipeline_steps/3_check_reconstruction.py --cancer_type KIRC\n</code></pre>"},{"location":"tutorials/pretrained-models/#arguments","title":"Arguments","text":"Argument Description Default <code>--cancer_type</code> Cancer type (KIRC or BRCA) KIRC <code>--hf_models</code> Load pretrained models from Hugging Face False <code>--sdmetrics</code> Evaluate using SDMetrics (very slow) False"},{"location":"tutorials/pretrained-models/#output-files","title":"Output Files","text":"<p>When running the reconstruction check, you'll get:</p> <pre><code>reports/figures/YYYYMMDD_CANCER_umap_reconstruction/\n\u251c\u2500\u2500 preprocessed.html                          # UMAP of original data\n\u251c\u2500\u2500 VAE_output.html                            # UMAP of VAE reconstruction\n\u251c\u2500\u2500 recnet_output.html                         # UMAP of final reconstruction\n\u251c\u2500\u2500 preprocessed_and_vae.html                  # Comparison: original vs VAE\n\u2514\u2500\u2500 preprocessed_and_recnet.html               # Comparison: original vs final\n\nmodels/pretrained/CANCER/\n\u251c\u2500\u2500 config.json                                # VAE configuration\n\u251c\u2500\u2500 network_dims.csv                           # Reconstruction network architecture\n\u251c\u2500\u2500 CANCER/\n\u2502   \u251c\u2500\u2500 20250321_VAE_*.pth                    # VAE weights\n\u2502   \u2514\u2500\u2500 network_reconstruction.pth             # Reconstruction network weights\n</code></pre>"},{"location":"tutorials/pretrained-models/#citation","title":"\ud83d\udcdc Citation","text":"<p>If you use these pretrained models in your research, please cite</p>"},{"location":"tutorials/pretrained-models/#see-also","title":"See Also","text":"<ul> <li>Reconstruction Tutorial</li> <li>VAE Training Tutorial</li> <li>Complete Pipeline</li> <li>API Documentation - Models</li> </ul>"},{"location":"tutorials/quickstart/","title":"Quick Start Tutorial","text":"<p>Get started with <code>renalprog</code> in 10 minutes! This tutorial demonstrates the core functionality using example data.</p>"},{"location":"tutorials/quickstart/#installation","title":"Installation","text":"<p>If you haven't installed <code>renalprog</code> yet:</p> <pre><code># Clone repository\ngit clone https://github.com/gprolcastelo/renalprog.git\ncd renalprog\n\n# Create environment with mamba/conda\nmamba env create -f environment.yml\nmamba activate renalprog\n\n# Install package\npip install -e .\n</code></pre> <p>See the Installation Guide for detailed instructions.</p>"},{"location":"tutorials/quickstart/#quick-example","title":"Quick Example","text":"<p>Let's run a minimal example that demonstrates the complete pipeline:</p>"},{"location":"tutorials/quickstart/#1-import-required-modules","title":"1. Import Required Modules","text":"<pre><code>from renalprog import config, dataset, features, modeling, plots\nfrom renalprog.modeling import VAE, generate_trajectories\nimport pandas as pd\nimport numpy as np\nimport torch\n</code></pre>"},{"location":"tutorials/quickstart/#2-load-example-data","title":"2. Load Example Data","text":"<p>You can either download preprocessed data from Zenodo or use your own data.</p>"},{"location":"tutorials/quickstart/#option-a-download-preprocessed-data-from-zenodo-recommended","title":"Option A: Download Preprocessed Data from Zenodo (Recommended)","text":"<pre><code>from pathlib import Path\n\n# Download preprocessed KIRC data from Zenodo\nrnaseq, clinical = dataset.download_preprocessed_from_zenodo(\n    rnaseq_url='https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1',\n    clinical_url='https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1',\n    output_dir=Path('data/interim/preprocessed_KIRC_data')\n)\n\nprint(f\"RNA-seq data shape: {rnaseq.shape}\")\nprint(f\"Genes: {rnaseq.shape[0]:,}, Samples: {rnaseq.shape[1]:,}\")\nprint(f\"Clinical data shape: {clinical.shape}\")\n</code></pre> <p>Expected output: <pre><code>================================================================================\nDownloading preprocessed data from Zenodo\n================================================================================\n\n[DOWNLOADING RNASEQ DATA]\n  Source URL: https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1\n  Destination: data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv\nDownloading RNAseq: 100.0%\n  \u2713 Successfully downloaded RNAseq data\n  Shape: 8,516 genes \u00d7 530 samples\n\n[DOWNLOADING CLINICAL DATA]\n  Source URL: https://zenodo.org/records/17987300/files/nodes_metadata.csv?download=1\n  Destination: data/interim/preprocessed_KIRC_data/clinical_data.csv\nDownloading clinical: 100.0%\n  \u2713 Successfully downloaded clinical data\n  Shape: 530 samples \u00d7 4 features\n\n[DATA VALIDATION]\n  RNAseq samples: 530\n  Clinical samples: 530\n  Common samples: 530\n\n================================================================================\nPreprocessed data download completed successfully\n================================================================================\n</code></pre></p> <p>Available Datasets</p> <p>Preprocessed data is available for:</p> <ul> <li>KIRC (Kidney Renal Clear Cell Carcinoma): Zenodo Record 17987300</li> <li>BRCA (Breast Cancer): Zenodo Record 17986123</li> </ul>"},{"location":"tutorials/quickstart/#option-b-load-your-own-preprocessed-data","title":"Option B: Load Your Own Preprocessed Data","text":"<pre><code># Load from local files\nrnaseq = pd.read_csv('data/processed/rnaseq_maha.csv', index_col=0)\nclinical = pd.read_csv('data/processed/clinical.csv', index_col=0)\n\nprint(f\"RNA-seq data shape: {rnaseq.shape}\")\nprint(f\"Samples: {rnaseq.shape[0]}, Genes: {rnaseq.shape[1]}\")\n</code></pre>"},{"location":"tutorials/quickstart/#3-train-vae","title":"3. Train VAE","text":"<p>First, create train/test splits:</p> <pre><code>from renalprog.config import VAEConfig, INTERIM_DATA_DIR, MODELS_DIR, get_dated_dir\nfrom renalprog.modeling.train import train_vae_with_postprocessing\nfrom pathlib import Path\n\n# Create train/test splits (80/20)\ntraintest_dir = INTERIM_DATA_DIR / \"train_test_split\"\nX_train, X_test, y_train, y_test, _, _ = dataset.create_train_test_split(\n    rnaseq_path=Path('data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv'),\n    clinical_path=Path('data/interim/preprocessed_KIRC_data/clinical_data.csv'),\n    test_size=0.2,\n    seed=2023,\n    output_dir=traintest_dir,\n    stage_column=\"stage\"\n)\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Test samples: {X_test.shape[0]}\")\nprint(f\"Number of genes: {X_train.shape[1]}\")\n</code></pre> <p>Now configure and train the VAE: the given parameters are intended for the quick start testing.  Increase the number of epochs for better performance in real applications.</p> <pre><code># Configure VAE\nvae_config = VAEConfig()\nvae_config.INPUT_DIM = X_train.shape[1]  # Number of genes\nvae_config.MID_DIM = 128\nvae_config.LATENT_DIM = 16\nvae_config.BETA_CYCLES = 1\nvae_config.EPOCHS = 100 * vae_config.BETA_CYCLES  # 600 epochs total\nvae_config.BETA_RATIO = 0.5\nvae_config.BATCH_SIZE = 8\n\n# Configure postprocessing Reconstruction Network\nrecnet_dims = [X_train.shape[1], 3_512, 824, 3_731, X_train.shape[1]]\n\n# Create output directory\nmodel_dir = MODELS_DIR / \"quickstart_vae\"\nmodel_dir.mkdir(parents=True, exist_ok=True)\n\n# Train VAE with postprocessing network\nvae_model, network, vae_history, reconstruction_history = train_vae_with_postprocessing(\n    X_train=X_train,\n    X_test=X_test,\n    vae_config=vae_config,\n    reconstruction_network_dims=recnet_dims,\n    reconstruction_epochs=1_000,\n    reconstruction_lr=1e-4,\n    batch_size_reconstruction=8,\n    save_dir=model_dir,\n    force_cpu=True,  # Set to False to use GPU if available\n)\n\nprint(f\"Training complete! Models saved to {model_dir}\")\n</code></pre> <p>Expected output: <pre><code>[INFO] VAE Configuration:\n[INFO]   Input dim: 8516\n[INFO]   Mid dim: 128\n[INFO]   Latent dim: 16\n[INFO]   Beta cycles: 1\n[INFO]   Total epochs: 100\n[INFO]   Batch size: 8\n[INFO] Reconstruction Network dims: [8516, 3512, 824, 3731, 8516]\n[INFO] Models will be saved to: models/quickstart_vae\n[INFO] \nStarting training...\n[INFO] Starting full VAE + postprocessing pipeline\n[INFO] Step 1: Training VAE\n[INFO] Saved config: models/quickstart_vae/vae/config.json\n[INFO] Using device: cpu\n[INFO] Model: VAE(input_dim=8516, mid_dim=128, latent_dim=16)\n[INFO] Parameters: 2,195,044\n[INFO] ModelCheckpointer initialized: models/quickstart_vae/vae\n[INFO] Monitoring: val_loss (min)\n[INFO] Using cyclical beta annealing: 0.0 -&gt; 1.0 over 1 cycles\n[INFO] Starting training for 100 epochs\nEpochs:   0%|                                                                                                                                         | 0/100 [00:00&lt;?, ?it/s, train_loss=1667.9040, val_loss=1361.7740, beta=0.000][INFO] Epoch 1/100 - train_loss: 1667.9040, val_loss: 1361.7740                                                                                                                                                                     \nEpochs:   9%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                       | 9/100 [00:09&lt;01:24,  1.08it/s, train_loss=552.9183, val_loss=710.7239, beta=0.180][INFO] Epoch 10/100 - train_loss: 552.9183, val_loss: 710.7239                                                                                                                                                                      \nEpochs:  19%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                         | 19/100 [00:22&lt;02:07,  1.58s/it, train_loss=556.0336, val_loss=709.1441, beta=0.380][INFO] Epoch 20/100 - train_loss: 556.0336, val_loss: 709.1441                                                                                                                                                                      \nEpochs:  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                            | 29/100 [00:37&lt;01:43,  1.46s/it, train_loss=526.8585, val_loss=669.9171, beta=0.580][INFO] Epoch 30/100 - train_loss: 526.8585, val_loss: 669.9171                                                                                                                                                                      \nEpochs:  39%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                               | 39/100 [00:52&lt;01:30,  1.48s/it, train_loss=462.7390, val_loss=596.3372, beta=0.780][INFO] Epoch 40/100 - train_loss: 462.7390, val_loss: 596.3372                                                                                                                                                                      \nEpochs:  49%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                  | 49/100 [01:06&lt;01:10,  1.38s/it, train_loss=389.8898, val_loss=518.6038, beta=0.980][INFO] Epoch 50/100 - train_loss: 389.8898, val_loss: 518.6038                                                                                                                                                                      \nEpochs:  59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                     | 59/100 [01:19&lt;00:53,  1.30s/it, train_loss=335.6482, val_loss=460.9847, beta=1.000][INFO] Epoch 60/100 - train_loss: 335.6482, val_loss: 460.9847                                                                                                                                                                      \nEpochs:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                        | 69/100 [01:31&lt;00:40,  1.31s/it, train_loss=307.4428, val_loss=433.4705, beta=1.000][INFO] Epoch 70/100 - train_loss: 307.4428, val_loss: 433.4705                                                                                                                                                                      \nEpochs:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                           | 79/100 [01:44&lt;00:25,  1.22s/it, train_loss=293.1740, val_loss=419.2723, beta=1.000][INFO] Epoch 80/100 - train_loss: 293.1740, val_loss: 419.2723                                                                                                                                                                      \nEpochs:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b              | 89/100 [01:57&lt;00:14,  1.29s/it, train_loss=278.1044, val_loss=404.8974, beta=1.000][INFO] Epoch 90/100 - train_loss: 278.1044, val_loss: 404.8974                                                                                                                                                                      \nEpochs:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 99/100 [02:10&lt;00:01,  1.28s/it, train_loss=261.5659, val_loss=388.2671, beta=1.000][INFO] Epoch 100/100 - train_loss: 261.5659, val_loss: 388.2671                                                                                                                                                                     \nEpochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [02:10&lt;00:00,  1.31s/it, train_loss=261.5659, val_loss=388.2671, beta=1.000]\n[INFO] Saved checkpoint: models/quickstart_vae/vae/final_model.pth\n[INFO] Step 3: Training reconstruction network\n[INFO] Training reconstruction network for 10 epochs\nReconstruction Network Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [05:38&lt;00:00, 33.87s/it, train_loss=53.913388, test_loss=54.212544]\n[INFO] Reconstruction network training complete        \n</code></pre></p> <p>Training Time</p> <p>With these settings (100 VAE epochs + 10 reconstruction epochs, and the rest of the configuration parameters), CPU training should take 6-7 minutes. Using a GPU will significantly speed up training.</p> <p>Two-Stage Training</p> <p>This pipeline trains:</p> <ol> <li>VAE: Learns compressed latent representation</li> <li>Reconstruction Network: Post-processes VAE output for better reconstruction</li> </ol> <p>Both models are saved and can be used for downstream analyses. The pipeline avoids data leakage by training moth models on training data only.</p>"},{"location":"tutorials/quickstart/#4-visualize-training-progress","title":"4. Visualize Training Progress","text":"<pre><code>from renalprog.plots import plot_training_history, plot_reconstruction_losses\n\n# Plot VAE training history\nplot_training_history(\n    history=vae_history,\n    save_path=model_dir / \"vae_training_history.png\",\n    title=\"VAE Training History\"\n)\n\n# Plot reconstruction network training history\nplot_reconstruction_losses(\n    loss_train=reconstruction_history[\"train_loss\"],\n    loss_test=reconstruction_history[\"test_loss\"],\n    save_path=model_dir / \"reconstruction_training_history.png\",\n    title=\"Reconstruction Network Training History\"\n)\n\nprint(\"Training plots saved!\")\n</code></pre>"},{"location":"tutorials/quickstart/#full-quick-start-code","title":"Full quick start code","text":"<p>You can find the complete code for this quick start example in the <code>scripts/quickstart_vae.py</code> script in the repository. Easily run it from the command line:</p> <pre><code>python scripts/quickstart_vae.py\n</code></pre>"},{"location":"tutorials/quickstart/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've successfully:</p> <p>\u2705 Downloaded preprocessed data from Zenodo \u2705 Split data into train/test sets \u2705 Trained a VAE with postprocessing network \u2705 Visualized training progress  </p> <p>Now that you've completed the quick start, explore the full tutorials:</p>"},{"location":"tutorials/quickstart/#recommended-tutorials","title":"\ud83d\udcda Recommended Tutorials","text":"<ol> <li>Complete Pipeline - Full analysis workflow from data processing to results</li> <li>Step 3: VAE Reconstruction Check - Evaluate VAE reconstruction quality</li> <li>Step 4: Generate Trajectories - Create disease progression trajectories</li> <li>Step 5: Classification - Train classifiers on gene expression</li> <li>Step 6: Enrichment Analysis - Perform pathway enrichment analysis</li> </ol>"},{"location":"tutorials/quickstart/#citation","title":"\ud83d\udcdc Citation","text":"<p>If you use this pipeline in your research, please cite this work. For details, see the citation page file.</p>"},{"location":"tutorials/step1-data-processing/","title":"Step 1: Data Processing Pipeline","text":"<p>This guide explains how to download and preprocess TCGA RNA-seq data for the RenalProg pipeline.</p>"},{"location":"tutorials/step1-data-processing/#overview","title":"Overview","text":"<p>The data processing pipeline performs the following steps:</p> <ol> <li>Option 1: Download Preprocessed Data - Download preprocessed data from Zenodo (recommended for reproducibility)</li> <li>Option 2: Process Raw Data - Download and process raw TCGA data from scratch</li> </ol> <p>The script provides flexible options for: - Downloading preprocessed RNAseq data from Zenodo repositories - Downloading raw TCGA data and processing it locally - Processing multiple cancer types (KIRC, BRCA) - Automatic clinical data filtering to match preprocessed samples</p>"},{"location":"tutorials/step1-data-processing/#prerequisites","title":"Prerequisites","text":"<p>Before running the data processing pipeline, ensure you have:</p> <ul> <li>Python environment: With required packages installed (pandas, numpy, scipy, etc.)</li> <li>Internet connection: For downloading data from Zenodo or TCGA</li> <li>Sufficient disk space: ~2-5 GB for raw data, ~500 MB for preprocessed data</li> </ul>"},{"location":"tutorials/step1-data-processing/#usage","title":"Usage","text":""},{"location":"tutorials/step1-data-processing/#option-1-download-preprocessed-data-from-zenodo-recommended","title":"Option 1: Download Preprocessed Data from Zenodo (Recommended)","text":"<p>This is the fastest and most reproducible option. It downloads preprocessed RNAseq data from publicly available Zenodo repositories.</p>"},{"location":"tutorials/step1-data-processing/#basic-usage-kirc","title":"Basic Usage - KIRC","text":"<pre><code>python scripts/pipeline_steps/1_data_processing.py --zenodo_preprocessed\n</code></pre> <p>This will: - Download preprocessed KIRC RNAseq data from Zenodo - Use existing clinical/phenotype files (assumes they're already downloaded) - Process and filter clinical data to match the RNAseq patients - Save everything to the appropriate directories</p>"},{"location":"tutorials/step1-data-processing/#with-raw-data-download-kirc","title":"With Raw Data Download - KIRC","text":"<pre><code>python scripts/pipeline_steps/1_data_processing.py --zenodo_preprocessed --download_raw\n</code></pre> <p>This will: - Download preprocessed KIRC RNAseq data from Zenodo - Download raw clinical and phenotype data from TCGA - Process and filter clinical data to match the RNAseq patients</p>"},{"location":"tutorials/step1-data-processing/#for-brca-cancer-type","title":"For BRCA Cancer Type","text":"<pre><code>python scripts/pipeline_steps/1_data_processing.py --zenodo_preprocessed --cancer_type BRCA --download_raw\n</code></pre>"},{"location":"tutorials/step1-data-processing/#option-2-process-raw-tcga-data-from-scratch","title":"Option 2: Process Raw TCGA Data from Scratch","text":"<p>This option downloads raw TCGA data and performs all preprocessing steps locally.</p>"},{"location":"tutorials/step1-data-processing/#process-kirc-assumes-raw-data-already-downloaded","title":"Process KIRC (assumes raw data already downloaded)","text":"<pre><code>python scripts/pipeline_steps/1_data_processing.py\n</code></pre>"},{"location":"tutorials/step1-data-processing/#download-and-process-kirc","title":"Download and Process KIRC","text":"<pre><code>python scripts/pipeline_steps/1_data_processing.py --download_raw\n</code></pre>"},{"location":"tutorials/step1-data-processing/#process-brca","title":"Process BRCA","text":"<pre><code>python scripts/pipeline_steps/1_data_processing.py --cancer_type BRCA --download_raw\n</code></pre>"},{"location":"tutorials/step1-data-processing/#command-line-arguments","title":"Command-Line Arguments","text":"Argument Type Default Description <code>--zenodo_preprocessed</code> Flag False Download preprocessed data from Zenodo instead of processing raw data <code>--cancer_type</code> String 'KIRC' Cancer type to process. Choices: 'KIRC', 'BRCA' <code>--download_raw</code> Flag False Download raw TCGA data (otherwise assumes data is already downloaded)"},{"location":"tutorials/step1-data-processing/#data-sources","title":"Data Sources","text":""},{"location":"tutorials/step1-data-processing/#zenodo-preprocessed-data","title":"Zenodo Preprocessed Data","text":""},{"location":"tutorials/step1-data-processing/#kirc","title":"KIRC","text":"<ul> <li>Repository: https://zenodo.org/records/17987300</li> <li>File: Static_KIRC.csv</li> <li>Direct Download: https://zenodo.org/records/17987300/files/Static_KIRC.csv?download=1</li> </ul>"},{"location":"tutorials/step1-data-processing/#brca","title":"BRCA","text":"<ul> <li>Repository: https://zenodo.org/records/17986123</li> <li>File: Static_BRCA.csv</li> <li>Direct Download: https://zenodo.org/records/17986123/files/Static_BRCA.csv?download=1</li> </ul>"},{"location":"tutorials/step1-data-processing/#raw-tcga-data","title":"Raw TCGA Data","text":"<p>When using <code>--download_raw</code>, the script downloads: - RNA-seq expression data from UCSC Xena - Clinical survival data - Phenotype data</p>"},{"location":"tutorials/step1-data-processing/#processing-steps","title":"Processing Steps","text":""},{"location":"tutorials/step1-data-processing/#when-using-zenodo_preprocessed","title":"When Using <code>--zenodo_preprocessed</code>","text":"<ol> <li>Download preprocessed RNAseq data from Zenodo</li> <li>Download clinical/phenotype data (if <code>--download_raw</code> is specified)</li> <li>Process clinical data:</li> <li>Load raw clinical and phenotype files</li> <li>Filter for specified cancer type</li> <li>Convert stage information (early/late classification)</li> <li>Filter to match only patients in the preprocessed RNAseq data</li> <li>Copy control data from repository:</li> <li>Control samples are included in the repository at <code>controls/{CANCER_TYPE}/</code></li> <li>Copies <code>rnaseq_controls.csv</code> and <code>clinical_controls.csv</code> to preprocessed data directory</li> <li>No need to download or process controls separately</li> <li>Save outputs:</li> <li><code>preprocessed_rnaseq.csv</code>: Preprocessed gene expression data</li> <li><code>clinical_data.csv</code>: Filtered clinical metadata</li> <li><code>stages.csv</code>: Stage information for matched patients</li> <li><code>controls/{CANCER_TYPE}_control_rnaseq.csv</code>: Control RNAseq data (copied from repo)</li> <li><code>controls/{CANCER_TYPE}_control_clinical.csv</code>: Control clinical data (copied from repo)</li> </ol>"},{"location":"tutorials/step1-data-processing/#when-processing-raw-data","title":"When Processing Raw Data","text":"<ol> <li>Download raw data (if <code>--download_raw</code> is specified)</li> <li>Process downloaded data:</li> <li>Filter for specified cancer type</li> <li>Merge clinical and phenotype information</li> <li>Extract control samples (healthy tissue)</li> <li>Convert stages to early/late classification</li> <li>Preprocess RNAseq data:</li> <li>Filter low-expression genes</li> <li>Detect and remove outlier samples using Mahalanobis distance</li> <li>Remove duplicate genes</li> <li>Process control samples:</li> <li>Filter control data to match preprocessed genes</li> <li>Save outputs:</li> <li><code>preprocessed_rnaseq.csv</code>: Preprocessed gene expression data</li> <li><code>preprocessing_info.csv/json</code>: Metadata about preprocessing steps</li> <li><code>stages.csv</code>: Stage information</li> <li><code>controls/preprocessed_control_rnaseq.csv</code>: Preprocessed control samples</li> </ol>"},{"location":"tutorials/step1-data-processing/#output-structure","title":"Output Structure","text":""},{"location":"tutorials/step1-data-processing/#using-zenodo_preprocessed","title":"Using <code>--zenodo_preprocessed</code>","text":"<pre><code>data/interim/preprocessed_{CANCER_TYPE}_data/\n\u251c\u2500\u2500 Static_{CANCER_TYPE}.csv                           # Original downloaded file from Zenodo\n\u251c\u2500\u2500 preprocessed_rnaseq.csv                            # Standard format (genes \u00d7 patients)\n\u251c\u2500\u2500 clinical_data.csv                                  # Filtered clinical data\n\u251c\u2500\u2500 stages.csv                                         # Stage information\n\u2514\u2500\u2500 controls/\n    \u251c\u2500\u2500 {CANCER_TYPE}_control_rnaseq.csv              # Control RNAseq (copied from repo)\n    \u2514\u2500\u2500 {CANCER_TYPE}_control_clinical.csv            # Control clinical (copied from repo)\n</code></pre> <p>Also creates (intermediate): <pre><code>data/interim/{CANCER_TYPE}_data/\n\u2514\u2500\u2500 [Processed clinical/phenotype files before filtering]\n</code></pre></p> <p>Note: Control data is included in the repository at: <pre><code>controls/{CANCER_TYPE}/\n\u251c\u2500\u2500 rnaseq_controls.csv\n\u2514\u2500\u2500 clinical_controls.csv\n</code></pre></p>"},{"location":"tutorials/step1-data-processing/#processing-raw-data","title":"Processing Raw Data","text":"<pre><code>data/raw/\n\u251c\u2500\u2500 EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\n\u251c\u2500\u2500 Survival_SupplementalTable_S1_20171025_xena_sp\n\u2514\u2500\u2500 TCGA_phenotype_denseDataOnlyDownload.tsv\n\ndata/interim/{CANCER_TYPE}_data/\n\u251c\u2500\u2500 {CANCER_TYPE}_rnaseq.csv\n\u251c\u2500\u2500 {CANCER_TYPE}_clinical.csv\n\u2514\u2500\u2500 controls/\n    \u251c\u2500\u2500 {CANCER_TYPE}_control_rnaseq.csv\n    \u2514\u2500\u2500 {CANCER_TYPE}_control_clinical.csv\n\ndata/interim/preprocessed_{CANCER_TYPE}_data/\n\u251c\u2500\u2500 preprocessed_rnaseq.csv\n\u251c\u2500\u2500 preprocessing_info.csv\n\u251c\u2500\u2500 preprocessing_info.json\n\u251c\u2500\u2500 stages.csv\n\u2514\u2500\u2500 controls/\n    \u2514\u2500\u2500 preprocessed_control_rnaseq.csv\n</code></pre>"},{"location":"tutorials/step1-data-processing/#preprocessing-details","title":"Preprocessing Details","text":"<p>When processing raw data, the following steps are applied:</p>"},{"location":"tutorials/step1-data-processing/#gene-expression-filtering","title":"Gene Expression Filtering","text":"<ul> <li>Low expression filtering: Removes genes with mean expression &lt; 0.5 and variance &lt; 0.5</li> <li>Sample fraction: Genes must be expressed in at least 20% of samples</li> <li>Statistical threshold: Alpha = 0.05 for statistical tests</li> </ul>"},{"location":"tutorials/step1-data-processing/#outlier-detection","title":"Outlier Detection","text":"<ul> <li>Method: Mahalanobis distance with Minimum Covariance Determinant (MCD)</li> <li>Purpose: Identifies and removes samples with atypical expression patterns</li> <li>Seed: 2023 (for reproducibility)</li> </ul>"},{"location":"tutorials/step1-data-processing/#duplicate-handling","title":"Duplicate Handling","text":"<ul> <li>Strategy: Keep first occurrence, remove subsequent duplicates</li> <li>Applied to: Both gene names and sample IDs</li> </ul>"},{"location":"tutorials/step1-data-processing/#stage-classification","title":"Stage Classification","text":"<ul> <li>Early stage: Stage I and Stage II</li> <li>Late stage: Stage III and Stage IV</li> </ul>"},{"location":"tutorials/step1-data-processing/#complete-example-workflow","title":"Complete Example Workflow","text":""},{"location":"tutorials/step1-data-processing/#reproducible-workflow-zenodo","title":"Reproducible Workflow (Zenodo)","text":"<p>For maximum reproducibility using published preprocessed data:</p> <pre><code># Download preprocessed RNAseq and raw clinical data\npython scripts/pipeline_steps/1_data_processing.py \\\n  --zenodo_preprocessed \\\n  --download_raw \\\n  --cancer_type KIRC\n</code></pre>"},{"location":"tutorials/step1-data-processing/#custom-workflow-raw-processing","title":"Custom Workflow (Raw Processing)","text":"<p>For full control over preprocessing parameters:</p> <pre><code># Download and process everything from scratch\npython scripts/pipeline_steps/1_data_processing.py \\\n  --download_raw \\\n  --cancer_type KIRC\n</code></pre>"},{"location":"tutorials/step1-data-processing/#verification","title":"Verification","text":"<p>After running the script, verify the outputs:</p> <pre><code># Check preprocessed data dimensions\npython -c \"\nimport pandas as pd\ndata = pd.read_csv('data/interim/preprocessed_KIRC_data/preprocessed_rnaseq.csv', index_col=0)\nclinical = pd.read_csv('data/interim/preprocessed_KIRC_data/clinical_data.csv', index_col=0)\nprint(f'RNAseq: {data.shape[0]} genes \u00d7 {data.shape[1]} patients')\nprint(f'Clinical: {clinical.shape[0]} patients \u00d7 {clinical.shape[1]} features')\n\"\n</code></pre>"},{"location":"tutorials/step1-data-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/step1-data-processing/#download-failures","title":"Download Failures","text":"<p>If downloads from Zenodo fail: - Check your internet connection - Verify the Zenodo URLs are accessible - Try using a VPN if regional restrictions apply</p>"},{"location":"tutorials/step1-data-processing/#clinical-data-mismatch","title":"Clinical Data Mismatch","text":"<p>If patient IDs don't match between RNAseq and clinical data: - Ensure you're using the same cancer type for both datasets - Verify that raw data files are not corrupted - Check that the preprocessing completed successfully</p>"},{"location":"tutorials/step1-data-processing/#memory-issues","title":"Memory Issues","text":"<p>For large datasets: - Process on a machine with sufficient RAM (recommend 16+ GB) - Consider processing cancer types separately - Monitor memory usage during preprocessing</p>"},{"location":"tutorials/step1-data-processing/#missing-raw-data-files","title":"Missing Raw Data Files","text":"<p>If raw data files are not found: - Use the <code>--download_raw</code> flag to download them automatically - Or manually download from UCSC Xena and place in <code>data/raw/</code></p>"},{"location":"tutorials/step1-data-processing/#next-steps","title":"Next Steps","text":"<p>After completing the data processing:</p> <ol> <li>Verify outputs: Check that all expected files were created</li> <li>Inspect data quality: Review preprocessing statistics</li> <li>Proceed to Step 2: Train VAE models on the preprocessed data</li> </ol> <pre><code>python scripts/pipeline_steps/2_models.py\n</code></pre>"},{"location":"tutorials/step1-data-processing/#additional-resources","title":"Additional Resources","text":"<ul> <li>TCGA Data Portal</li> <li>UCSC Xena Browser</li> <li>Zenodo Repository Browser</li> <li>Feature Selection Guide</li> </ul>"},{"location":"tutorials/step2-vae-training/","title":"Step 2: VAE Training Pipeline","text":"<p>This guide explains how to train Variational Autoencoder (VAE) models and Reconstruction Networks for the RenalProg pipeline.</p>"},{"location":"tutorials/step2-vae-training/#overview","title":"Overview","text":"<p>The VAE training pipeline performs the following steps:</p> <ol> <li>Data Loading: Load preprocessed RNAseq data from Step 1</li> <li>Train/Test Split: Create stratified train/test splits</li> <li>VAE Training: Train a Variational Autoencoder with cyclic \u03b2-annealing</li> <li>Reconstruction Network Training: Train a postprocessing network to refine VAE outputs</li> <li>Visualization: Plot training histories and save models</li> </ol> <p>The pipeline trains two sequential models: - VAE: Learns a compressed latent representation of gene expression data - Reconstruction Network: Refines the VAE's reconstructions back to original gene space</p>"},{"location":"tutorials/step2-vae-training/#prerequisites","title":"Prerequisites","text":"<p>Before running the VAE training pipeline, ensure you have:</p> <ul> <li>Preprocessed data: Completed Step 1 data processing</li> <li>Python environment: With PyTorch, numpy, pandas installed</li> <li>Sufficient compute: GPU recommended but not required (can use CPU with <code>use_cpu=True</code>)</li> </ul>"},{"location":"tutorials/step2-vae-training/#usage","title":"Usage","text":""},{"location":"tutorials/step2-vae-training/#basic-usage","title":"Basic Usage","text":"<pre><code>python scripts/pipeline_steps/2_models.py\n</code></pre> <p>This will: - Load preprocessed KIRC data - Create train/test split (80/20) - Train VAE with default hyperparameters - Train Reconstruction Network - Save models and training plots</p>"},{"location":"tutorials/step2-vae-training/#customizing-parameters","title":"Customizing Parameters","text":"<p>Edit the script to modify: - Cancer type: Change <code>cancer_type = \"KIRC\"</code> to <code>\"BRCA\"</code> or other types - Data paths: Update <code>path_rnaseq</code> and <code>path_clinical</code> for your data - Model architecture: Modify VAE and Reconstruction Network dimensions - Training parameters: Adjust epochs, batch size, learning rates, etc.</p>"},{"location":"tutorials/step2-vae-training/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"tutorials/step2-vae-training/#data-parameters","title":"Data Parameters","text":"Parameter Default Description <code>cancer_type</code> <code>\"KIRC\"</code> Cancer type identifier <code>path_rnaseq</code> <code>\"data/interim/preprocessed_KIRC/preprocessed_rnaseq.csv\"</code> Path to preprocessed RNAseq data <code>path_clinical</code> <code>\"data/interim/preprocessed_KIRC/clinical_data.csv\"</code> Path to clinical data <code>test_size</code> <code>0.2</code> Fraction of data to use for testing (20%)"},{"location":"tutorials/step2-vae-training/#vae-architecture","title":"VAE Architecture","text":"Parameter Default Description <code>INPUT_DIM</code> <code>X_train.shape[1]</code> Number of input features (genes), auto-detected <code>MID_DIM</code> <code>512</code> Dimension of middle hidden layer <code>LATENT_DIM</code> <code>256</code> Dimension of latent space <code>BETA_CYCLES</code> <code>3</code> Number of \u03b2-annealing cycles <code>EPOCHS</code> <code>600</code> Total training epochs (200 per cycle \u00d7 3 cycles) <code>BETA_RATIO</code> <code>0.5</code> Fraction of each cycle spent increasing \u03b2 <code>BATCH_SIZE</code> <code>8</code> Training batch size"},{"location":"tutorials/step2-vae-training/#reconstruction-network-architecture","title":"Reconstruction Network Architecture","text":"Parameter Default Description <code>recnet_dims</code> <code>[INPUT_DIM, 3512, 824, 3731, INPUT_DIM]</code> Layer dimensions for reconstruction network <code>recnet_lr</code> <code>1e-4</code> Learning rate for reconstruction network <code>recnet_epochs</code> <code>1000</code> Number of training epochs <code>batch_recnet</code> <code>8</code> Batch size for reconstruction network"},{"location":"tutorials/step2-vae-training/#hardware","title":"Hardware","text":"Parameter Default Description <code>use_cpu</code> <code>True</code> Force CPU usage even if GPU is available"},{"location":"tutorials/step2-vae-training/#training-process","title":"Training Process","text":""},{"location":"tutorials/step2-vae-training/#step-1-traintest-split","title":"Step 1: Train/Test Split","text":"<p>The pipeline creates a stratified split preserving the distribution of cancer stages:</p> <pre><code>X_train, X_test, y_train, y_test = create_train_test_split(\n    rnaseq_path=path_rnaseq,\n    clinical_path=path_clinical,\n    test_size=0.2,\n    seed=2023\n)\n</code></pre> <p>Output saved to: <code>data/interim/YYYYMMDD_train_test_split/</code></p>"},{"location":"tutorials/step2-vae-training/#step-2-vae-training","title":"Step 2: VAE Training","text":"<p>The VAE uses cyclic \u03b2-annealing to balance reconstruction and regularization:</p> <ul> <li>\u03b2-annealing: Gradually increases \u03b2 from 0 to 1 during first half of each cycle</li> <li>Purpose: Prevents posterior collapse and improves latent space quality</li> <li>Cycles: 3 cycles of 200 epochs each (600 total epochs)</li> </ul> <p>VAE Architecture: <pre><code>Input (genes) \u2192 Encoder \u2192 Latent (256D) \u2192 Decoder \u2192 Reconstruction (genes)\n                  \u2193\n              Sampling\n</code></pre></p>"},{"location":"tutorials/step2-vae-training/#step-3-reconstruction-network-training","title":"Step 3: Reconstruction Network Training","text":"<p>After VAE training, a postprocessing network refines the reconstructions:</p> <ul> <li>Purpose: Correct systematic reconstruction errors from VAE</li> <li>Architecture: Deep feedforward network with hidden layers</li> <li>Training: 1000 epochs on VAE-encoded then decoded data</li> </ul>"},{"location":"tutorials/step2-vae-training/#output-files","title":"Output Files","text":""},{"location":"tutorials/step2-vae-training/#model-files","title":"Model Files","text":"<pre><code>models/YYYYMMDD_models_KIRC/\n\u251c\u2500\u2500 vae/\n\u2502   \u251c\u2500\u2500 final_model.pth                    # Trained VAE weights\n\u2502   \u251c\u2500\u2500 config.json                        # VAE architecture configuration\n\u2502   \u2514\u2500\u2500 vae_training_history.png           # Training loss plots\n\u251c\u2500\u2500 reconstruction_network.pth             # Trained reconstruction network\n\u251c\u2500\u2500 network_dims.csv                       # Network layer dimensions\n\u2514\u2500\u2500 reconstruction_network_history.png     # Reconstruction training plots\n</code></pre>"},{"location":"tutorials/step2-vae-training/#traintest-split","title":"Train/Test Split","text":"<pre><code>data/interim/YYYYMMDD_train_test_split/\n\u251c\u2500\u2500 X_train.csv                            # Training gene expression data\n\u251c\u2500\u2500 X_test.csv                             # Test gene expression data\n\u251c\u2500\u2500 y_train.csv                            # Training labels (stages)\n\u251c\u2500\u2500 y_test.csv                             # Test labels (stages)\n\u251c\u2500\u2500 train_indices.csv                      # Training sample indices\n\u2514\u2500\u2500 test_indices.csv                       # Test sample indices\n</code></pre>"},{"location":"tutorials/step2-vae-training/#model-architecture-details","title":"Model Architecture Details","text":""},{"location":"tutorials/step2-vae-training/#vae-configuration","title":"VAE Configuration","text":"<p>The default VAE architecture for KIRC (~8500 genes):</p> <pre><code>Encoder:\n  Input: 8516 genes\n  \u2193\n  Hidden: 512 dimensions\n  \u2193\n  Latent: 256 dimensions (\u03bc and \u03c3)\n\nDecoder:\n  Latent: 256 dimensions\n  \u2193\n  Hidden: 512 dimensions\n  \u2193\n  Output: 8516 genes\n</code></pre>"},{"location":"tutorials/step2-vae-training/#reconstruction-network","title":"Reconstruction Network","text":"<p>The default reconstruction network:</p> <pre><code>Input: 8516 genes (from VAE decoder)\n  \u2193\nLayer 1: 3512 neurons\n  \u2193\nLayer 2: 824 neurons\n  \u2193\nLayer 3: 3731 neurons\n  \u2193\nOutput: 8516 genes (refined reconstruction)\n</code></pre>"},{"location":"tutorials/step2-vae-training/#training-strategies","title":"Training Strategies","text":""},{"location":"tutorials/step2-vae-training/#-annealing","title":"\u03b2-Annealing","text":"<p>The pipeline uses cyclic \u03b2-annealing:</p> <ol> <li>Warmup Phase (first 50% of cycle):</li> <li>\u03b2 increases from 0 to 1</li> <li> <p>Model focuses on reconstruction first, then regularization</p> </li> <li> <p>Full Training (second 50% of cycle):</p> </li> <li>\u03b2 = 1 (standard VAE loss)</li> <li> <p>Model balances reconstruction and regularization</p> </li> <li> <p>Repeat for multiple cycles (default: 3 cycles)</p> </li> </ol>"},{"location":"tutorials/step2-vae-training/#loss-functions","title":"Loss Functions","text":"<p>VAE Loss: <pre><code>Total Loss = Reconstruction Loss + \u03b2 \u00d7 KL Divergence\n</code></pre></p> <p>Reconstruction Network Loss: <pre><code>MSE Loss = Mean Squared Error between refined output and original data\n</code></pre></p>"},{"location":"tutorials/step2-vae-training/#complete-example-workflow","title":"Complete Example Workflow","text":""},{"location":"tutorials/step2-vae-training/#default-training-kirc","title":"Default Training (KIRC)","text":"<pre><code># Train with default parameters\npython scripts/pipeline_steps/2_models.py\n</code></pre>"},{"location":"tutorials/step2-vae-training/#custom-training-brca","title":"Custom Training (BRCA)","text":"<p>Edit the script:</p> <pre><code># Change cancer type\ncancer_type = \"BRCA\"\n\n# Update paths\npath_rnaseq = \"data/interim/preprocessed_BRCA_data/preprocessed_rnaseq.csv\"\npath_clinical = \"data/interim/preprocessed_BRCA_data/clinical_data.csv\"\n\n# Adjust VAE architecture for different gene count\nvae_config.INPUT_DIM = X_train.shape[1]  # Auto-detects gene count\nvae_config.MID_DIM = 1024  # Larger middle layer for BRCA\nvae_config.LATENT_DIM = 512  # Larger latent space\n\n# Adjust reconstruction network\nrecnet_dims = [X_train.shape[1], 4096, 1024, 4096, X_train.shape[1]]\n</code></pre>"},{"location":"tutorials/step2-vae-training/#monitoring-training","title":"Monitoring Training","text":""},{"location":"tutorials/step2-vae-training/#training-progress","title":"Training Progress","text":"<p>The script logs training progress:</p> <pre><code>[INFO] Epoch 1/600: Loss=1234.56, Recon=1200.00, KL=34.56, \u03b2=0.01\n[INFO] Epoch 50/600: Loss=456.78, Recon=400.00, KL=56.78, \u03b2=0.50\n[INFO] Epoch 100/600: Loss=234.56, Recon=180.00, KL=54.56, \u03b2=1.00\n...\n</code></pre>"},{"location":"tutorials/step2-vae-training/#training-plots","title":"Training Plots","text":"<p>After training, inspect the generated plots:</p> <ol> <li>VAE Training History: Shows total loss, reconstruction loss, and KL divergence over epochs</li> <li>Reconstruction Network History: Shows training and test MSE loss</li> </ol>"},{"location":"tutorials/step2-vae-training/#verification","title":"Verification","text":"<p>After training, verify the models:</p> <pre><code>import torch\nimport pandas as pd\nfrom renalprog.modeling.train import VAE\n\n# Load trained VAE\nmodel_dir = \"models/YYYYMMDD_models_KIRC\"\ncheckpoint = torch.load(f\"{model_dir}/vae/final_model.pth\")\n\n# Load test data\nX_test = pd.read_csv(\"data/interim/YYYYMMDD_train_test_split/X_test.csv\", index_col=0)\n\n# Test reconstruction\n# ... (see Step 3 for full reconstruction testing)\n</code></pre>"},{"location":"tutorials/step2-vae-training/#next-steps","title":"Next Steps","text":"<p>After completing VAE training:</p> <ol> <li>Verify models: Check training plots and final losses</li> <li>Test reconstruction: Evaluate on test set</li> <li>Proceed to Step 3: Check reconstruction quality and generate synthetic data</li> </ol> <pre><code>python scripts/pipeline_steps/3_check_reconstruction.py\n</code></pre>"},{"location":"tutorials/step2-vae-training/#additional-resources","title":"Additional Resources","text":"<ul> <li>VAE Tutorial - Original VAE paper</li> <li>\u03b2-VAE - Disentangled representations</li> <li>PyTorch VAE Examples</li> <li>Understanding VAE Loss</li> </ul>"},{"location":"tutorials/step3-reconstruction/","title":"Step 3: Reconstruction Quality Check","text":"<p>This guide explains how to evaluate the reconstruction quality of trained VAE and Reconstruction Network models.</p>"},{"location":"tutorials/step3-reconstruction/#overview","title":"Overview","text":"<p>The reconstruction quality check pipeline performs the following steps:</p> <ol> <li>Model Loading: Load trained VAE and Reconstruction Network models</li> <li>Data Reconstruction: Apply models to preprocessed data</li> <li>UMAP Visualization: Create UMAP plots comparing original and reconstructed data</li> <li>Quality Metrics (optional): Calculate reconstruction quality metrics using sdmetrics</li> </ol> <p>The pipeline supports two model loading options: - Local models: Load models trained in Step 2 from local disk - Hugging Face models: Download and use pre-trained models from Hugging Face Hub</p>"},{"location":"tutorials/step3-reconstruction/#prerequisites","title":"Prerequisites","text":"<p>Before running the reconstruction check pipeline, ensure you have:</p> <ul> <li>Trained models: Either from Step 2 or available on Hugging Face</li> <li>Preprocessed data: From Step 1 data processing</li> <li>Python environment: With PyTorch, pandas, plotly installed</li> <li>Optional: sdmetrics package for quality metrics (only if using <code>--sdmetrics</code>)</li> </ul>"},{"location":"tutorials/step3-reconstruction/#usage","title":"Usage","text":""},{"location":"tutorials/step3-reconstruction/#basic-usage-local-models","title":"Basic Usage (Local Models)","text":"<pre><code>python scripts/pipeline_steps/3_check_reconstruction.py\n</code></pre> <p>This will: - Load locally trained KIRC models - Load preprocessed KIRC data - Generate reconstructions with VAE and Reconstruction Network - Create UMAP visualizations - Save all outputs</p>"},{"location":"tutorials/step3-reconstruction/#using-hugging-face-pre-trained-models","title":"Using Hugging Face Pre-trained Models","text":"<pre><code>python scripts/pipeline_steps/3_check_reconstruction.py --hf_models\n</code></pre> <p>This will: - Download pre-trained KIRC models from Hugging Face - Load preprocessed KIRC data - Generate reconstructions - Create UMAP visualizations</p>"},{"location":"tutorials/step3-reconstruction/#for-brca-cancer-type","title":"For BRCA Cancer Type","text":"<pre><code># With local models\npython scripts/pipeline_steps/3_check_reconstruction.py --cancer_type BRCA\n\n# With Hugging Face models\npython scripts/pipeline_steps/3_check_reconstruction.py --cancer_type BRCA --hf_models\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#with-quality-metrics-evaluation","title":"With Quality Metrics Evaluation","text":"<pre><code>python scripts/pipeline_steps/3_check_reconstruction.py --sdmetrics\n</code></pre> <p>Warning: Using <code>--sdmetrics</code> can take a very long time (several hours) as it computes comprehensive quality metrics.</p>"},{"location":"tutorials/step3-reconstruction/#command-line-arguments","title":"Command-Line Arguments","text":"Argument Type Default Description <code>--cancer_type</code> String 'KIRC' Cancer type to process (KIRC or BRCA) <code>--hf_models</code> Flag False Load pre-trained models from Hugging Face Hub <code>--sdmetrics</code> Flag False Compute comprehensive quality metrics (very slow!)"},{"location":"tutorials/step3-reconstruction/#model-loading","title":"Model Loading","text":""},{"location":"tutorials/step3-reconstruction/#local-models","title":"Local Models","text":"<p>When not using <code>--hf_models</code>, the script expects models in:</p> <pre><code>models/models_{CANCER_TYPE}/\n\u251c\u2500\u2500 vae/\n\u2502   \u251c\u2500\u2500 final_model.pth           # VAE weights\n\u2502   \u2514\u2500\u2500 config.json               # VAE architecture\n\u251c\u2500\u2500 reconstruction_network.pth    # Reconstruction Network weights\n\u2514\u2500\u2500 network_dims.csv              # Network dimensions\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#hugging-face-models","title":"Hugging Face Models","text":"<p>When using <code>--hf_models</code>, the script downloads from:</p> <p>Repository: <code>gpcastelo/evenflow_models</code></p> <p>Files downloaded: - <code>{CANCER_TYPE}/config.json</code> - VAE configuration - <code>{CANCER_TYPE}/20250321_VAE_idim8516_md512_feat256mse_relu.pth</code> (KIRC) - VAE weights - <code>{CANCER_TYPE}/20251209_VAE_idim8954_md1024_feat512mse_relu.pth</code> (BRCA) - VAE weights - <code>{CANCER_TYPE}/network_dims.csv</code> - Reconstruction Network dimensions - <code>{CANCER_TYPE}/network_reconstruction.pth</code> - Reconstruction Network weights</p> <p>Note: Hugging Face models are cached locally after first download.</p>"},{"location":"tutorials/step3-reconstruction/#processing-steps","title":"Processing Steps","text":""},{"location":"tutorials/step3-reconstruction/#step-1-load-preprocessed-data","title":"Step 1: Load Preprocessed Data","text":"<p>Loads data from Step 1: - <code>data/interim/preprocessed_{CANCER_TYPE}_data/preprocessed_rnaseq.csv</code> - <code>data/interim/preprocessed_{CANCER_TYPE}_data/clinical_data.csv</code></p>"},{"location":"tutorials/step3-reconstruction/#step-2-load-models","title":"Step 2: Load Models","text":"<p>Either from local disk or Hugging Face, including: - VAE model with configuration - Reconstruction Network with layer dimensions</p>"},{"location":"tutorials/step3-reconstruction/#step-3-generate-reconstructions","title":"Step 3: Generate Reconstructions","text":"<ol> <li>VAE Reconstruction:</li> <li>Encode data to latent space</li> <li>Decode back to gene expression space</li> <li> <p>Save latent representations</p> </li> <li> <p>Reconstruction Network Refinement:</p> </li> <li>Take VAE output as input</li> <li>Apply postprocessing network</li> <li>Generate refined reconstructions</li> </ol>"},{"location":"tutorials/step3-reconstruction/#step-4-create-umap-visualizations","title":"Step 4: Create UMAP Visualizations","text":"<p>Generate multiple UMAP plots:</p> <ol> <li>Original Data: UMAP of preprocessed data</li> <li>VAE Reconstruction: UMAP of VAE output</li> <li>Reconstruction Network Output: UMAP of postprocessed data</li> <li>Original vs VAE: Comparison plot</li> <li>Original vs RecNet: Comparison plot</li> </ol>"},{"location":"tutorials/step3-reconstruction/#step-5-quality-metrics-optional","title":"Step 5: Quality Metrics (Optional)","text":"<p>If <code>--sdmetrics</code> is enabled: - Calculate Bland-Altman plots - Compute Kolmogorov-Smirnov statistics - Generate comprehensive quality reports</p>"},{"location":"tutorials/step3-reconstruction/#output-files","title":"Output Files","text":""},{"location":"tutorials/step3-reconstruction/#umap-visualizations","title":"UMAP Visualizations","text":"<pre><code>reports/figures/YYYYMMDD_{CANCER_TYPE}_umap_reconstruction/\n\u251c\u2500\u2500 preprocessed.html                    # Original data UMAP\n\u251c\u2500\u2500 VAE_output.html                      # VAE reconstruction UMAP\n\u251c\u2500\u2500 reconstruction_network_output.html   # RecNet output UMAP\n\u251c\u2500\u2500 preprocessed_and_vae.html           # Comparison: original vs VAE\n\u2514\u2500\u2500 preprocessed_and_recnet.html        # Comparison: original vs RecNet\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#quality-metrics-if-using-sdmetrics","title":"Quality Metrics (if using --sdmetrics)","text":"<pre><code>data/processed/YYYYMMDD_reconstruction_evaluation/\n\u251c\u2500\u2500 vae_reconstruction/\n\u2502   \u251c\u2500\u2500 bland_altman_plots/\n\u2502   \u251c\u2500\u2500 ks_statistics.csv\n\u2502   \u2514\u2500\u2500 bland_altman_summary.csv\n\u2514\u2500\u2500 recnet_reconstruction/\n    \u251c\u2500\u2500 bland_altman_plots/\n    \u251c\u2500\u2500 ks_statistics.csv\n    \u2514\u2500\u2500 bland_altman_summary.csv\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#complete-example-workflow","title":"Complete Example Workflow","text":""},{"location":"tutorials/step3-reconstruction/#quick-check-with-local-models","title":"Quick Check with Local Models","text":"<pre><code># Step 1: Train models (from Step 2)\npython scripts/pipeline_steps/2_models.py\n\n# Step 2: Check reconstruction quality\npython scripts/pipeline_steps/3_check_reconstruction.py\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#reproducible-check-with-hugging-face-models","title":"Reproducible Check with Hugging Face Models","text":"<pre><code># Download and use pre-trained models\npython scripts/pipeline_steps/3_check_reconstruction.py --hf_models --cancer_type KIRC\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#comprehensive-evaluation","title":"Comprehensive Evaluation","text":"<pre><code># Full evaluation with quality metrics (takes several hours!)\npython scripts/pipeline_steps/3_check_reconstruction.py --hf_models --sdmetrics\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/step3-reconstruction/#hugging-face-download-fails","title":"Hugging Face Download Fails","text":"<p>If model download from Hugging Face fails: - Check your internet connection - Verify Hugging Face Hub is accessible - Try authenticating: <code>huggingface-cli login</code> - Check repository is public: <code>https://huggingface.co/gpcastelo/evenflow_models</code></p>"},{"location":"tutorials/step3-reconstruction/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>If GPU memory is insufficient: - The script uses <code>force_cpu=True</code> by default - If you enabled GPU, reduce batch size or use CPU</p>"},{"location":"tutorials/step3-reconstruction/#dimension-mismatch","title":"Dimension Mismatch","text":"<p>If you see dimension mismatch errors: - Verify models match the cancer type - Check preprocessed data has correct number of genes - Ensure models were trained on same preprocessed data</p>"},{"location":"tutorials/step3-reconstruction/#sdmetrics-takes-too-long","title":"sdmetrics Takes Too Long","text":"<p>The <code>--sdmetrics</code> option can take several hours: - Run in background: <code>nohup python ... --sdmetrics &amp;</code> - Use a smaller subset for testing - Consider skipping if not needed for your analysis</p>"},{"location":"tutorials/step3-reconstruction/#next-steps","title":"Next Steps","text":"<p>After verifying reconstruction quality:</p> <ol> <li>If reconstruction is good: Proceed to Step 4 (trajectory generation)</li> <li>If reconstruction needs improvement: Return to Step 2 and adjust training parameters</li> </ol> <pre><code># Proceed to trajectory generation\npython scripts/pipeline_steps/4_trajectories.py\n</code></pre>"},{"location":"tutorials/step3-reconstruction/#additional-resources","title":"Additional Resources","text":"<ul> <li>UMAP Documentation</li> <li>Bland-Altman Analysis</li> <li>Kolmogorov-Smirnov Test</li> <li>SDMetrics Documentation</li> <li>Hugging Face Hub Documentation</li> </ul>"},{"location":"tutorials/step4-trajectories/","title":"Step 4: Patient Trajectory Generation","text":"<p>This guide explains how to construct patient trajectories and generate synthetic temporal gene expression data representing disease progression.</p>"},{"location":"tutorials/step4-trajectories/#overview","title":"Overview","text":"<p>The trajectory generation pipeline performs the following steps:</p> <ol> <li>Load Data and Models: Load preprocessed data and trained VAE/Reconstruction Network models</li> <li>Calculate Distances: Compute Wasserstein distances between all patient pairs</li> <li>Link Patients: Create progression trajectories by linking patients from early to late stages</li> <li>Build Networks: Construct trajectory networks showing disease progression paths</li> <li>Generate Positive Trajectories: Create synthetic data for disease progression (early\u2192late)</li> <li>Generate Negative Controls: Create synthetic data for same-stage transitions (no progression)</li> <li>Train/Test Split: Generate separate trajectories for training and testing sets</li> </ol> <p>The pipeline produces: - Positive trajectories: Synthetic gene expression data representing disease progression - Negative trajectories: Control data representing no progression (same stage) - Trajectory metadata: Information about patient connections and progression paths</p>"},{"location":"tutorials/step4-trajectories/#prerequisites","title":"Prerequisites","text":"<p>Before running the trajectory generation pipeline, ensure you have:</p> <ul> <li>Trained models: VAE and Reconstruction Network from Step 2</li> <li>Preprocessed data: From Step 1 data processing</li> <li>Train/test split: Created in Step 2</li> <li>Python environment: With PyTorch, pandas, scipy installed</li> <li>Sufficient disk space: ~1-2 GB for synthetic trajectories</li> </ul>"},{"location":"tutorials/step4-trajectories/#usage","title":"Usage","text":""},{"location":"tutorials/step4-trajectories/#basic-usage","title":"Basic Usage","text":"<pre><code>python scripts/pipeline_steps/4_trajectories.py\n</code></pre> <p>This will: - Load preprocessed KIRC data - Load trained models - Calculate patient distances - Generate progression trajectories - Create synthetic temporal data - Save all outputs</p>"},{"location":"tutorials/step4-trajectories/#configuration-parameters","title":"Configuration Parameters","text":"<p>Edit the script to customize parameters:</p>"},{"location":"tutorials/step4-trajectories/#data-parameters","title":"Data Parameters","text":"Parameter Default Description <code>cancer_type</code> <code>\"KIRC\"</code> Cancer type identifier <code>early_late</code> <code>True</code> Use early/late stage classification (vs. I/II/III/IV)"},{"location":"tutorials/step4-trajectories/#trajectory-parameters","title":"Trajectory Parameters","text":"Parameter Default Description <code>start_with_first_stage</code> <code>True</code> Trajectories must start from early stage <code>link_next</code> <code>5</code> Number of potential next patients to link to <code>distance_metric</code> <code>\"wasserstein\"</code> Distance metric for comparing patients <code>n_timepoints</code> <code>50</code> Number of timepoints in synthetic trajectories <code>interpolation_method</code> <code>\"linear\"</code> Interpolation method in latent space"},{"location":"tutorials/step4-trajectories/#hardware","title":"Hardware","text":"Parameter Default Description <code>force_cpu</code> <code>True</code> Force CPU usage even if GPU is available"},{"location":"tutorials/step4-trajectories/#processing-steps","title":"Processing Steps","text":""},{"location":"tutorials/step4-trajectories/#step-1-load-and-preprocess-data","title":"Step 1: Load and Preprocess Data","text":"<p>Loads data and metadata: - Gene expression data (genes \u00d7 patients) - Clinical metadata (stage, histological type, race, gender) - Processes stage information (early: I/II, late: III/IV)</p>"},{"location":"tutorials/step4-trajectories/#step-2-generate-positive-trajectories","title":"Step 2: Generate Positive Trajectories","text":"<p>Calculates all possible patient-to-patient transitions:</p> <ol> <li>Distance Calculation:</li> <li>Computes Wasserstein distance between gene expression distributions</li> <li>Only considers early\u2192late transitions (disease progression)</li> <li> <p>Stores distances for all valid pairs</p> </li> <li> <p>Patient Linking:</p> </li> <li>For each patient, finds closest <code>link_next</code> patients in next stage</li> <li>Creates connections representing likely progression paths</li> <li>Ensures trajectories start from early stage</li> <li> <p>If race and/or gender are provided as metadata, only links patients with matching attributes, i.e., connections are only made between patients of the same race and gender.</p> </li> <li> <p>Network Building:</p> </li> <li>Constructs directed graph of patient transitions</li> <li>Identifies complete trajectories (early\u2192late paths)</li> <li>Removes isolated nodes</li> </ol>"},{"location":"tutorials/step4-trajectories/#step-3-generate-negative-control-trajectories","title":"Step 3: Generate Negative Control Trajectories","text":"<p>Creates control trajectories with same-stage transitions: - Calculates distances within same stage (early\u2192early, late\u2192late) - Links patients within same stage - Generates control synthetic data (no progression)</p>"},{"location":"tutorials/step4-trajectories/#step-4-traintest-split-trajectories","title":"Step 4: Train/Test Split Trajectories","text":"<p>If train/test split exists from Step 2: - Creates separate trajectory networks for train and test patients - Ensures no data leakage between sets - Generates train-to-train and test-to-test synthetic data</p>"},{"location":"tutorials/step4-trajectories/#step-5-generate-synthetic-temporal-data","title":"Step 5: Generate Synthetic Temporal Data","text":"<p>For each trajectory:</p> <ol> <li>Latent Space Interpolation:</li> <li>Encodes start and end patients to latent space using VAE</li> <li>Performs linear interpolation between latent points</li> <li> <p>Creates <code>n_timepoints</code> intermediate representations</p> </li> <li> <p>Decoding:</p> </li> <li>Decodes interpolated latent points back to gene expression</li> <li>Applies Reconstruction Network for refinement</li> <li> <p>Normalizes using MinMaxScaler (fitted on train data only)</p> </li> <li> <p>Save Data:</p> </li> <li>Each trajectory saved as CSV file</li> <li>Filename format: <code>{patient1}_to_{patient2}_to_{patientN}.csv</code></li> <li>Rows: timepoints, Columns: genes</li> </ol>"},{"location":"tutorials/step4-trajectories/#output-structure","title":"Output Structure","text":"<pre><code>data/processed/patient_trajectories_KIRC/\n\u251c\u2500\u2500 nodes_metadata.csv                         # Patient clinical metadata\n\u251c\u2500\u2500 all_distances.csv                          # All computed distances\n\u251c\u2500\u2500 random_connections_to_5_next.csv           # Selected patient links\n\u251c\u2500\u2500 trajectories.csv                           # Complete trajectory definitions\n\u251c\u2500\u2500 trajectory_mapping.csv                     # Patient to trajectory mapping\n\u251c\u2500\u2500 random_connections_to_5_next_train.csv     # Train set links\n\u251c\u2500\u2500 random_connections_to_5_next_test.csv      # Test set links\n\u251c\u2500\u2500 negatives_random_connections_to_5_next.csv # Negative control links\n\u2514\u2500\u2500 negatives_trajectories.csv                 # Negative trajectory definitions\n\ndata/processed/YYYYMMDD_synthetic_data/kirc/recnet/\n\u251c\u2500\u2500 early_to_late/                             # Positive trajectories\n\u2502   \u251c\u2500\u2500 train_to_train/\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-XX-1234_to_TCGA-YY-5678.csv\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 test_to_test/\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-AA-9999_to_TCGA-BB-0000.csv\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 all/\n\u2502       \u2514\u2500\u2500 {patient1}_to_{patient2}.csv       # All trajectories\n\u2514\u2500\u2500 negatives/                                 # Negative control trajectories\n    \u251c\u2500\u2500 early_to_early/\n    \u2514\u2500\u2500 late_to_late/\n</code></pre>"},{"location":"tutorials/step4-trajectories/#trajectory-file-format","title":"Trajectory File Format","text":"<p>Each trajectory CSV file contains:</p> <p>Structure: - Rows: Timepoints (0 to n_timepoints-1) - Columns: Genes (same as preprocessed data) - Values: Normalized gene expression values [0, 1]</p> <p>Example: <pre><code>,GENE1,GENE2,GENE3,...\n0,0.234,0.567,0.123,...\n1,0.245,0.578,0.134,...\n2,0.256,0.589,0.145,...\n...\n49,0.567,0.890,0.456,...\n</code></pre></p>"},{"location":"tutorials/step4-trajectories/#interpolation-methods","title":"Interpolation Methods","text":""},{"location":"tutorials/step4-trajectories/#linear-interpolation-default","title":"Linear Interpolation (Default)","text":"<p>Performs straight-line interpolation in latent space:</p> <pre><code>z(t) = (1-t) \u00d7 z_start + t \u00d7 z_end\n</code></pre> <p>Where: - <code>t \u2208 [0, 1]</code>: Normalized time - <code>z_start</code>: Latent representation of starting patient - <code>z_end</code>: Latent representation of ending patient</p>"},{"location":"tutorials/step4-trajectories/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"tutorials/step4-trajectories/#trajectory-metadata-files","title":"Trajectory Metadata Files","text":"<p>nodes_metadata.csv: - Patient IDs with clinical information - Used for filtering and trajectory construction</p> <p>all_distances.csv: - All computed patient-to-patient distances - Columns: patient1, patient2, distance, stage1, stage2</p> <p>random_connections_to_5_next.csv: - Selected patient links for trajectories - Each patient linked to up to 5 nearest neighbors in next stage</p> <p>trajectories.csv: - Complete trajectory definitions - Each row is one trajectory with patient sequence</p> <p>trajectory_mapping.csv: - Maps each patient to their trajectory ID - Used for classification in Step 5</p>"},{"location":"tutorials/step4-trajectories/#synthetic-data-files","title":"Synthetic Data Files","text":"<p>Each CSV file represents one trajectory: - Filename shows patient progression path - 50 timepoints (rows) by default - Same genes as input data (columns)</p>"},{"location":"tutorials/step4-trajectories/#advanced-usage","title":"Advanced Usage","text":""},{"location":"tutorials/step4-trajectories/#adjust-trajectory-density","title":"Adjust Trajectory Density","text":"<p>Control number of connections per patient:</p> <pre><code># Link each patient to more/fewer neighbors\nlink_next = 10  # More trajectories\nlink_next = 3   # Fewer trajectories\n</code></pre>"},{"location":"tutorials/step4-trajectories/#modify-temporal-resolution","title":"Modify Temporal Resolution","text":"<p>Change number of timepoints:</p> <pre><code># More detailed trajectories\nn_timepoints = 100\n</code></pre>"},{"location":"tutorials/step4-trajectories/#different-stage-classifications","title":"Different Stage Classifications","text":"<p>Use 4-stage system instead of early/late:</p> <pre><code>early_late = False  # Use Stage I, II, III, IV\n</code></pre>"},{"location":"tutorials/step4-trajectories/#biological-interpretation","title":"Biological Interpretation","text":""},{"location":"tutorials/step4-trajectories/#what-do-trajectories-represent","title":"What Do Trajectories Represent?","text":"<p>Trajectories represent potential disease progression paths: - Starting point: Patient with early-stage cancer - Intermediate points: Hypothetical gene expression states during progression - Ending point: Patient with late-stage cancer</p>"},{"location":"tutorials/step4-trajectories/#positive-vs-negative-trajectories","title":"Positive vs. Negative Trajectories","text":"<p>Positive (early\u2192late): - Represent actual disease progression - Show gene expression changes during cancer advancement - Used for training progression classifiers</p> <p>Negative (same stage): - Control trajectories with no progression - Help distinguish true progression from random variation - Important for model specificity</p>"},{"location":"tutorials/step4-trajectories/#next-steps","title":"Next Steps","text":"<p>After generating trajectories:</p> <ol> <li>Inspect outputs: Check trajectory files and metadata</li> <li>Visualize trajectories: Plot gene expression changes over time</li> <li>Proceed to Step 5: Train classification models on trajectory data</li> </ol> <pre><code>python scripts/pipeline_steps/5_classification.py\n</code></pre>"},{"location":"tutorials/step5-classification/","title":"Step 5: Classification Pipeline","text":"<p>This guide explains how to train classification models on static data and apply them to synthetic trajectory data for disease progression prediction.</p>"},{"location":"tutorials/step5-classification/#overview","title":"Overview","text":"<p>The classification pipeline performs the following steps:</p> <ol> <li>Load Data: Load preprocessed data and train/test splits</li> <li>Train Classifiers: Train multiple XGBoost classifiers with different seeds</li> <li>Select Best Model: Choose best model based on Cohen's Kappa score</li> <li>Apply to Trajectories: Classify synthetic trajectory data</li> <li>Visualize Results: Create plots showing progression predictions over time</li> </ol> <p>The pipeline trains classifiers on: - Static data: Real patient gene expression (early vs. late stage)</p> <p>Then applies classifiers to: - Train-to-train trajectories: Synthetic progressions from training patients - Test-to-test trajectories: Synthetic progressions from test patients (unseen)</p>"},{"location":"tutorials/step5-classification/#prerequisites","title":"Prerequisites","text":"<p>Before running the classification pipeline, ensure you have:</p> <ul> <li>Preprocessed data: From Step 1 data processing</li> <li>Train/test split: Created in Step 2 (VAE training)</li> <li>Synthetic trajectories: Generated in Step 4</li> <li>Python environment: With XGBoost, pandas, plotly installed</li> <li>Sufficient compute: Multi-core CPU recommended (uses all available cores)</li> </ul>"},{"location":"tutorials/step5-classification/#usage","title":"Usage","text":""},{"location":"tutorials/step5-classification/#basic-usage","title":"Basic Usage","text":"<pre><code>python scripts/pipeline_steps/5_classification.py\n</code></pre> <p>This will: - Load preprocessed KIRC data - Train 10 XGBoost classifiers with different seeds - Select best model based on Cohen's Kappa - Apply to train-to-train and test-to-test trajectories - Generate visualization plots</p>"},{"location":"tutorials/step5-classification/#configuration-parameters","title":"Configuration Parameters","text":"<p>Edit the script to customize parameters:</p>"},{"location":"tutorials/step5-classification/#data-parameters","title":"Data Parameters","text":"Parameter Default Description <code>cancer_type</code> <code>\"KIRC\"</code> Cancer type identifier ### Classification Parameters Parameter Default Description <code>n_seeds</code> <code>10</code> Number of classifiers to train with different seeds <code>n_trials</code> <code>100</code> Number of Optuna trials for hyperparameter tuning <code>n_boosting_rounds</code> <code>100</code> Number of XGBoost boosting rounds <code>num_threads</code> <code>os.cpu_count()-1</code> Number of CPU threads to use"},{"location":"tutorials/step5-classification/#trajectory-parameters","title":"Trajectory Parameters","text":"Parameter Default Description <code>n_timepoints</code> <code>50</code> Number of timepoints in trajectories"},{"location":"tutorials/step5-classification/#processing-steps","title":"Processing Steps","text":""},{"location":"tutorials/step5-classification/#step-1-load-data","title":"Step 1: Load Data","text":"<p>Loads preprocessed data and train/test splits: - Gene expression data (samples \u00d7 genes) - Clinical metadata (stage labels) - Train/test patient lists - Optional: Important genes for feature selection</p>"},{"location":"tutorials/step5-classification/#step-2-train-multiple-classifiers","title":"Step 2: Train Multiple Classifiers","text":"<p>Trains multiple XGBoost classifiers:</p> <ol> <li>Hyperparameter Optimization:</li> <li>Uses Optuna for Bayesian optimization</li> <li>Optimizes Cohen's Kappa score</li> <li> <p>Searches over learning rate, depth, regularization, etc.</p> </li> <li> <p>Training:</p> </li> <li>Trains on training set only</li> <li>Uses stratified cross-validation</li> <li> <p>Records all metrics (accuracy, precision, recall, F1, Kappa)</p> </li> <li> <p>Multiple Seeds:</p> </li> <li>Trains with different random seeds</li> <li>Captures model variance</li> <li>Enables robust model selection</li> </ol>"},{"location":"tutorials/step5-classification/#step-3-select-best-model","title":"Step 3: Select Best Model","text":"<p>Selects best performing model: - Ranks models by Cohen's Kappa score - Saves best model and metadata - Logs performance metrics</p>"},{"location":"tutorials/step5-classification/#step-4-apply-to-trajectories","title":"Step 4: Apply to Trajectories","text":"<p>Classifies synthetic trajectory data:</p> <ol> <li>Train-to-Train:</li> <li>Trajectories from training patients</li> <li>Shows classifier behavior on training distribution</li> <li> <p>Expected to predict progression well</p> </li> <li> <p>Test-to-Test:</p> </li> <li>Trajectories from held-out test patients</li> <li>True test of generalization</li> <li> <p>Most important for model evaluation</p> </li> <li> <p>Time-Course Classification:</p> </li> <li>Applies classifier to each timepoint</li> <li>Tracks predicted stage probability over time</li> <li>Visualizes progression dynamics</li> </ol>"},{"location":"tutorials/step5-classification/#step-5-visualize-results","title":"Step 5: Visualize Results","text":"<p>Generates multiple visualizations: - Classification metrics boxplots - Trajectory predictions over time - Stage probability heatmaps - Individual trajectory plots</p>"},{"location":"tutorials/step5-classification/#output-files","title":"Output Files","text":""},{"location":"tutorials/step5-classification/#classification-models-and-metrics","title":"Classification Models and Metrics","text":"<pre><code>models/YYYYMMDD_classification_KIRC/\n\u251c\u2500\u2500 xgboost_model.json                    # Best trained model\n\u251c\u2500\u2500 best_model_metadata.csv               # Model metadata (seed, kappa, etc.)\n\u251c\u2500\u2500 classification_metrics.csv            # All models' metrics\n\u251c\u2500\u2500 classification_summary.csv            # Summary statistics\n\u2514\u2500\u2500 splits_cv/\n    \u2514\u2500\u2500 fold_*.csv                        # Cross-validation splits\n\nreports/figures/YYYYMMDD_classification_KIRC/\n\u2514\u2500\u2500 boxplot_metrics.html/png/pdf/svg     # Metrics visualization\n</code></pre>"},{"location":"tutorials/step5-classification/#trajectory-classifications","title":"Trajectory Classifications","text":"<pre><code>data/processed/YYYYMMDD_trajectory_classifications/\n\u251c\u2500\u2500 train_to_train/\n\u2502   \u2514\u2500\u2500 predictions_train_to_train.csv    # Training trajectory predictions\n\u251c\u2500\u2500 test_to_test/\n\u2502   \u2514\u2500\u2500 predictions_test_to_test.csv      # Test trajectory predictions\n\u2514\u2500\u2500 figures/\n    \u251c\u2500\u2500 train_to_train/\n    \u2502   \u251c\u2500\u2500 heatmap_trajectories.html     # Prediction heatmap\n    \u2502   \u251c\u2500\u2500 individual_trajectories/      # Individual plots\n    \u2502   \u2514\u2500\u2500 summary_statistics.csv\n    \u2514\u2500\u2500 test_to_test/\n        \u251c\u2500\u2500 heatmap_trajectories.html\n        \u251c\u2500\u2500 individual_trajectories/\n        \u2514\u2500\u2500 summary_statistics.csv\n</code></pre>"},{"location":"tutorials/step5-classification/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tutorials/step5-classification/#good-classification-performance","title":"Good Classification Performance","text":"<p>Indicators of good performance: - Cohen's Kappa &gt; 0.60: Substantial agreement - Smooth progression: Gradual shift from early\u2192late predictions in trajectories</p>"},{"location":"tutorials/step5-classification/#poor-classification-performance","title":"Poor Classification Performance","text":"<p>Warning signs: - Kappa &lt; 0.40: Weak classification - Sharp jumps in predictions: Unrealistic progression</p>"},{"location":"tutorials/step5-classification/#expected-trajectory-behavior","title":"Expected Trajectory Behavior","text":"<p>For early\u2192late trajectories: 1. Initial timepoints: High probability of early stage 2. Middle timepoints: Gradual transition zone 3. Final timepoints: High probability of late stage</p>"},{"location":"tutorials/step5-classification/#advanced-usage","title":"Advanced Usage","text":""},{"location":"tutorials/step5-classification/#feature-selection","title":"Feature Selection","text":"<p>Use only a set of genes to train the classifier and perform trajectory classification:</p> <pre><code>USE_IMPORTANT_GENES = True\n\n# Specify path to important genes, e.g.:\nimportant_genes_path = EXTERNAL_DATA_DIR / \"genes.csv\"\n</code></pre>"},{"location":"tutorials/step5-classification/#adjust-hyperparameter-search","title":"Adjust Hyperparameter Search","text":"<p>Modify optimization ranges in the <code>classification_benchmark</code> function:</p> <pre><code># Example: Limit tree depth\ntrial.suggest_int(\"max_depth\", 3, 6)  # Instead of 3, 10\n\n# Example: Adjust learning rate\ntrial.suggest_float(\"learning_rate\", 0.001, 0.1)  # Instead of 0.001, 0.3\n</code></pre>"},{"location":"tutorials/step5-classification/#custom-metrics","title":"Custom Metrics","text":"<p>Track additional metrics in training loop:</p> <pre><code># Add to metrics dictionary\nmetrics = {\n    \"Accuracy\": accuracy,\n    \"Custom Metric\": custom_metric,\n    # ... other metrics\n}\n</code></pre>"},{"location":"tutorials/step5-classification/#next-steps","title":"Next Steps","text":"<p>After classification:</p> <ol> <li>Analyze results: Review metrics and trajectory predictions</li> <li>Proceed to Step 6: Perform pathway enrichment analysis</li> </ol> <pre><code>python scripts/enrichment/pipeline.sh\n</code></pre>"},{"location":"tutorials/step5-classification/#additional-resources","title":"Additional Resources","text":"<ul> <li>XGBoost Documentation</li> <li>Cohen's Kappa</li> <li>Optuna Tutorial</li> <li>Feature Importance with SHAP</li> <li>Classification Metrics Guide</li> </ul>"},{"location":"tutorials/step6-enrichment/","title":"Step 6: Enrichment Analysis Pipeline","text":"<p>This guide explains how to run the enrichment analysis pipeline for trajectory data using DESeq2 and GSEA (Gene Set Enrichment Analysis).</p>"},{"location":"tutorials/step6-enrichment/#overview","title":"Overview","text":"<p>The enrichment pipeline performs three main steps:</p> <ol> <li>DESeq2 Analysis: Performs differential expression analysis on trajectory data</li> <li>GSEA Analysis: Runs Gene Set Enrichment Analysis on the DESeq2 results</li> <li>Trajectory Formatting: Creates the final trajectory dataset with enrichment results</li> </ol> <p>The pipeline generates \"greasy files\" (batch job files) that need to be executed manually, allowing you to choose the best execution method for your computing environment.</p>"},{"location":"tutorials/step6-enrichment/#prerequisites","title":"Prerequisites","text":"<p>Before running the enrichment pipeline, ensure you have:</p> <ul> <li>GSEA installed: The pipeline expects GSEA CLI to be available at <code>GSEA_4.3.2/gsea-cli.sh</code></li> <li>See GSEA Installation Guide for setup instructions</li> <li>Python environment: With required packages (DESeq2, pandas, etc.)</li> <li>Trajectory data: CSV files containing trajectory information (see previous step).</li> <li>Preprocessed data: RNASeq and clinical metadata files.</li> <li>Control data: Control RNASeq and clinical metadata.</li> <li>Note: Control data is included in the repository at <code>controls/{CANCER_TYPE}/</code> with files:<ul> <li><code>rnaseq_controls.csv</code></li> <li><code>clinical_controls.csv</code></li> </ul> </li> <li>You can use repo controls by setting <code>USE_REPO_CONTROLS=true</code></li> <li>Alternatively, controls from <code>data/interim/preprocessed_{CANCER_TYPE}_data/controls/</code> will be used by default</li> <li>Pathways file: GMT format pathway file (default: <code>data/external/ReactomePathways.gmt</code>).</li> </ul>"},{"location":"tutorials/step6-enrichment/#usage","title":"Usage","text":""},{"location":"tutorials/step6-enrichment/#basic-usage-with-defaults","title":"Basic Usage (with defaults)","text":"<p>For KIRC cancer type with default paths:</p> <pre><code>bash scripts/enrichment/pipeline.sh kirc\n</code></pre>"},{"location":"tutorials/step6-enrichment/#custom-configuration","title":"Custom Configuration","text":"<p>You can override default paths by setting environment variables:</p> <pre><code># Example: Custom trajectory directory\nCMD_DIR=/path/to/custom/trajectories bash scripts/enrichment/pipeline.sh kirc\n\n# Example: Use controls from repository root\nUSE_REPO_CONTROLS=true bash scripts/enrichment/pipeline.sh kirc\n\n# Example: Multiple custom parameters\nCMD_DIR=/path/to/trajectories \\\nDATA_DIR=/path/to/rnaseq.csv \\\nMETADATA_DIR=/path/to/clinical.csv \\\nPATHWAYS_FILE=/path/to/custom_pathways.gmt \\\nbash scripts/enrichment/pipeline.sh kirc\n</code></pre>"},{"location":"tutorials/step6-enrichment/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"tutorials/step6-enrichment/#required-parameter","title":"Required Parameter","text":"<ul> <li>cancer_type: Cancer type identifier (e.g., <code>kirc</code>, <code>brca</code>, <code>luad</code>)</li> <li>Passed as the first argument to the script</li> </ul>"},{"location":"tutorials/step6-enrichment/#optional-parameters-environment-variables","title":"Optional Parameters (Environment Variables)","text":"Parameter Description Default (KIRC) <code>CMD_DIR</code> Directory containing trajectory CSV files <code>data/processed/synthetic_data/kirc/recnet/early_to_late/test_to_test/</code> <code>ERR_OUT_DIR</code> Directory for error/output logs <code>data/processed/synthetic_data/kirc/recnet/early_to_late/err_out_kirc_test</code> <code>ST_FILE</code> Source-target file path <code>data/processed/patient_trajectories_KIRC/random_connections_to_5_next_test.csv</code> <code>DATA_DIR</code> RNASeq data file path <code>data/interim/preprocessed_KIRC_data/KIRC_rnaseq.csv</code> <code>METADATA_DIR</code> Clinical metadata file path <code>data/interim/preprocessed_KIRC_data/KIRC_clinical.csv</code> <code>CONTROL_DATA</code> Control RNASeq data file path <code>data/interim/preprocessed_KIRC_data/controls/KIRC_control_rnaseq.csv</code> <code>CONTROL_METADATA</code> Control clinical metadata file path <code>data/interim/preprocessed_KIRC_data/controls/KIRC_control_clinical.csv</code> <code>META_NODES</code> Nodes metadata file path <code>data/processed/patient_trajectories_KIRC/nodes_metadata.csv</code> <code>STAGE_TRANSITION</code> Stage transition identifier <code>early_to_late</code> <code>PATHWAYS_FILE</code> Pathways GMT file <code>data/external/ReactomePathways.gmt</code> <code>USE_REPO_CONTROLS</code> Use controls from repo root <code>false</code> (uses preprocessed controls by default)"},{"location":"tutorials/step6-enrichment/#default-paths-for-other-cancer-types","title":"Default Paths for Other Cancer Types","text":"<p>For cancer types other than KIRC, the script uses different default paths. You can override these by setting the environment variables explicitly.</p>"},{"location":"tutorials/step6-enrichment/#available-gsea-parameters","title":"Available GSEA Parameters","text":"<p>GSEA parameters can be passed as arguments to the enrichment functions, making it easy to customize GSEA behavior without modifying configuration files.</p> <p>All functions that use <code>build_gsea_command</code> accept the following optional parameters:</p> Parameter Type Default Description <code>pathway_file</code> str <code>'data/external/ReactomePathways.gmt'</code> Path to the GMT file containing gene sets <code>mode</code> str <code>'Max_probe'</code> GSEA collapse mode for handling multiple probes per gene <code>norm</code> str <code>'meandiv'</code> Normalization mode for gene set enrichment scores <code>nperm</code> int <code>1000</code> Number of permutations for significance testing <code>rnd_seed</code> str <code>'timestamp'</code> Random seed for reproducibility <code>scoring_scheme</code> str <code>'weighted'</code> Scoring scheme for enrichment calculation <code>set_max</code> int <code>500</code> Maximum size of gene sets to include <code>set_min</code> int <code>15</code> Minimum size of gene sets to include"},{"location":"tutorials/step6-enrichment/#functions-supporting-gsea-parameters","title":"Functions Supporting GSEA Parameters","text":"<p>The following functions now accept GSEA parameters as kwargs:</p> <ol> <li><code>build_gsea_command()</code> - Core function that builds the GSEA command</li> <li><code>get_rnk_single_patient()</code> - Generate GSEA command for a single patient</li> <li><code>fun_single_patient_and_gsea()</code> - Perform analysis for a single patient</li> <li><code>fun_synth_single_patient_and_gsea()</code> - Perform analysis for synthetic patients</li> </ol>"},{"location":"tutorials/step6-enrichment/#pipeline-execution-steps","title":"Pipeline Execution Steps","text":""},{"location":"tutorials/step6-enrichment/#step-1-generate-deseq2-greasy-file","title":"Step 1: Generate DESeq2 Greasy File","text":"<p>The script will automatically generate a greasy file containing all DESeq2 commands:</p> <pre><code>scripts/enrichment/greasy_deseq_file_&lt;cancer_type&gt;.txt\n</code></pre> <p>You must execute this file manually. The script will display instructions when this file is generated.</p>"},{"location":"tutorials/step6-enrichment/#execution-options-for-deseq2-greasy-file","title":"Execution Options for DESeq2 Greasy File:","text":"<p>Even though the commands can be run sequentially, this would be very slow for large datasets. It is recommended to parallelize the execution, and highly recommended to use HPC.</p> <p>Here are several options to run the DESeq2 greasy file:</p> <p>Option 1: Sequential Execution <pre><code>bash scripts/enrichment/greasy_deseq_file_kirc.txt\n</code></pre></p> <p>Option 2: GNU Parallel (Recommended) <pre><code># Run with 8 parallel jobs\nparallel -j 8 &lt; scripts/enrichment/greasy_deseq_file_kirc.txt\n\n# Run with all available cores\nparallel &lt; scripts/enrichment/greasy_deseq_file_kirc.txt\n</code></pre></p> <p>Option 3: SLURM Job Scheduler for HPC <pre><code>#!/bin/bash\n#SBATCH --job-name=deseq_kirc\n#SBATCH --output=logs/deseq_%j.out\n#SBATCH --error=logs/deseq_%j.err\n#SBATCH --time=02:00:00\n#SBATCH --ntasks=10\n#SBATCH --cpus-per-task=4\n\n# Load required modules\nmodule load parallel\n\n# Run greasy file with parallel\nparallel -j 10 &lt; scripts/enrichment/greasy_deseq_file_kirc.txt\n</code></pre></p>"},{"location":"tutorials/step6-enrichment/#step-2-generate-gsea-greasy-file","title":"Step 2: Generate GSEA Greasy File","text":"<p>After DESeq2 completes, the script will generate a GSEA greasy file:</p> <pre><code>&lt;CMD_DIR&gt;/greasy_&lt;cancer_type&gt;.sh\n</code></pre> <p>You must execute this file manually. The script will display instructions when this file is generated.</p>"},{"location":"tutorials/step6-enrichment/#execution-options-for-gsea-greasy-file","title":"Execution Options for GSEA Greasy File:","text":"<p>As mentioned before, sequential execution is possible but slow. Parallel execution is recommended.</p> <p>Option 1: Sequential Execution <pre><code>bash data/processed/.../greasy_kirc.sh\n</code></pre></p> <p>Option 2: GNU Parallel (Recommended) <pre><code># Run with 8 parallel jobs\nparallel -j 8 &lt; data/processed/.../greasy_kirc.sh\n\n# Monitor progress with a progress bar\nparallel --progress -j 8 &lt; data/processed/.../greasy_kirc.sh\n</code></pre></p> <p>Option 3: SLURM with GNU Parallel <pre><code>#!/bin/bash\n#SBATCH --job-name=gsea_kirc\n#SBATCH --output=logs/gsea_%j.out\n#SBATCH --error=logs/gsea_%j.err\n#SBATCH --time=04:00:00\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1\n\n# Load required modules\nmodule load parallel\nmodule load java  # Required for GSEA\n\n# Export GSEA function\ngsea() {\n    \"$(pwd)/GSEA_4.3.2/gsea-cli.sh\" \"$@\"\n}\nexport -f gsea\n\n# Run greasy file with parallel\nparallel -j 8 &lt; data/processed/.../greasy_kirc.sh\n</code></pre></p>"},{"location":"tutorials/step6-enrichment/#step-3-final-trajectory-formatting","title":"Step 3: Final Trajectory Formatting","text":"<p>After GSEA completes, you need to run the trajectory formatting step manually.  The script will display the exact command to execute with all the correct parameters.</p> <p>You must execute this command manually. The script will display instructions.</p>"},{"location":"tutorials/step6-enrichment/#execution","title":"Execution:","text":"<p>Run the command displayed by the script:</p> <pre><code>python scripts/enrichment/trajectory_formatting.py \\\n  --path_synth &lt;CMD_DIR&gt; \\\n  --pathways_file &lt;PATHWAYS_FILE&gt; \\\n  --save_dir &lt;parent_dir&gt; \\\n  --cancer_type &lt;cancer_type&gt;\n</code></pre> <p>Replace the placeholders with the actual values shown in the script output.  This creates the final trajectory dataset with enrichment results.</p>"},{"location":"tutorials/step6-enrichment/#complete-example-workflow","title":"Complete Example Workflow","text":"<p>Here's a complete example for KIRC with custom paths:</p> <pre><code># Step 1: Set up environment variables\nexport CMD_DIR=\"/data/trajectories/kirc/test\"\nexport DATA_DIR=\"/data/preprocessed/kirc/rnaseq.csv\"\nexport METADATA_DIR=\"/data/preprocessed/kirc/clinical.csv\"\nexport CONTROL_DATA=\"/data/controls/kirc/rnaseq_control.csv\"\nexport CONTROL_METADATA=\"/data/controls/kirc/clinical_control.csv\"\nexport PATHWAYS_FILE=\"/data/pathways/ReactomePathways.gmt\"\n\n# Step 2: Run the enrichment pipeline script\nbash scripts/enrichment/pipeline.sh kirc\n\n# The script will pause after generating the DESeq2 greasy file\n# Step 3: Execute the DESeq2 greasy file\nparallel -j 8 &lt; scripts/enrichment/greasy_deseq_file_kirc.txt\n\n# The script will then pause after generating the GSEA greasy file\n# Step 4: Execute the GSEA greasy file\nparallel -j 8 &lt; data/processed/.../greasy_kirc.sh\n\n# Step 5: Execute the trajectory formatting step\n# Use the exact command shown by the script output\npython scripts/enrichment/trajectory_formatting.py \\\n  --path_synth /data/trajectories/kirc/test \\\n  --pathways_file /data/pathways/ReactomePathways.gmt \\\n  --save_dir /data/trajectories/kirc \\\n  --cancer_type kirc\n\n# Results will be saved to the parent directory of CMD_DIR\n</code></pre>"},{"location":"tutorials/step6-enrichment/#background-execution","title":"Background Execution","text":"<p>For long-running pipelines, you can run the script in the background:</p> <pre><code>nohup bash scripts/enrichment/pipeline.sh kirc &gt; enrichment.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Monitor progress: <pre><code>tail -f enrichment.log\n</code></pre></p>"},{"location":"tutorials/step6-enrichment/#output-files","title":"Output Files","text":""},{"location":"tutorials/step6-enrichment/#generated-greasy-files","title":"Generated Greasy Files:","text":"<ul> <li><code>scripts/enrichment/greasy_deseq_file_&lt;cancer_type&gt;.txt</code>: DESeq2 commands</li> <li><code>&lt;CMD_DIR&gt;/greasy_&lt;cancer_type&gt;.sh</code>: GSEA commands</li> </ul>"},{"location":"tutorials/step6-enrichment/#final-output","title":"Final Output:","text":"<ul> <li><code>&lt;parent_dir&gt;/&lt;cancer_type&gt;_trajectory_enrichment.csv</code>: Final trajectory dataset with enrichment results</li> <li>DESeq2 results in <code>&lt;CMD_DIR&gt;/</code> directory</li> <li>GSEA results in <code>&lt;CMD_DIR&gt;/</code> subdirectories</li> </ul>"},{"location":"tutorials/step6-enrichment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/step6-enrichment/#gsea-not-found","title":"GSEA Not Found","text":"<p>If you see errors about GSEA not being found: - Verify GSEA is installed at <code>GSEA_4.3.2/gsea-cli.sh</code> - Check execution permissions: <code>chmod +x GSEA_4.3.2/gsea-cli.sh</code> - See GSEA Installation Guide</p>"},{"location":"tutorials/step6-enrichment/#no-csv-files-found","title":"No CSV Files Found","text":"<p>If the script reports no CSV files in CMD_DIR: - Verify CMD_DIR contains trajectory CSV files - Check file permissions - Ensure the trajectory generation step completed successfully</p>"},{"location":"tutorials/step6-enrichment/#additional-resources","title":"Additional Resources","text":"<ul> <li>GSEA Documentation</li> <li>pyDESeq2 Documentation</li> <li>GNU Parallel Tutorial</li> <li>Enrichment Analysis Guide</li> </ul>"},{"location":"tutorials/step6-enrichment/#next-steps","title":"Next Steps","text":"<p>After completing the enrichment pipeline: - Analyze the final trajectory enrichment results - Visualize pathway changes along trajectories - Generate pathway heatmaps (see Pathway Heatmap Guide) - Perform downstream analysis and interpretation</p>"},{"location":"tutorials/visualization/","title":"Visualization Tutorial","text":"<p>This guide shows you how to use the visualization functions in <code>renalprog</code> to create plots for your analysis.</p>"},{"location":"tutorials/visualization/#overview","title":"Overview","text":"<p>The <code>renalprog</code> package provides two main modules for visualization:</p> <ol> <li><code>plots</code> module: General-purpose plotting functions using Plotly (interactive plots)</li> <li><code>enrichment</code> module: Specialized heatmaps for pathway enrichment analysis using Matplotlib</li> </ol> <p>Interactive vs Static</p> <ul> <li>Plotly plots (<code>plots</code> module): Interactive HTML plots with hover tooltips, zoom, and pan</li> <li>Matplotlib plots (<code>enrichment</code> module): Static publication-ready heatmaps</li> </ul> <p>Both save in multiple formats: HTML, PNG, PDF, and SVG</p> <p>Dependencies</p> <p>All functions using plotly require the <code>kaleido</code> package for static image export and Google Chrome.</p> <p>Plotly outputs</p> <p>The <code>plots</code> module functions save interactive plots in HTML, as well as static images (PNG, PDF, SVG).</p>"},{"location":"tutorials/visualization/#plots-module","title":"Plots Module","text":"<p>The <code>plots</code> module provides interactive visualizations using Plotly.</p>"},{"location":"tutorials/visualization/#available-functions","title":"Available Functions","text":"Function Purpose Output <code>save_plot()</code> Helper function to save Plotly figures Save in multiple formats (.html, .png, .pdf, and .svg by default) <code>plot_training_history()</code> VAE training progress Loss curves over epochs <code>plot_reconstruction_losses()</code> Reconstruction network training Train/test loss curves <code>plot_trajectory()</code> Gene expression trajectories Line plots across timepoints <code>plot_pca_variance()</code> PCA variance explained Bar chart of PCs' variances <code>plot_umap_plotly()</code> Dimensionality reduction 2D/3D UMAP scatter plot <code>plot_enrichment_results()</code> Enrichment scores Bar chart of pathway scores"},{"location":"tutorials/visualization/#importing-the-functions","title":"Importing the functions","text":"<pre><code>from renalprog.plots import (\n    plot_training_history,\n    plot_reconstruction_losses,\n    plot_trajectory,\n    plot_umap_plotly,\n\n)\nfrom renalprog.enrichment import plot_enrichment_results\nfrom pathlib import Path\n</code></pre>"},{"location":"tutorials/visualization/#1-training-history-visualization","title":"1. Training History Visualization","text":"<p>Visualize VAE training progress with train and validation losses.</p> <pre><code># After training a VAE\nfrom renalprog.modeling.train import train_vae_with_postprocessing\n\n# ... training code ...\nvae_model, network, vae_history, reconstruction_history = train_vae_with_postprocessing(\n    X_train=X_train,\n    X_test=X_test,\n    vae_config=vae_config,\n    # ... other parameters ...\n)\n\n# Plot VAE training history\nplot_training_history(\n    history=vae_history,\n    save_path=Path('reports/figures/vae_training.png'),\n    title='VAE Training History',\n    log_scale=False  # Set to True for log scale\n)\n</code></pre> <p>Expected Output: - Interactive plot showing train/validation loss over epochs - Separate curves for total loss, reconstruction loss, and KL divergence - Saved in HTML, PNG, PDF, and SVG formats</p>"},{"location":"tutorials/visualization/#2-reconstruction-network-training","title":"2. Reconstruction Network Training","text":"<p>Visualize the training of the reconstruction network (post-processing).</p> <pre><code># Plot reconstruction network training\nplot_reconstruction_losses(\n    loss_train=reconstruction_history[\"train_loss\"],\n    loss_test=reconstruction_history[\"test_loss\"],\n    save_path=Path('reports/figures/reconstruction_training.png'),\n    title='Reconstruction Network Training',\n    show_best_epoch=True  # Highlight epoch with lowest validation loss\n)\n</code></pre> <p>Features: - Shows train vs test loss - Optionally marks the best epoch - Helps identify overfitting</p>"},{"location":"tutorials/visualization/#3-gene-expression-trajectories","title":"3. Gene Expression Trajectories","text":"<p>Plot how gene expression changes along disease progression trajectories.</p> <pre><code>import pandas as pd\n\n# Load trajectory data (timepoints \u00d7 genes)\ntrajectory_df = pd.read_csv('trajectory_expression.csv', index_col=0)\n\n# Plot multiple genes\ngenes_to_plot = ['TP53', 'VEGFA', 'HIF1A', 'VHL']\n\nplot_trajectory(\n    trajectory_df=trajectory_df,\n    genes=genes_to_plot,\n    save_path=Path('reports/figures/gene_trajectories.png'),\n    title='Gene Expression Trajectories',\n    normalize=True,  # Normalize each gene to [0, 1]\n    show_markers=True,\n    colormap='Viridis'\n)\n</code></pre> <p>Output: - Line plot with one trace per gene - X-axis: Timepoints (pseudo-time from early to late) - Y-axis: Expression level - (HTML) Interactive hover showing exact values</p>"},{"location":"tutorials/visualization/#4-umap-visualization","title":"4. UMAP Visualization","text":"<p>Create 2D or 3D UMAP plots to visualize high-dimensional data.</p> <pre><code>import pandas as pd\n\n# Load expression data and clinical metadata\ndata = pd.read_csv('preprocessed_rnaseq.csv', index_col=0)  # samples \u00d7 genes\nclinical = pd.read_csv('clinical_data.csv', index_col=0)    # samples \u00d7 metadata\n\n# Ensure data is samples \u00d7 genes (transpose if needed)\nif data.shape[0] &gt; data.shape[1]:\n    data = data.T\n\n# Create UMAP plot colored by disease stage\nplot_umap_plotly(\n    data=data,\n    clinical=clinical,\n    colors_dict={'early': '#6495ed', 'late': '#b22222'},\n    n_components=2,  # 2D plot (use 3 for 3D)\n    save_fig=True,\n    save_as='reports/figures/umap_by_stage',\n    seed=2023,\n    title='UMAP: Original Data by Stage',\n    show=True\n)\n</code></pre> <p>Parameters: - <code>data</code>: Gene expression matrix (samples \u00d7 genes) - <code>clinical</code>: Clinical metadata with 'stage' column (values: 'early' or 'late') - <code>colors_dict</code>: Mapping of stage to color - <code>n_components</code>: 2 for 2D plot, 3 for 3D plot - <code>seed</code>: Random seed for reproducibility</p> <p>Output: - Interactive scatter plot - Hover shows sample ID and stage - Can zoom, pan, and (in 3D) rotate</p>"},{"location":"tutorials/visualization/#enrichment-module-heatmaps","title":"Enrichment Module Heatmaps","text":"<p>The <code>enrichment</code> module provides specialized functions for creating publication-quality pathway enrichment heatmaps.</p>"},{"location":"tutorials/visualization/#main-functions","title":"Main Functions","text":""},{"location":"tutorials/visualization/#1-generate_pathway_heatmap","title":"1. <code>generate_pathway_heatmap()</code>","text":"<p>Creates multiple heatmaps showing pathway regulation across disease progression.</p> <p>Purpose: Visualize how biological pathways are regulated over pseudo-time from early to late disease stages.</p> <p>What it does: - Aggregates enrichment scores across all trajectories - Creates 4-5 heatmaps:   1. Top 50 most changing pathways   2. Top 50 upregulated pathways   3. Top 50 downregulated pathways   4. High-level Reactome pathways   5. Literature-curated pathways (optional)</p>"},{"location":"tutorials/visualization/#2-plot_heatmap_regulation-internal-helper","title":"2. <code>plot_heatmap_regulation()</code> (internal helper)","text":"<p>Creates individual heatmap with custom styling.</p>"},{"location":"tutorials/visualization/#complete-example-pathway-enrichment-heatmaps","title":"Complete Example: Pathway Enrichment Heatmaps","text":"<pre><code>from renalprog.enrichment import generate_pathway_heatmap\nimport pandas as pd\nfrom pathlib import Path\n\n# ============================================================================\n# Step 1: Load Enrichment Data\n# ============================================================================\n# Load GSEA results from trajectory analysis\n# Expected columns: [Patient, Idx, Transition, NAME, ES, NES, FDR q-val]\nenrichment_df = pd.read_csv('trajectory_enrichment_results.csv')\n\nprint(f\"Enrichment data shape: {enrichment_df.shape}\")\nprint(f\"Columns: {enrichment_df.columns.tolist()}\")\nprint(f\"Number of pathways: {enrichment_df['NAME'].nunique()}\")\nprint(f\"Number of timepoints: {enrichment_df['Idx'].nunique()}\")\n\n# ============================================================================\n# Step 2: Generate Heatmaps\n# ============================================================================\noutput_dir = Path('reports/figures/pathway_heatmaps')\n\nheatmap_data, figures = generate_pathway_heatmap(\n    enrichment_df=enrichment_df,\n    output_dir=output_dir,\n    fdr_threshold=0.05,      # Only include significant pathways\n    colorbar=True,           # Show colorbar\n    legend=False,            # Hide legend (optional)\n    yticks_fontsize=12,      # Font size for pathway names\n    show=False               # Don't display plots (just save)\n)\n\nprint(f\"\\nGenerated {len(figures)} heatmaps:\")\nfor name in figures.keys():\n    print(f\"  - {name}\")\n\n# ============================================================================\n# Step 3: Examine Results\n# ============================================================================\n# The heatmap_data DataFrame contains summed NES values\nprint(f\"\\nHeatmap data shape: {heatmap_data.shape}\")\nprint(f\"Pathways (rows): {heatmap_data.shape[0]}\")\nprint(f\"Timepoints (columns): {heatmap_data.shape[1]}\")\n\n# View top pathways at first and last timepoint\nfirst_timepoint = heatmap_data.iloc[:, 0].sort_values(ascending=False)\nlast_timepoint = heatmap_data.iloc[:, -1].sort_values(ascending=False)\n\nprint(\"\\nTop 5 pathways at early stage:\")\nprint(first_timepoint.head())\n\nprint(\"\\nTop 5 pathways at late stage:\")\nprint(last_timepoint.head())\n</code></pre>"},{"location":"tutorials/visualization/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"tutorials/visualization/#heatmap-structure","title":"Heatmap Structure","text":"<pre><code>Rows: Pathway names (e.g., \"Immune System\", \"DNA Repair\")\nColumns: Pseudo-time (early \u2192 late)\nValues: Sum of NES (Normalized Enrichment Score)\n  - Positive (red): Pathway upregulated\n  - Negative (blue): Pathway downregulated\n  - Zero (white): No change\nThe values can be any other metric deemed appropriate.\n</code></pre>"},{"location":"tutorials/visualization/#color-scheme","title":"Color Scheme","text":"<p>The heatmaps use a diverging colormap centered at zero:</p> <ul> <li>Red: Upregulated pathways (positive NES)</li> <li>White: No change (NES \u2248 0)</li> <li>Blue: Downregulated pathways (negative NES)</li> </ul> <p>The color scale is symmetric around zero, making it easy to identify regulation direction.</p>"},{"location":"tutorials/visualization/#output-files","title":"Output Files","text":"<p>After running <code>generate_pathway_heatmap()</code>, you'll find:</p> <pre><code>reports/figures/pathway_heatmaps/\n\u251c\u2500\u2500 heatmap_top50_changing.png          # Top 50 most dynamic pathways\n\u251c\u2500\u2500 heatmap_top50_upregulated.png       # Top 50 upregulated pathways\n\u251c\u2500\u2500 heatmap_top50_downregulated.png     # Top 50 downregulated pathways\n\u251c\u2500\u2500 heatmap_selected_high_level.png     # Reactome high-level pathways\n\u2514\u2500\u2500 heatmap_selected_literature.png     # Literature-curated pathways (if present)\n</code></pre> <p>Each heatmap is saved in PNG format at high resolution (300 DPI).</p>"},{"location":"tutorials/visualization/#customization-options","title":"Customization Options","text":""},{"location":"tutorials/visualization/#adjust-significance-threshold","title":"Adjust Significance Threshold","text":"<pre><code># More stringent: only FDR &lt; 0.01\nheatmap_data, figures = generate_pathway_heatmap(\n    enrichment_df=enrichment_df,\n    output_dir='reports/figures/',\n    fdr_threshold=0.01,  # More stringent\n    # ... other parameters\n)\n</code></pre>"},{"location":"tutorials/visualization/#change-visual-appearance","title":"Change Visual Appearance","text":"<pre><code># Larger font for pathway names\nheatmap_data, figures = generate_pathway_heatmap(\n    enrichment_df=enrichment_df,\n    output_dir='reports/figures/',\n    yticks_fontsize=14,  # Larger font\n    colorbar=True,       # Show colorbar\n    legend=True,         # Show legend\n    show=True            # Display plots interactively\n)\n</code></pre>"},{"location":"tutorials/visualization/#access-individual-figures","title":"Access Individual Figures","text":"<pre><code># Generate heatmaps\nheatmap_data, figures = generate_pathway_heatmap(...)\n\n# Access specific figure\nfig_changing = figures['heatmap_top50_changing']\nfig_upregulated = figures['heatmap_top50_upregulated']\n\n# Further customize with matplotlib\nimport matplotlib.pyplot as plt\n\nfig_changing.suptitle('My Custom Title', fontsize=20)\nfig_changing.savefig('custom_heatmap.pdf', dpi=300, bbox_inches='tight')\nplt.close(fig_changing)\n</code></pre>"},{"location":"tutorials/visualization/#understanding-the-heatmap-data","title":"Understanding the Heatmap Data","text":"<p>The returned <code>heatmap_data</code> DataFrame can be used for further analysis:</p> <pre><code># Generate heatmaps\nheatmap_data, figures = generate_pathway_heatmap(...)\n\n# Analyze pathway dynamics\n# Calculate change from early to late\npathway_changes = heatmap_data.iloc[:, -1] - heatmap_data.iloc[:, 0]\npathway_changes_sorted = pathway_changes.sort_values(ascending=False)\n\nprint(\"Most increasing pathways:\")\nprint(pathway_changes_sorted.head(10))\n\nprint(\"\\nMost decreasing pathways:\")\nprint(pathway_changes_sorted.tail(10))\n\n# Find pathways active throughout\nmean_nes = heatmap_data.mean(axis=1)\nstd_nes = heatmap_data.std(axis=1)\n\nconsistently_high = mean_nes[mean_nes &gt; 0.5].sort_values(ascending=False)\nprint(\"\\nConsistently upregulated pathways:\")\nprint(consistently_high.head(10))\n\n# Export for further analysis\nheatmap_data.to_csv('pathway_nes_matrix.csv')\n</code></pre>"},{"location":"tutorials/visualization/#see-also","title":"See Also","text":"<ul> <li>API Documentation - Plots</li> <li>API Documentation - Enrichment</li> <li>Enrichment Analysis Tutorial</li> </ul>"}]}